% This file was created with JabRef 2.10.
% Encoding: ISO8859_1


@Article{Abrardo-2016-p1333-1345,
  Title                    = {A Game-Theoretic Framework for Optimum Decision Fusion in the Presence of Byzantines},
  Author                   = {A. Abrardo and M. Barni and K. Kallas and B. Tondi},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1333-1345},
  Volume                   = {11},

  Abstract                 = {Optimum decision fusion in the presence of malicious nodes - often referred to as Byzantines - is hindered by the necessity of exactly knowing the statistical behavior of Byzantines. In this paper, we focus on a simple, yet widely adopted, setup in which a fusion center (FC) is asked to make a binary decision about a sequence of system states by relying on the possibly corrupted decisions provided by local nodes. We propose a game-theoretic framework, which permits to exploit the superior performance provided by optimum decision fusion, while limiting the amount of a priori knowledge required. We use numerical simulations to derive the optimum behavior of the FC and the Byzantines in a game-theoretic sense, and to evaluate the achievable performance at the equilibrium point of the game. We analyze several different setups, showing that in all cases, the proposed solution permits to improve the accuracy of data fusion. We also show that, in some cases, it is preferable for the Byzantines to minimize the mutual information between the status of the observed system and the reports submitted to the FC, rather than always flipping the decision made by the local nodes.},
  Doi                      = {10.1109/TIFS.2016.2526963},
  File                     = {Abrardo-2016-p1333-1345.pdf:References\\Abrardo-2016-p1333-1345.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {decision making;game theory;minimisation;numerical analysis;performance evaluation;sensor fusion;telecommunication security;wireless sensor networks;Byzantines;binary decision making;data fusion accuracy improvement;fusion center;game equilibrium point;game theory;malicious nodes;mutual information minimization;numerical simulations;optimum decision fusion;performance evaluation;Cognitive radio;Data integration;Dynamic programming;Error probability;Games;Knowledge engineering;Numerical simulation;Adversarial signal processing;Byzantine nodes;byzantine nodes;data fusion in malicious settings;decision fusion;distributed detection with corrupted reports;dynamic programming;game theory},
  Reviewtime               = {212},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7401067}
}

@Article{Almalawi-2016-p893-906,
  Title                    = {An Efficient Data-Driven Clustering Technique to Detect Attacks in SCADA Systems},
  Author                   = {A. Almalawi and A. Fahad and Z. Tari and A. Alamri and R. AlGhamdi and A. Y. Zomaya},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {893-906},
  Volume                   = {11},

  Abstract                 = {Supervisory control and data acquisition (SCADA) systems have become a salient part in controlling critical infrastructures, such as power plants, energy grids, and water distribution systems. In the past decades, these systems were isolated and use proprietary software, operating systems, and protocols. In recent years, SCADA systems have been interfaced with enterprise systems, which therefore exposed them to the vulnerabilities of the Internet and the security threats. Traditional security solutions (e.g., firewalls, antivirus software, and intrusion detection systems) cannot fully protect SCADA systems, because they have different requirements. This paper presents an innovative intrusion detection approach to detect SCADA tailored attacks. This is based on a data-driven clustering technique of process parameters, which automatically identifies the normal and critical states of a given system. Later, it extracts proximity-based detection rules from the identified states for monitoring purposes. The effectiveness of the proposed approach is tested by conducting experiments on eight data sets that consist of process parameters' values. The empirical results demonstrated an average accuracy of 98% in automatically identifying the critical states, while facilitating the monitoring of the SCADA system.},
  Doi                      = {10.1109/TIFS.2015.2512522},
  File                     = {Almalawi-2016-p893-906.pdf:References\\Almalawi-2016-p893-906.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {SCADA systems;control engineering computing;pattern clustering;security of data;Internet vulnerability;SCADA systems;SCADA tailored attack detection;data-driven clustering technique;intrusion detection approach;operating systems;process parameters;proprietary software;protocols;proximity-based detection rules;security threats;supervisory control and data acquisition system;Computer science;Monitoring;SCADA systems;Security;Servers;Water resources;Classification;Clustering;IDS;SCADA Security},
  Reviewtime               = {324},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7366594}
}

@Article{Bajaj-2016-p303-312,
  Title                    = {Practical Foundations of History Independence},
  Author                   = {S. Bajaj and A. Chakraborti and R. Sion},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {303-312},
  Volume                   = {11},

  Abstract                 = {The way data structures organize data is often a function of the sequence of past operations. The organization of data is referred to as the data structure's state, and the sequence of past operations constitutes the data structure's history. A data structure state can, therefore, be used as an oracle to derive information about its history. For history-sensitive applications, such as privacy in e-voting, it is imperative to conceal historical information contained within data structure states. Data structure history can be hidden by making data structures history independent. In this paper, we explore how to achieve history independence (HI). We observe that the current HI notions are significantly limited in number and scope. There are two existing notions of HI: 1) weak HI (WHI) and 2) strong HI (SHI). WHI does not protect against insider adversaries, and SHI mandates canonical representations, resulting in inefficiency. We postulate the need for a broad, encompassing notion of HI, which can capture WHI, SHI, and a broad spectrum of new HI notions. To this end, we introduce AHI, a generic game-based framework that is malleable enough to accommodate the existing and new HI notions. As an essential step toward formalizing AHI, we explore the concepts of abstract data types, data structures, machine models, memory representations, and HI. Finally, to bridge the gap between theory and practice, we outline a general recipe for building end-to-end, history-independent systems and demonstrate the use of the recipe in designing two historyindependent file systems.},
  Doi                      = {10.1109/TIFS.2015.2491309},
  File                     = {Bajaj-2016-p303-312.pdf:References\\Bajaj-2016-p303-312.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {abstract data types;data privacy;AHI generic game-based framework;abstract data types;data structure history independence;e-voting;end-to-end history-independent file systems;history-sensitive applications;machine models;memory representations;privacy;Central Processing Unit;Computational modeling;Data models;Data structures;History;Organizations;Random access memory;History independence;data structures;regulatory compliance},
  Reviewtime               = {241},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7299314}
}

@Article{Bharadwaj-2016-p1630-1641,
  Title                    = {Domain Specific Learning for Newborn Face Recognition},
  Author                   = {S. Bharadwaj and H. S. Bhatt and M. Vatsa and R. Singh},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1630-1641},
  Volume                   = {11},

  Abstract                 = {Biometric recognition of newborn babies is an opportunity for the realization of several useful applications, such as improved security against swapping and abduction, accurate census, and effective drug delivery. This paper explores the possibility of using face recognition toward an affordable and friendly biometric modality for newborns. The paper proposes an autoencoder-based feature representation followed by problem specific distance metric learning via one-shot similarity with one class-online support vector machine. The largest publicly available database of newborns collected from various sources to study face recognition is introduced. Several existing face recognition approaches and commercial systems are also evaluated on a common benchmark protocol. The efficacy of the proposed algorithm is evaluated under both verification and identification settings. With multiple galleries, rank-1 identification accuracy of 78.5% and verification accuracy of 63.4% at 0.1% false accept rate are achieved.},
  Doi                      = {10.1109/TIFS.2016.2538744},
  File                     = {Bharadwaj-2016-p1630-1641.pdf:References\\Bharadwaj-2016-p1630-1641.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {biometrics (access control);face recognition;learning (artificial intelligence);support vector machines;autoencoder-based feature representation;biometric modality;distance metric learning;domain specific learning;newborn face recognition;one-shot similarity;online support vector machine;Databases;Face;Face recognition;Hospitals;Manuals;Pediatrics;Newborn face recognition;autoencoder;distance metric learning;domain adaptation;infant identification;semi-supervised learning},
  Reviewtime               = {152},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7432030}
}

@Article{Bianchi-2016-p313-327,
  Title                    = {Analysis of One-Time Random Projections for Privacy Preserving Compressed Sensing},
  Author                   = {T. Bianchi and V. Bioglio and E. Magli},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {313-327},
  Volume                   = {11},

  Abstract                 = {In this paper, the security of the compressed sensing (CS) framework as a form of data confidentiality is analyzed. Two important properties of one-time random linear measurements acquired using a Gaussian independent identically distributed matrix are outlined: 1) the measurements reveal only the energy of the sensed signal and 2) only the energy of the measurements leaks information about the signal. An important consequence of the above facts is that CS provides information theoretic secrecy in a particular setting. Namely, a simple strategy based on the normalization of the Gaussian measurements achieves, at least in theory, perfect secrecy, enabling the use of CS as an additional security layer in privacy preserving applications. In the generic setting in which CS does not provide information theoretic secrecy, two alternative security notions linked to the difficulty of estimating the energy of the signal and distinguishing equal-energy signals are introduced. Useful bounds on the mean square error of any possible estimator and the probability of error of any possible detector are provided and compared with the simulations. The results indicate that CS is in general not secure according to cryptographic standards, but may provide a useful built-in data obfuscation layer.},
  Doi                      = {10.1109/TIFS.2015.2493982},
  File                     = {Bianchi-2016-p313-327.pdf:References\\Bianchi-2016-p313-327.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {Gaussian processes;compressed sensing;cryptography;data privacy;information theory;matrix algebra;mean square error methods;random processes;Gaussian independent identically distributed matrix;built-in data obfuscation layer;cryptographic standards;data confidentiality;equal-energy signals;information theoretic secrecy;mean square error;one-time random linear measurements;one-time random projections;privacy preserving compressed sensing;Compressed sensing;Cryptography;Energy measurement;Privacy;Sensors;Compressed sensing;confidentiality;encryption;privacy preservation;random matrices;security},
  Reviewtime               = {200},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7305773}
}

@Article{Bock-2016-p19-34,
  Title                    = {JPGcarve: An Advanced Tool for Automated Recovery of Fragmented JPEG Files},
  Author                   = {J. De Bock and P. De Smet},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {19-34},
  Volume                   = {11},

  Abstract                 = {In this paper, we present a new tool for forensic recovery of single and multi-fragment JPEG/JFIF data files. First, we discuss the basic design and the technical methods composing our proposed data carving algorithm. Next, we compare the performance of our method with the well-known Adroit Photo Forensics (APF) state-of-the art tool. This comparison is centered on both the carving results as well as the obtained data processing speed, and is evaluated in terms of the results that can be obtained for several well-known reference data sets. Important to note is that we specifically focus on the fundamental recovery and fragment matching performance of the tools by forcing them to use various assumed cluster sizes. We show that on all accounts our new tool can significantly outperform APF. This improvement in data processing speed and carving results can be mostly attributed to novel methods to iterate and reduce the data search space and to a novel parameterless method to determine the end of a fragment based on the pixel data. Finally, we discuss several options for future research.},
  Doi                      = {10.1109/TIFS.2015.2475238},
  File                     = {Bock-2016-p19-34.pdf:References\\Bock-2016-p19-34.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {image coding;image matching;search problems;APF;Adroit photo forensics;JPGcarve;automated fragmented JPEG files recovery;data carving algorithm;data processing speed;data search space;forensic recovery;fragment matching performance;fundamental recovery;multifragment JPEG-JFIF data files;parameterless method;pixel data;reference data sets;Data processing;Decoding;Encoding;Forensics;Image coding;Reliability;Transform coding;JPEG recovery;digital forensics;file carving},
  Reviewtime               = {210},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7230269}
}

@Article{Carvalho-2016-p720-733,
  Title                    = {Illuminant-Based Transformed Spaces for Image Forensics},
  Author                   = {T. Carvalho and F. A. Faria and H. Pedrini and R. da S. Torres and A. Rocha},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {720-733},
  Volume                   = {11},

  Abstract                 = {In this paper, we explore transformed spaces, represented by image illuminant maps, to propose a methodology for selecting complementary forms of characterizing visual properties for an effective and automated detection of image forgeries. We combine statistical telltales provided by different image descriptors that explore color, shape, and texture features. We focus on detecting image forgeries containing people and present a method for locating the forgery, specifically, the face of a person in an image. Experiments performed on three different open-access data sets show the potential of the proposed method for pinpointing image forgeries containing people. In the two first data sets (DSO-1 and DSI-1), the proposed method achieved a classification accuracy of 94% and 84%, respectively, a remarkable improvement when compared with the state-of-the-art methods. Finally, when evaluating the third data set comprising questioned images downloaded from the Internet, we also present a detailed analysis of target images.},
  Doi                      = {10.1109/TIFS.2015.2506548},
  File                     = {Carvalho-2016-p720-733.pdf:References\\Carvalho-2016-p720-733.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {image colour analysis;image forensics;image texture;automated image forgery detection;illuminant-based transformed spaces;image color features;image descriptors;image forensics;image illuminant maps;image shape features;image texture features;open-access data sets;visual property characaterization;Forgery;Image color analysis;Light sources;Lighting;Shape;Splicing;Visualization;Digital forensics;diversity measures;illuminant maps;image descriptors;machine learning;splicing detection},
  Reviewtime               = {102},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7349174}
}

@Article{Castiglione-2016-p850-865,
  Title                    = {Hierarchical and Shared Access Control},
  Author                   = {A. Castiglione and A. De Santis and B. Masucci and F. Palmieri and A. Castiglione and J. Li and X. Huang},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {850-865},
  Volume                   = {11},

  Abstract                 = {Access control ensures that only the authorized users of a system are allowed to access certain resources or tasks. Usually, according to their roles and responsibilities, users are organized in hierarchies formed by a certain number of disjoint classes. Such hierarchies are implemented by assigning a key to each class, so that the keys for descendant classes can be efficiently derived from classes higher in the hierarchy. However, pure hierarchical access may represent a limitation in many real-world cases. In fact, sometimes it is necessary to ensure access to a resource or task by considering both its directly responsible user and a group of users possessing certain credentials. In this paper, we first propose a novel model that generalizes the conventional hierarchical access control paradigm, by extending it to certain additional sets of qualified users. Afterward, we propose two constructions for hierarchical key assignment schemes in this new model, which are provably secure with respect to key indistinguishability. In particular, the former construction relies on both symmetric encryption and perfect secret sharing, whereas, the latter is based on public-key threshold broadcast encryption.},
  Doi                      = {10.1109/TIFS.2015.2512533},
  File                     = {Castiglione-2016-p850-865.pdf:References\\Castiglione-2016-p850-865.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {access control;public key cryptography;hierarchical access control paradigm;hierarchical key assignment schemes;perfect secret sharing;public-key threshold broadcast encryption;shared access control;symmetric encryption;Access control;Collaboration;Computer science;Encryption;Medical services;Generalized access control;Generalized access model;Key assignment;Multiple access structures;Provable security;Shared key reconstruction;generalized access model;key assignment;multiple access structures;provable security;shared key reconstruction},
  Reviewtime               = {146},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7366590}
}

@Article{Caviglione-2016-p799-810,
  Title                    = {Seeing the Unseen: Revealing Mobile Malware Hidden Communications via Energy Consumption and Artificial Intelligence},
  Author                   = {L. Caviglione and M. Gaggero and J. F. Lalande and W. Mazurczyk and M. UrbaÅ„ski},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {799-810},
  Volume                   = {11},

  Abstract                 = {Modern malware uses advanced techniques to hide from static and dynamic analysis tools. To achieve stealthiness when attacking a mobile device, an effective approach is the use of a covert channel built by two colluding applications to exchange data locally. Since this process is tightly coupled with the used hiding method, its detection is a challenging task, also worsened by the very low transmission rates. As a consequence, it is important to investigate how to reveal the presence of malicious software using general indicators, such as the energy consumed by the device. In this perspective, this paper aims to spot malware covertly exchanging data using two detection methods based on artificial intelligence tools, such as neural networks and decision trees. To verify their effectiveness, seven covert channels have been implemented and tested over a measurement framework using Android devices. Experimental results show the feasibility and effectiveness of the proposed approach to detect the hidden data exchange between colluding applications.},
  Doi                      = {10.1109/TIFS.2015.2510825},
  File                     = {Caviglione-2016-p799-810.pdf:References\\Caviglione-2016-p799-810.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {decision trees;energy consumption;invasive software;mobile computing;neural nets;program diagnostics;Android devices;artificial intelligence;decision trees;dynamic analysis tools;energy consumption;mobile malware hidden communications;neural networks;static analysis tools;Energy consumption;Energy measurement;Malware;Mobile handsets;Neural networks;Performance evaluation;Power demand;Energy-based malware detection;colluding applications;covert channels;decision trees;neural networks},
  Reviewtime               = {68},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7362010}
}

@Article{Chen-2016-p1093-1105,
  Title                    = {Horizontal and Vertical Side Channel Analysis of a McEliece Cryptosystem},
  Author                   = {C. Chen and T. Eisenbarth and I. von Maurich and R. Steinwandt},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1093-1105},
  Volume                   = {11},

  Abstract                 = {This paper presents horizontal and vertical side channel analysis techniques for an implementation of the McEliece cryptosystem. The target of this side-channel attack is a state-of-the-art field-programmable gate array (FPGA) implementation of the efficient quasi-cyclic moderate-density parity-check McEliece decryption operation, as presented at Design, Automation and Test in Europe (DATE) 2014. The presented cryptanalysis succeeds to recover the complete secret key after a few observed decryptions. It consists of a combination of a differential leakage analysis during the syndrome computation followed by an algebraic step that exploits the relation between the public key and the private key.},
  Doi                      = {10.1109/TIFS.2015.2509944},
  File                     = {Chen-2016-p1093-1105.pdf:References\\Chen-2016-p1093-1105.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {field programmable gate arrays;private key cryptography;public key cryptography;FPGA;field-programmable gate array;horizontal side channel analysis;private key;public key;quasicyclic moderate-density parity-check McEliece decryption operation;secret key;side-channel attack;syndrome computation;vertical side channel analysis;Decoding;Encryption;Field programmable gate arrays;Generators;Public key;Differential Power Analysis;Differential power analysis;FPGA;McEliece Cryptosystem;McEliece cryptosystem;QC-MDPC Codes;QC-MDPC codes},
  Reviewtime               = {143},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7360160}
}

@Article{Chen-2016-p1476-1485,
  Title                    = {Iris Recognition Based on Human-Interpretable Features},
  Author                   = {J. Chen and F. Shen and D. Z. Chen and P. J. Flynn},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1476-1485},
  Volume                   = {11},

  Abstract                 = {The iris is a stable biometric trait that has been widely used for human recognition in various applications. However, deployment of iris recognition in forensic applications has not been reported. A primary reason is the lack of human-friendly techniques for iris comparison. To further promote the use of iris recognition in forensics, the similarity between irises should be made visualizable and interpretable. Recently, a human-in-the-loop iris recognition system was developed, based on detecting and matching iris crypts. Building on this framework, we propose a new approach for detecting and matching iris crypts automatically. Our detection method is able to capture iris crypts of various sizes. Our matching scheme is designed to handle potential topological changes in the detection of the same crypt in different images. Our approach outperforms the known visible-feature-based iris recognition method on three different data sets. In particular, our approach achieves over 22% higher rank one hit rate in identification, and over 51% lower equal error rate in verification. In addition, the benefit of our approach on multi-enrollment is experimentally demonstrated.},
  Doi                      = {10.1109/TIFS.2016.2535901},
  File                     = {Chen-2016-p1476-1485.pdf:References\\Chen-2016-p1476-1485.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {iris recognition;human recognition;human-friendly techniques;human-in-the-loop iris recognition system;human-interpretable features;iris recognition;matching scheme;visible-feature-based iris recognition method;Algorithm design and analysis;Cryptography;Feature extraction;Forensics;Image segmentation;Iris recognition;Probes;Iris recognition;forensics;human-in-the-loop;iris recognition;visible feature},
  Reviewtime               = {175},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7422104}
}

@Article{Chen-2016-p1453-1460,
  Title                    = {Evaluating Node Reliability in Cooperative MIMO Networks},
  Author                   = {K. Chen and B. B. Natarajan},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1453-1460},
  Volume                   = {11},

  Abstract                 = {Cooperative multiple input multiple output (Co-MIMO) strategies represent one approach to meet the growing requirements (i.e., higher throughput, enhanced coverage, low latencies, and reduced cost) of wireless communication services. In Co-MIMO networks, low-power relay nodes (RNs) are recruited by mobile users to cooperate as virtual antenna arrays. Although Co-MIMO architectures can offer significant improvement in both the performance and security of wireless networks, they are susceptible to attacks. In this paper, we propose a novel node reliability evaluation scheme to enhance the security of Co-MIMO networks. Leveraging the probe signal transmissions involved in physical-layer secret key generation schemes, two distributed node level reliability detection methods (one-shot and dynamic) are proposed to detect RNs that are noncooperative. Based on the fusion of information from the RNs, an overall reliability evaluation can be accomplished at a central server. Mobile users interested in collaboration can access this central server to determine which nodes to recruit for cooperation. Both the theoretical analysis and the simulation results are presented to illustrate the proposed node reliability evaluation schemes.},
  Doi                      = {10.1109/TIFS.2016.2532841},
  File                     = {Chen-2016-p1453-1460.pdf:References\\Chen-2016-p1453-1460.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {MIMO communication;antenna arrays;cooperative communication;mobile communication;telecommunication network reliability;Co-MIMO networks;cooperative MIMO networks;cooperative multiple input multiple output strategies;low-power relay nodes;mobile users;novel node reliability evaluation scheme;physical-layer secret key generation schemes;virtual antenna arrays;wireless communication services;MIMO;Mobile communication;Probes;Relays;Reliability;Security;Transmitters;Physical layer security;cooperative MIMO;node reliability evaluation;power detection/sensing;statistical test},
  Reviewtime               = {157},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7415975}
}

@Article{Chen-2016-p789-798,
  Title                    = {Dual-Server Public-Key Encryption With Keyword Search for Secure Cloud Storage},
  Author                   = {R. Chen and Y. Mu and G. Yang and F. Guo and X. Wang},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {789-798},
  Volume                   = {11},

  Abstract                 = {Searchable encryption is of increasing interest for protecting the data privacy in secure searchable cloud storage. In this paper, we investigate the security of a well-known cryptographic primitive, namely, public key encryption with keyword search (PEKS) which is very useful in many applications of cloud storage. Unfortunately, it has been shown that the traditional PEKS framework suffers from an inherent insecurity called inside keyword guessing attack (KGA) launched by the malicious server. To address this security vulnerability, we propose a new PEKS framework named dual-server PEKS (DS-PEKS). As another main contribution, we define a new variant of the smooth projective hash functions (SPHFs) referred to as linear and homomorphic SPHF (LH-SPHF). We then show a generic construction of secure DS-PEKS from LH-SPHF. To illustrate the feasibility of our new framework, we provide an efficient instantiation of the general framework from a Decision Diffie-Hellman-based LH-SPHF and show that it can achieve the strong security against inside the KGA.},
  Doi                      = {10.1109/TIFS.2015.2510822},
  File                     = {Chen-2016-p789-798.pdf:References\\Chen-2016-p789-798.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cloud computing;public key cryptography;storage management;DS-PEKS framework;KGA;LH-SPHF;cryptographic primitive security;data privacy protection;decision Diffie-Hellman-based LH-SPHF;dual-server public key encryption with keyword search;keyword guessing attack;linear and homomorphic SPHF;secure searchable cloud storage;security vulnerability;smooth projective hash functions;Encryption;Keyword search;Public key;Receivers;Servers;Diffie-Hellman language;Keyword search;encryption;inside keyword guessing attack;secure cloud storage;smooth projective hash function},
  Reviewtime               = {112},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7361997}
}

@Article{Cheng-2016-p993-1002,
  Title                    = {Security Analysis and Improvements on Two Homomorphic Authentication Schemes for Network Coding},
  Author                   = {C. Cheng and J. Lee and T. Jiang and T. Takagi},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {993-1002},
  Volume                   = {11},

  Abstract                 = {Recently, based on the homomorphic signatures, the authentication schemes, such as homomorphic subspace signature (HSS) and key predistribution-based tag encoding (KEPTE), have been proposed to resist against pollution attacks in network coding. In this paper, we show that there exists an efficient multi-generation pollution attack on HSS and KEPTE. In particular, we show that using packets and their signatures of different generations, the adversary can create invalid packets and their corresponding signatures that pass the verification of HSS and KEPTE at intermediate the nodes as well as at the destination nodes. After giving a more generic attack, we analyze the cause of the proposed attack. We then propose the improved key distribution schemes for HSS and KEPTE, respectively. Next, we show that the proposed key distribution schemes can combat against the proposed multi-generation pollution attacks. Finally, we analyze the computation and communication costs of the proposed key distribution schemes for HSS and KEPTE, and by implementing experiments, we demonstrate that the proposed schemes add acceptable burden on the system.},
  Doi                      = {10.1109/TIFS.2016.2515517},
  File                     = {Cheng-2016-p993-1002.pdf:References\\Cheng-2016-p993-1002.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptography;network coding;telecommunication security;homomorphic authentication schemes;homomorphic subspace signature;key distribution schemes;key predistribution-based tag encoding;multigeneration pollution attack;network coding;security analysis;Authentication;Encoding;Network coding;Pollution;Resists;Routing;Network coding;homomorphic authentication;homomorphic signature;pollution attack},
  Reviewtime               = {210},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7373629}
}

@Article{Cheng-2016-p273-288,
  Title                    = {Opportunistic Piggyback Marking for IP Traceback},
  Author                   = {L. Cheng and D. M. Divakaran and W. Y. Lim and V. L. L. Thing},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {273-288},
  Volume                   = {11},

  Abstract                 = {IP traceback is a solution for attributing cyber attacks, and it is also useful for accounting user traffic and network diagnosis. Marking-based traceback (MBT) has been considered a promising traceback approach, and has received considerable attention. However, we find that the traceback message delivery problem in MBT, which is important to the successful completion of a traceback, has not been adequately studied in the literature. To address this issue, we present the design, analysis, and evaluation of opportunistic piggyback marking (OPM) for IP traceback in this paper. The OPM distinguishes itself from the existing works by decoupling the traceback message content encoding and delivery functions in MBT, and efficiently achieves expedited and robust traceback message delivery by exploiting piggyback marking opportunities. Based on the proposed OPM scheme, we then present the flexible marking-based traceback framework, which is a novel design paradigm for IP traceback and has several favorable features for practical deployment of IP traceback. Through the numerical analysis and the comprehensive simulation evaluations, we demonstrate that our design effectively reduces the traceback completion delay and router processing overhead, and increases the message delivery ratio compared with other baseline approaches.},
  Doi                      = {10.1109/TIFS.2015.2491299},
  File                     = {Cheng-2016-p273-288.pdf:References\\Cheng-2016-p273-288.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {IP networks;Internet;computer network security;IP traceback;MBT approach;OPM scheme;cyber attacks;marking-based traceback;network diagnosis;opportunistic piggyback marking;traceback message content encoding;traceback message delivery problem;user traffic diagnosis;Delays;Encoding;IP networks;Probabilistic logic;Robustness;Routing;Routing protocols;IP Traceback;IP traceback;Marking-based Traceback;Network Forensics;Opportunistic Piggyback Marking;marking-based traceback;network forensics;opportunistic piggyback marking},
  Reviewtime               = {164},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7299300}
}

@Article{Cheon-2016-p188-199,
  Title                    = {Optimized Search-and-Compute Circuits and Their Application to Query Evaluation on Encrypted Data},
  Author                   = {J. H. Cheon and M. Kim and M. Kim},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {188-199},
  Volume                   = {11},

  Abstract                 = {Private query processing on encrypted databases allows users to obtain data from encrypted databases in such a way that the users' sensitive data will be protected from exposure. Given an encrypted database, users typically submit queries similar to the following examples: 1) How many employees in an organization make over U.S. $100000? 2) What is the average age of factory workers suffering from leukemia? Answering the questions requires one to search and then compute over the relevant encrypted data sets in sequence. In this paper, we are interested in efficiently processing queries that require both operations to be performed on fully encrypted databases. One immediate solution is to use several special-purpose encryption schemes simultaneously; however, this approach is associated with a high computational cost for maintaining multiple encryption contexts. Another solution is to use a privacy homomorphic scheme. However, no secure solutions have been developed that satisfy the efficiency requirements. In this paper, we construct a unified framework to efficiently and privately process queries with search and compute operations. For this purpose, the first part of our work involves devising several underlying circuits as primitives for queries on encrypted data. Second, we apply two optimization techniques to improve the efficiency of these circuit primitives. One technique involves exploiting single-instruction-multiple-data (SIMD) techniques to accelerate the basic circuit operations. Unlike general SIMD approaches, our SIMD implementation can be applied even to a single basic operation. The other technique is to use a large integer ring (e.g., Z2t) as a message space rather than a binary field. Even for an integer of k bits with k > t, addition can be performed using degree 1 circuits with lazy carry operations. Finally, we present various experiments performed by varying the considered parameters, such as the query type and the number of tuple- .},
  Doi                      = {10.1109/TIFS.2015.2483486},
  File                     = {Cheon-2016-p188-199.pdf:References\\Cheon-2016-p188-199.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptography;data protection;optimisation;parallel processing;query processing;question answering (information retrieval);SIMD techniques;degree 1 circuits with lazy carry operations;encrypted databases;large integer ring;message space;optimized search-and-compute circuits;privacy homomorphic scheme;private query processing;query evaluation;question answering;single-instruction-multiple-data techniques;special-purpose encryption schemes;user sensitive data protection;Encryption;Optimization;Polynomials;Query processing;Servers;Encrypted databases;Homomorphic encryption;Private query processing;homomorphic encryption;private query processing},
  Reviewtime               = {180},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7279139}
}

@Article{Cherkaoui-2016-p1291-1305,
  Title                    = {Design, Evaluation, and Optimization of Physical Unclonable Functions Based on Transient Effect Ring Oscillators},
  Author                   = {A. Cherkaoui and L. Bossuet and C. Marchand},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1291-1305},
  Volume                   = {11},

  Abstract                 = {This paper proposes a theoretical study and a full overview of the design, evaluation, and optimization of a PUF based on transient element ring oscillators (TERO-PUF). We show how, by following some simple design rules and strategies, designers can build and optimize a TERO-PUF with the state-of-the-art PUF characteristics in a standard CMOS technology. To this end, we analyzed the uniqueness, steadiness, and randomness of responses generated from 30 test chips in a CMOS 350-nm process in nominal and corner voltage and temperature conditions. Response generation schemes are proposed and discussed to optimize the PUF performances and reduce its area without noticeable loss in its output quality. In particular, we show that the large area of the basic blocks in the TERO-PUF is balanced by the high level of entropy extracted in each basic block. Guidelines are provided to balance reliability and randomness of the responses and the design area.},
  Doi                      = {10.1109/TIFS.2016.2524666},
  File                     = {Cherkaoui-2016-p1291-1305.pdf:References\\Cherkaoui-2016-p1291-1305.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {CMOS logic circuits;circuit optimisation;cryptography;digital signatures;entropy;integrated circuit design;oscillators;performance evaluation;transient analysis;PUF performance optimization;TERO-PUF optimization;corner voltage conditions;cryptography;design rules;digital signatures;entropy;nominal temperature conditions;nominal voltage conditions;physical unclonable function design;physical unclonable function evaluation;physical unclonable function optimization;reliability;response generation schemes;size 350 nm;standard CMOS technology;transient effect ring oscillators;Entropy;Inverters;Optimization;Propagation delay;Ring oscillators;Transient analysis;Information security;authentication;cryptography;digital signatures},
  Reviewtime               = {210},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7398065}
}

@Article{Chernyshev-2016-p584-593,
  Title                    = {On 802.11 Access Point Locatability and Named Entity Recognition in Service Set Identifiers},
  Author                   = {M. Chernyshev and C. Valli and P. Hannay},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {584-593},
  Volume                   = {11},

  Abstract                 = {The 802.11 active service discovery mechanism requires the transmission of various attributes in a plain text. These attributes can be collected using passive monitoring and can be used to enumerate the preferred network list (PNL) of client devices. In this paper, we focus on the information that can be obtained using the service set identifiers (SSIDs) that make up the PNL. First, we describe a simple model based on a wireless access point geolocation technique to gauge the potential device locatability using data available on WiGLE.net. Second, we look at additional information that can be extracted from the SSID strings. Our hypothesis is that the entities of potential interest, such as locations and personal names contained within SSIDs, can be recognized in an automated fashion. Using two freely available pretrained named entity recognizers, we were able to identify up to 49% of SSIDs as possibly carrying entities of interest based on multiple data sets. We also show that extracted attributes can be used as an inference basis for additional inference attacks, which presents further opportunities in forensic and intelligence contexts.},
  Doi                      = {10.1109/TIFS.2015.2507542},
  File                     = {Chernyshev-2016-p584-593.pdf:References\\Chernyshev-2016-p584-593.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {inference mechanisms;text analysis;wireless LAN;802.11 access point locatability;802.11 active service discovery mechanism;PNL;SSID strings;WiGLE.net;client devices;device locatability;freely-available pretrained named entity recognizers;inference attacks;inference basis;passive monitoring;preferred network list;service set identifiers;wireless access point geolocation technique;Australia;Communication system security;IEEE 802.11 Standard;Probes;Security;Surveillance;Wireless communication;Forensics;Geospatial analysis;Sensor systems and applications;Surveillance;Text recognition;Wireless networks;forensics;geospatial analysis;sensor systems and applications;surveillance;text recognition},
  Reviewtime               = {197},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7352345}
}

@Article{Chu-2016-p823-836,
  Title                    = {Detectability of the Order of Operations: An Information Theoretic Approach},
  Author                   = {X. Chu and Y. Chen and K. J. R. Liu},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {823-836},
  Volume                   = {11},

  Abstract                 = {As it is more and more convenient to manipulate multimedia content, the authenticity of multimedia content becomes questionable. While there are many forensic techniques developed to identify the use of a single manipulation operation, a few has considered the cases where multiple operations may be involved. In these cases, investigators not only need to identify the use of each operation, but also need to detect the order of these operations. However, due to the interplay among operations, the order of operations may not always be detectable. This leads to a fundamental question of when we can and cannot detect the order of operations. In this paper, we formulate the problem of detecting the order of operations as a multiple hypotheses testing problem. Then, we propose an information theoretical framework to model the relationship between the detected hypothesis and the true hypothesis. Under this framework, we propose a mutual information-based criterion to obtain the best detector and use it to determine whether we can or cannot detect the order of operations based on certain set of features. A case study of detecting the order of resizing and blurring has been examined to demonstrate the effectiveness of the proposed framework and criteria. In addition, two known forensic problems are considered in the simulations to show that the results obtained from the proposed framework and criteria match those of the existing works.},
  Doi                      = {10.1109/TIFS.2015.2510958},
  File                     = {Chu-2016-p823-836.pdf:References\\Chu-2016-p823-836.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {data compression;digital forensics;image coding;image restoration;information theory;multimedia computing;object detection;statistical testing;blurring order detection;forensic techniques;information theoretical framework;multimedia content authenticity;multimedia content manipulation;multiple hypotheses testing problem;mutual information-based criterion;operation order detectability;resizing order detection;Detectors;Discrete Fourier transforms;Feature extraction;Forensics;Image coding;Multimedia communication;Testing;Order forensics;conditional fingerprints;mutual information;resizing and blurring},
  Reviewtime               = {186},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7362200}
}

@Article{Chu-2016-p774-788,
  Title                    = {Information Theoretical Limit of Media Forensics: The Forensicability},
  Author                   = {X. Chu and Y. Chen and M. C. Stamm and K. J. R. Liu},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {774-788},
  Volume                   = {11},

  Abstract                 = {While more and more forensic techniques have been proposed to detect the processing history of multimedia content, one starts to wonder if there exists a fundamental limit on the capability of forensics. In other words, besides keeping on searching what investigators can do, it is also important to find out the limit of their capability and what they cannot do. In this paper, we explore the fundamental limit of operation forensics by proposing an information theoretical framework. In particular, we consider a general forensic system of estimating operations' hypotheses based on extracted features from the multimedia content. In this system, forensicability is defined as the maximum forensic information that features contain about operations. Then, due to its conceptual similarity with mutual information in an information theory, forensicability is measured as the mutual information between features and operations' hypotheses. Such a measurement gives the error probability lower bound of all practical estimators, which use these features to detect the operations' hypotheses. Furthermore, it can determine the maximum number of hypotheses that we can theoretically detect. To demonstrate the effectiveness of our proposed information theoretical framework, we apply this framework on a forensic example of detecting the number of JPEG compressions based on normalized discrete cosine transform (DCT) coefficient histograms. We conclude that, when subband (2, 3) is used in detection and the size of the testing database is <;20000, the maximum number of JPEG compressions that we can expectedly perfectly detect using normalized DCT coefficient histogram features is four. Furthermore, we obtain the optimal strategies for investigators and forgers based on the fundamental measurement of forensicability.},
  Doi                      = {10.1109/TIFS.2015.2510820},
  File                     = {Chu-2016-p774-788.pdf:References\\Chu-2016-p774-788.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {data compression;digital forensics;discrete cosine transforms;feature extraction;image coding;information theory;probability;JPEG compression;feature extraction;forensicability;information theoretical limit;maximum forensic information;media forensics;multimedia content;mutual information theory;normalized DCT coefficient histogram feature;normalized discrete cosine transform coefficient histogram;Discrete cosine transforms;Feature extraction;Forensics;Histograms;History;Image coding;Multimedia communication;Forensicability;JPEG compression;fundamental limit;information theory;operation forensics},
  Reviewtime               = {188},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7362016}
}

@Article{Compagno-2016-p1565-1577,
  Title                    = {Modeling Enlargement Attacks Against UWB Distance Bounding Protocols},
  Author                   = {A. Compagno and M. Conti and A. A. Dâ€™Amico and G. Dini and P. Perazzo and L. Taponecco},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1565-1577},
  Volume                   = {11},

  Abstract                 = {Distance bounding protocols make it possible to determine a trusted upper bound on the distance between two devices. Their key property is to resist reduction attacks, i.e., attacks aimed at reducing the distance measured by the protocol. Recently, researchers have also focused on enlargement attacks, aimed at enlarging the measured distance. Providing security against such attacks is important for secure positioning techniques. The contribution of this paper is to provide a probabilistic model for the success of an enlargement attack against a distance bounding protocol realized with the IEEE 802.15.4a ultra-wideband standard. The model captures several variables, such as the propagation environment, the signal-to-noise ratio, and the time-of-arrival estimation algorithm. We focus on non-coherent receivers, which can be used in low-cost low-power applications. We validate our model by comparison with physical-layer simulations and goodness-of-fit tests. The results show that our probabilistic model is sufficiently realistic to replace physical-layer simulations. Our model can be used to evaluate the security of the ranging/positioning solutions that can be subject to enlargement attacks. We expect that it will significantly facilitate future research on secure ranging and secure positioning.},
  Doi                      = {10.1109/TIFS.2016.2541613},
  File                     = {Compagno-2016-p1565-1577.pdf:References\\Compagno-2016-p1565-1577.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {probability;protocols;radio receivers;telecommunication security;ultra wideband communication;IEEE 802.15.4a ultrawideband standard;UWB distance bounding protocol;distance measurement;enlargement attack modeling;goodness-of-fit testing;low-power application;noncoherent receiver;physical-layer simulation;probabilistic model;propagation environment;secure positioning technique;secure ranging technique;signal-to-noise ratio;time-of-arrival estimation algorithm;trusted upper bound;Delays;IEEE 802.15 Standard;Mathematical model;Probabilistic logic;Protocols;Receivers;Time of arrival estimation;Security;distance bounding;enlargement attacks;ranging;ultrawideband},
  Reviewtime               = {159},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7432025}
}

@Article{Conti-2016-p114-125,
  Title                    = {Analyzing Android Encrypted Network Traffic to Identify User Actions},
  Author                   = {M. Conti and L. V. Mancini and R. Spolaor and N. V. Verde},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {114-125},
  Volume                   = {11},

  Abstract                 = {Mobile devices can be maliciously exploited to violate the privacy of people. In most attack scenarios, the adversary takes the local or remote control of the mobile device, by leveraging a vulnerability of the system, hence sending back the collected information to some remote web service. In this paper, we consider a different adversary, who does not interact actively with the mobile device, but he is able to eavesdrop the network traffic of the device from the network side (e.g., controlling a Wi-Fi access point). The fact that the network traffic is often encrypted makes the attack even more challenging. In this paper, we investigate to what extent such an external attacker can identify the specific actions that a user is performing on her mobile apps. We design a system that achieves this goal using advanced machine learning techniques. We built a complete implementation of this system, and we also run a thorough set of experiments, which show that our attack can achieve accuracy and precision higher than 95%, for most of the considered actions. We compared our solution with the three state-of-the-art algorithms, and confirming that our system outperforms all these direct competitors.},
  Doi                      = {10.1109/TIFS.2015.2478741},
  File                     = {Conti-2016-p114-125.pdf:References\\Conti-2016-p114-125.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {Web services;cryptography;data privacy;learning (artificial intelligence);mobile computing;smart phones;telecommunication traffic;Android encrypted network traffic;advanced machine learning technique;mobile application;mobile device;remote Web service;remote control;Cryptography;IP networks;Machine learning algorithms;Mobile communication;Mobile handsets;Privacy;Time series analysis;Cellular phones;information security;privacy},
  Reviewtime               = {178},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7265055}
}

@Article{Dai-2016-p410-425,
  Title                    = {Relay Broadcast Channel With Confidential Messages},
  Author                   = {B. Dai and L. Yu and Z. Ma},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {410-425},
  Volume                   = {11},

  Abstract                 = {In this paper, we investigate the effects of a trusted relay node on the secrecy of the broadcast channel by considering the model of relay broadcast channel with confidential messages (RBC-CM). Inner and outer bounds on the capacity-equivocation region of the RBC-CM are provided, and the capacity results are further explained via a degraded Gaussian example, which we call the degraded Gaussian relay broadcast channel with one common and one confidential messages. Numerical results show that this trusted relay node helps to enhance the security of the Gaussian broadcast channel with one common and one confidential messages.},
  Doi                      = {10.1109/TIFS.2015.2503259},
  File                     = {Dai-2016-p410-425.pdf:References\\Dai-2016-p410-425.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {Gaussian channels;broadcast channels;relay networks (telecommunication);telecommunication security;RBC-CM;capacity-equivocation region;confidential messages;degraded Gaussian relay broadcast channel;relay node;Mobile communication;Noise measurement;Receivers;Relays;Security;Transmitters;Capacity-equivocation region;confidential messages;relay broadcast channel;secrecy capacity region},
  Reviewtime               = {279},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7335625}
}

@Article{Dantcheva-2016-p441-467,
  Title                    = {What Else Does Your Biometric Data Reveal? A Survey on Soft Biometrics},
  Author                   = {A. Dantcheva and P. Elia and A. Ross},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {441-467},
  Volume                   = {11},

  Abstract                 = {Recent research has explored the possibility of extracting ancillary information from primary biometric traits viz., face, fingerprints, hand geometry, and iris. This ancillary information includes personal attributes, such as gender, age, ethnicity, hair color, height, weight, and so on. Such attributes are known as soft biometrics and have applications in surveillance and indexing biometric databases. These attributes can be used in a fusion framework to improve the matching accuracy of a primary biometric system (e.g., fusing face with gender information), or can be used to generate qualitative descriptions of an individual (e.g., young Asian female with dark eyes and brown hair). The latter is particularly useful in bridging the semantic gap between human and machine descriptions of the biometric data. In this paper, we provide an overview of soft biometrics and discuss some of the techniques that have been proposed to extract them from the image and the video data. We also introduce a taxonomy for organizing and classifying soft biometric attributes, and enumerate the strengths and limitations of these attributes in the context of an operational biometric system. Finally, we discuss open research problems in this field. This survey is intended for researchers and practitioners in the field of biometrics.},
  Doi                      = {10.1109/TIFS.2015.2480381},
  File                     = {Dantcheva-2016-p441-467.pdf:References\\Dantcheva-2016-p441-467.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {anthropometry;biometrics (access control);computer vision;age attribute;ancillary information extraction;ethnicity attribute;face information;fingerprint information;fusion framework;gender attribute;hair color attribute;hand geometry information;height attribute;human descriptions;image data extraction;iris information;machine descriptions;matching accuracy improvement;personal attributes;primary biometric traits;qualitative descriptions;soft biometric data;video data extraction;weight attribute;Accuracy;Bioinformatics;Data mining;Face;Feature extraction;Hair;Iris recognition;Age;Biometrics;Computer Vision;Cosmetics;Ethnicity;Gender;Privacy;Race;Semantics;Soft biometrics;Visual Attributes;age;biometrics;computer vision;cosmetics;ethnicity;gender;privacy;race;semantics;visual attributes},
  Reviewtime               = {100},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7273870}
}

@Article{Das-2016-p289-302,
  Title                    = {Semantics-Based Online Malware Detection: Towards Efficient Real-Time Protection Against Malware},
  Author                   = {S. Das and Y. Liu and W. Zhang and M. Chandramohan},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {289-302},
  Volume                   = {11},

  Abstract                 = {Recently, malware has increasingly become a critical threat to embedded systems, while the conventional software solutions, such as antivirus and patches, have not been so successful in defending the ever-evolving and advanced malicious programs. In this paper, we propose a hardware-enhanced architecture, GuardOL, to perform online malware detection. GuardOL is a combined approach using processor and field-programmable gate array (FPGA). Our approach aims to capture the malicious behavior (i.e., high-level semantics) of malware. To this end, we first propose the frequency-centric model for feature construction using system call patterns of known malware and benign samples. We then develop a machine learning approach (using multilayer perceptron) in FPGA to train classifier using these features. At runtime, the trained classifier is used to classify the unknown samples as malware or benign, with early prediction. The experimental results show that our solution can achieve high classification accuracy, fast detection, low power consumption, and flexibility for easy functionality upgrade to adapt to new malware samples. One of the main advantages of our design is the support of early prediction-detecting 46% of malware within first 30% of their execution, while 97% of the samples at 100% of their execution, with <;3% false positives.},
  Doi                      = {10.1109/TIFS.2015.2491300},
  File                     = {Das-2016-p289-302.pdf:References\\Das-2016-p289-302.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {computer architecture;embedded systems;field programmable gate arrays;invasive software;learning (artificial intelligence);multilayer perceptrons;pattern classification;FPGA;GuardOL;antivirus;classification accuracy;critical threat;embedded systems;feature construction;field-programmable gate array;frequency-centric model;hardware-enhanced architecture;high-level semantics;machine learning;malicious behavior;malicious programs;multilayer perceptron;patches;processor;real-time protection;semantics-based online malware detection;software solutions;system call patterns;trained classifier;Feature extraction;Field programmable gate arrays;Malware;Runtime;Semantics;Software;Training;Early Prediction;Hardware-enhanced Architecture;Malware Detection;Malware detection;Reconfigurable Malware Detection;Runtime Security;early prediction;hardware-enhanced architecture;reconfigurable malware detection;runtime security},
  Reviewtime               = {126},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7299317}
}

@Article{Daugm-2016-p400-409,
  Title                    = {Information Theory and the IrisCode},
  Author                   = {J. Daugman},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {400-409},
  Volume                   = {11},

  Abstract                 = {Iris recognition has legendary resistance to false matches, and the tools of information theory can help to explain why. The concept of entropy is fundamental to understanding biometric collision avoidance. This paper analyses the bit sequences of IrisCodes computed both from real iris images and from synthetic white noise iris images, whose pixel values are random and uncorrelated. The capacity of the IrisCode as a channel is found to be 0.566 bits per bit encoded, of which 0.469 bits of entropy per bit is encoded from natural iris images. The difference between these two rates reflects the existence of anatomical correlations within a natural iris, and the remaining gap from one full bit of entropy per bit encoded reflects the correlations in both phase and amplitude introduced by the Gabor wavelets underlying the IrisCode. A simple two-state hidden Markov model is shown to emulate exactly the statistics of bit sequences generated both from natural and white noise iris images, including their imposter distributions, and may be useful for generating large synthetic IrisCode databases.},
  Doi                      = {10.1109/TIFS.2015.2500196},
  File                     = {Daugm-2016-p400-409.pdf:References\\Daugm-2016-p400-409.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {entropy;hidden Markov models;image sequences;iris recognition;wavelet transforms;white noise;Gabor wavelets;IrisCode bit sequences;biometric collision avoidance;entropy;information theory;iris recognition;random uncorrelated pixel values;two-state hidden Markov model;white noise iris images;Entropy;Image coding;Iris recognition;Mutual information;Random variables;Uncertainty;Entropy;Hidden Markov Models;IrisCode;hidden Markov models},
  Reviewtime               = {174},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7328287}
}

@Article{David-2016-p59-73,
  Title                    = {Unconditionally Secure, Universally Composable Privacy Preserving Linear Algebra},
  Author                   = {B. David and R. Dowsley and J. van de Graaf and D. Marques and A. C. A. Nascimento and A. C. B. Pinto},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {59-73},
  Volume                   = {11},

  Abstract                 = {Linear algebra operations on private distributed data are frequently required in several practical scenarios (e.g., statistical analysis and privacy preserving databases). We present universally composable two-party protocols to compute inner products, determinants, eigenvalues, and eigenvectors. These protocols are built for a two-party scenario where the inputs are provided by mutually distrustful parties. After execution, the protocols yield the results of the intended operation while preserving the privacy of their inputs. Universal composability is obtained in the trusted initializer model, ensuring information theoretical security under arbitrary protocol composition in complex environments. Furthermore, our protocols are computationally efficient since they only require field multiplication and addition operations.},
  Doi                      = {10.1109/TIFS.2015.2476783},
  File                     = {David-2016-p59-73.pdf:References\\David-2016-p59-73.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptographic protocols;data privacy;eigenvalues and eigenfunctions;linear algebra;arbitrary protocol composition;data privacy;eigenvalue;eigenvector;linear algebra operation;private distributed data;theoretical information security;trusted initializer model;Computational modeling;Cryptography;Eigenvalues and eigenfunctions;Linear algebra;Privacy;Protocols;Linear Algebra;Privacy Preserving;Secure Computation;Trusted Initializer Model;UC security;linear algebra;privacy preserving;secure computation;trusted initializer model},
  Reviewtime               = {220},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7239604}
}

@Article{Deng-2016-p1128-1138,
  Title                    = {Physical Layer Security in Three-Tier Wireless Sensor Networks: A Stochastic Geometry Approach},
  Author                   = {Y. Deng and L. Wang and M. Elkashlan and A. Nallanathan and R. K. Mallik},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1128-1138},
  Volume                   = {11},

  Abstract                 = {This paper develops a tractable framework for exploiting the potential benefits of physical layer security in three-tier wireless sensor networks (WSNs) using stochastic geometry. In such networks, the sensing data from the remote sensors are collected by sinks with the help of access points, and the external eavesdroppers intercept the data transmissions. We focus on the secure transmission in two scenarios: 1) the active sensors transmit their sensing data to the access points and 2) the active access points forward the data to the sinks. We derive new compact expressions for the average secrecy rate in these two scenarios. We also derive a new compact expression for the overall average secrecy rate. Numerical results corroborate our analysis and show that multiple antennas at the access points can enhance the security of three-tier WSNs. Our results show that increasing the number of access points decreases the average secrecy rate between the access point and its associated sink. However, we find that increasing the number of access points first increases the overall average secrecy rate, with a critical value beyond which the overall average secrecy rate then decreases. When increasing the number of active sensors, both the average secrecy rate between the sensor and its associated access point, and the overall average secrecy rate decrease. In contrast, increasing the number of sinks improves both the average secrecy rate between the access point and its associated sink, and the overall average secrecy rate.},
  Doi                      = {10.1109/TIFS.2016.2516917},
  File                     = {Deng-2016-p1128-1138.pdf:References\\Deng-2016-p1128-1138.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {antenna arrays;decode and forward communication;geometry;stochastic processes;wireless sensor networks;beamforming;data transmissions;decode-and-forward;multiple antennas;physical layer security;remote sensors;stochastic geometry approach;three-tier wireless sensor networks;Ad hoc networks;Geometry;Physical layer;Relays;Security;Sensors;Wireless sensor networks;Beamforming;decode-and-forward (DF);physical layer security;stochastic geometry;wireless sensor networks (WSNs)},
  Reviewtime               = {249},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7386675}
}

@Article{Diyanat-2016-p1321-1332,
  Title                    = {A Dummy-Based Approach for Preserving Source Rate Privacy},
  Author                   = {A. Diyanat and A. Khonsari and S. P. Shariatpanahi},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1321-1332},
  Volume                   = {11},

  Abstract                 = {Recent studies reveal that an adversary might trace the apparently insignificant traffic rate of source nodes over the net and turn such data to invaluable information so as to breach the privacy of the victim sources. Inhibiting the adversary of being able to extract information from the traffic rate of source nodes is a complicated task unless taking into consideration the flow conservation law effect of the transmitter queue. A reliable method of preserving the rate privacy that copes with the flow conservation law is to transmit original packets augmented with probabilistically dummy ones so as to change the observable aggregated traffic rate. Augmenting dummy packets, however, bears redundancy, and hence, requires extra resources in terms of bandwidth and buffer requirements, and more importantly suggests higher transmitting energy consumption. Grounded on the queueing and information theories, in this paper, we present an efficient method that minimally augments dummy packets to preserve the source rate privacy at a given degree while preserving the delay distribution of the original packets intact, and thus does not affect the quality of service parameters of the transmitted data in terms of delay and jitter. The presented method models the original packets and dummy ones with a preemptive resume 2-priority queueing system and then using information theory attempts to maximize the Fano lower bound of the best estimation of the adversary's speculation. All of the theoretically obtained results have been validated by conducting simulation experiments.},
  Doi                      = {10.1109/TIFS.2016.2515050},
  File                     = {Diyanat-2016-p1321-1332.pdf:References\\Diyanat-2016-p1321-1332.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {computer network security;data privacy;information theory;queueing theory;Fano lower bound;bandwidth requirements;buffer requirements;dummy-based approach;flow conservation law;flow conservation law effect;information theories;preemptive resume 2-priority queueing system;privacy breach;probabilistically dummy packets;queueing theories;source nodes;source rate privacy;transmitter queue;victim sources;Data privacy;Delays;Estimation;Monitoring;Privacy;Processor scheduling;Queueing analysis;Dummy packets;Fanoâ€™s inequality;Fano???s inequality;Flow conservation law;flow conservation law;preemptive resume 2-priority queue;rate privacy},
  Reviewtime               = {220},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7373646}
}

@Article{Dubey-2016-p1461-1475,
  Title                    = {Fingerprint Liveness Detection From Single Image Using Low-Level Features and Shape Analysis},
  Author                   = {R. K. Dubey and J. Goh and V. L. L. Thing},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1461-1475},
  Volume                   = {11},

  Abstract                 = {Fingerprint-based authentication systems have developed rapidly in the recent years. However, current fingerprint-based biometric systems are vulnerable to spoofing attacks. Moreover, single feature-based static approach does not perform equally over different fingerprint sensors and spoofing materials. In this paper, we propose a static software approach. We propose to combine low-level gradient features from speeded-up robust features, pyramid extension of the histograms of oriented gradient and texture features from Gabor wavelet using dynamic score level integration. We extract these features from a single fingerprint image to overcome the issues faced in dynamic software approaches, which require user cooperation and longer computational time. A experimental analysis done on LivDet 2011 data produced an average equal error rate (EER) of 3.95% over four databases. The result outperforms the existing best average EER of 9.625%. We also performed experiments with LivDet 2013 database and achieved an average classification error rate of 2.27% in comparison with 12.87% obtained by the LivDet 2013 competition winner.},
  Doi                      = {10.1109/TIFS.2016.2535899},
  File                     = {Dubey-2016-p1461-1475.pdf:References\\Dubey-2016-p1461-1475.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {feature extraction;fingerprint identification;image texture;visual databases;wavelet transforms;EER;Gabor wavelet;LivDet 2011 data;LivDet 2013 competition winner;LivDet 2013 database;average equal error rate;dynamic score level integration;dynamic software approach;feature extraction;feature-based static approach;fingerprint image;fingerprint liveness detection;fingerprint-based authentication system;fingerprint-based biometric systems;histograms of oriented gradient pyramid extension;low-level features;low-level gradient features;shape analysis;single image;speeded-up robust features;spoofing attacks;static software approach;texture features;Authentication;Databases;Feature extraction;Hardware;Sensors;Skin;Software;Fingerprint liveness;Gabor filters;fingerprint liveness;low level features},
  Reviewtime               = {365},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7422065}
}

@Article{Duda-2016-p691-703,
  Title                    = {Image-Like 2D Barcodes Using Generalizations of the Kuznetsov - Tsybakov Problem},
  Author                   = {J. Duda and P. Korus and N. J. Gadgil and K. Tahboub and E. J. Delp},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {691-703},
  Volume                   = {11},

  Abstract                 = {In this paper, we propose a novel method for generating visually appealing two-dimensional (2D) barcodes that resemble meaningful images to human observers. The technology of 2D barcodes, currently dominated by quick response codes, is widely adopted in many applications, including product tracking, document management, and general marketing. Such barcodes typically lack user friendly appearance and do not convey any visual significance to human observers. The proposed method addresses this problem by allowing 2D barcodes to resemble an arbitrary image or a logo. Our method is based on a generalization of the Kuznetsov-Tsybakov problem that served as a foundation for wet paper codes, commonly adopted in digital steganography. We introduce weaker statistical constraints to obtain additional flexibility allowing the barcode to assume the appearance of an arbitrary pattern. This paper provides the theoretical analysis of the proposed coding framework and a practical algorithm for rapid approximation of the optimal code. We also discuss the introduction of error correction capabilities, and experimentally evaluate a prototype implementation in a smartphone-based acquisition scenario.},
  Doi                      = {10.1109/TIFS.2015.2506002},
  File                     = {Duda-2016-p691-703.pdf:References\\Duda-2016-p691-703.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {QR codes;bar codes;image processing;statistical analysis;steganography;Kuznetsov-Tsybakov problem;digital steganography;image-like 2D barcodes;rapid approximation;smartphone-based acquisition scenario;statistical constraints;wet paper codes;Encoding;Error correction;Error correction codes;Image coding;Payloads;Receivers;Kuznetsov-Tsybakov problem;QR codes;Two dimensional (2D) barcodes;steganography;wet-paper channel},
  Reviewtime               = {196},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7348695}
}

@Article{Fang-2016-p1515-1527,
  Title                    = {Mimicry Attacks Against Wireless Link Signature and New Defense Using Time-Synched Link Signature},
  Author                   = {S. Fang and Y. Liu and P. Ning},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1515-1527},
  Volume                   = {11},

  Abstract                 = {Wireless link signature is a physical layer authentication mechanism, using the multipath effect between a transmitter and a receiver to provide authentication of wireless signals. This paper identifies a new attack, called mimicry attack, against the existing wireless link signature schemes. An attacker can forge a legitimate transmitter's link signature as long as it knows the legitimate signal at the receiver's location, and the attacker does not have to be at exactly the same location as the legitimate transmitter. We also extend the mimicry attack to multiple-input multiple-output (MIMO) systems, and conclude that the mimicry attack is feasible only when the number of attacker' antennas is equal to or larger than that of the receiver's antennas. To defend against the mimicry attack, this paper proposes a novel construction for wireless link signature, called time-synched link signature, by integrating cryptographic protection and time factor into wireless physical layer features. Experimental results confirm that the mimicry attack is a real threat and the newly proposed time-synched link signatures are effective in physical layer authentication.},
  Doi                      = {10.1109/TIFS.2016.2541307},
  File                     = {Fang-2016-p1515-1527.pdf:References\\Fang-2016-p1515-1527.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {MIMO communication;cryptography;data protection;digital signatures;MIMO systems;cryptographic protection;legitimate signal;legitimate transmitter;mimicry attacks;multipath effect;multiple-input multiple-output systems;physical layer authentication mechanism;time factor;time-synched link signature;wireless link signature schemes;wireless signals;Authentication;Channel estimation;Communication system security;Physical layer;Receivers;Transmitters;Wireless communication;Link signature;MIMO;Time-synched;time-synched},
  Reviewtime               = {166},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7432029}
}

@Article{Gao-2016-p429-430,
  Title                    = {Rebuttal to Comments on Joint Global and Local Structure Discriminant Analysis},
  Author                   = {Q. Gao},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {429-430},
  Volume                   = {11},

  Abstract                 = {In the above paper, the authors pointed out that our motivation was flawed in our previous paper, and provided some comments. In this paper, we point out the inexact understanding and representation in the comment paper and then present a detail explanation for our previous paper.},
  Doi                      = {10.1109/TIFS.2015.2490622},
  File                     = {Gao-2016-p429-430.pdf:References\\Gao-2016-p429-430.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {data reduction;learning (artificial intelligence);dimensionality reduction;global structure discriminant analysis;image recognition;local structure discriminant analysis;locality preserving projection;manifold learning;Intserv networks;Joints;Laplace equations;Linear programming;Principal component analysis;Robustness;Topology;Discriminant analysis;dimensionality reduction;image recognition},
  Reviewtime               = {66},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7298415}
}

@Article{Garnaev-2016-p837-849,
  Title                    = {A Bandwidth Monitoring Strategy Under Uncertainty of the Adversaryâ€˜s Activity},
  Author                   = {A. Garnaev and W. Trappe},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {837-849},
  Volume                   = {11},

  Abstract                 = {When an adversary illicitly uses spectrum that it is not authorized for, it does so with a purpose in mind, such as to download a file or perhaps engage in a real-time communication session. In this paper, we examine how the incorporation of knowledge related to an adversary's purpose can improve the effectiveness of spectrum scanning protocols. First, we study the difference in the thief's behavior when considering throughput and delay as the two primary QoS parameters he is concerned with. Through our analysis, we show that the detection probability of unlicensed access to spectrum resources depends on the application type. Knowledge of the application type can be incorporated to spectrum scanning to tune better it to detect the thief. To illustrate this, we examine two Bayesian games. In the first game, the scanner wants to minimize the time needed to detect the invader. In the second game, the scanner wants to maximize the detection probability at each time slot by adapting its belief regarding the adversary's activity. In particular, it is shown in the minimizing detection time game that the equilibrium strategies are continuous with respect to priori knowledge of the invader's activity. Meanwhile, for the maximizing detection probability game, the strategies can have a jump discontinuity. This phenomena can be explained as the difference between tactical and strategic decision making: tactical decision making allows short-term, unpredictable moves, while strategic decision making is inclined to predictable moves. Finally, since the bandwidth model used in this paper is general, the conclusion as well as the approach provided can be applied to a variety of different network protection problems.},
  Doi                      = {10.1109/TIFS.2015.2510959},
  File                     = {Garnaev-2016-p837-849.pdf:References\\Garnaev-2016-p837-849.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {Bayes methods;game theory;probability;protocols;radio spectrum management;Bayesian game theory;QoS;bandwidth monitoring strategy;decision making;detection probability game;network protection problem;spectrum scanning protocol;Delays;Games;Interference;Jamming;Security;Signal to noise ratio;Throughput;Bayes methods;Intrusion detection;wireless networks},
  Reviewtime               = {224},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7362001}
}

@Article{Giaretta-2016-p665-676,
  Title                    = {Security Vulnerabilities and Countermeasures for Target Localization in Bio-NanoThings Communication Networks},
  Author                   = {A. Giaretta and S. Balasubramaniam and M. Conti},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {665-676},
  Volume                   = {11},

  Abstract                 = {The emergence of molecular communication has provided an avenue for developing biological nanonetworks. Synthetic biology is a platform that enables reprogramming cells, which we refer to as Bio-NanoThings, that can be assembled to create nanonetworks. In this paper, we focus on specific Bio-NanoThings, i.e, bacteria, where engineering their ability to emit or sense molecules can result in functionalities, such as cooperative target localization. Although this opens opportunities, e.g., for novel healthcare applications of the future, this can also lead to new problems, such as a new form of bioterrorism. In this paper, we investigate the disruptions that malicious Bio-NanoThings (M-BNTs) can create for molecular nanonetworks. In particular, we introduce two types of attacks: blackhole and sentry attacks. In blackhole attack M-BNTs emit attractant chemicals to draw-in the legitimate Bio-NanoThings (L-BNTs) from searching for their target, while in the sentry attack, the M-BNTs emit repellents to disperse the L-BNTs from reaching their target. We also present a countermeasure that L-BNTs can take to be resilient to the attacks, where we consider two forms of decision processes that includes Bayes' rule as well as a simple threshold approach. We run a thorough set of simulations to assess the effectiveness of the proposed attacks as well as the proposed countermeasure. Our results show that the attacks can significantly hinder the regular behavior of Bio-NanoThings, while the countermeasures are effective for protecting against such attacks.},
  Doi                      = {10.1109/TIFS.2015.2505632},
  File                     = {Giaretta-2016-p665-676.pdf:References\\Giaretta-2016-p665-676.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {molecular communication (telecommunication);telecommunication security;Bayes rule;L-BNT;M-BNT;assembling;bacteria;biological nanonetwork;bionanothings communication network;bioterrorism;blackhole attack;cooperative target localization;decision processes;healthcare application;legitimate bionanothing;malicious bionanothing;molecular communication;molecular nanonetwork;reprogramming cell;security vulnerability;sentry attack;synthetic biology;Biological system modeling;Bioterrorism;Chemicals;Microorganisms;Molecular communication;Nanobioscience;Bioterrorism;Internet of Nano Things;Molecular Communication;Molecular communication;Security;bioterrorism;security},
  Reviewtime               = {88},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7347397}
}

@Article{Guo-2016-p247-257,
  Title                    = {Distance-Based Encryption: How to Embed Fuzziness in Biometric-Based Encryption},
  Author                   = {F. Guo and W. Susilo and Y. Mu},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {247-257},
  Volume                   = {11},

  Abstract                 = {We introduce a new encryption notion called distance-based encryption (DBE) to apply biometrics in identity-based encryption. In this notion, a ciphertext encrypted with a vector and a threshold value can be decrypted with a private key of another vector, if and only if the distance between these two vectors is less than or equal to the threshold value. The adopted distance measurement is called Mahalanobis distance, which is a generalization of Euclidean distance. This novel distance is a useful recognition approach in the pattern recognition and image processing community. The primary application of this new encryption notion is to incorporate biometric identities, such as face, as the public identity in an identity-based encryption. In such an application, usually the input biometric identity associated with a private key will not be exactly the same as the input biometric identity in the encryption phase, even though they are from the same user. The introduced DBE addresses this problem well as the decryption condition does not require identities to be identical but having small distance. The closest encryption notion to DBE is the fuzzy identity-based encryption, but it measures biometric identities using a different distance called an overlap distance (a variant of Hamming distance) that is not widely accepted by the pattern recognition community, due to its long binary representations. In this paper, we study this new encryption notion and its constructions. We show how to generically and efficiently construct such a DBE from an inner product encryption (IPE) with reasonable size of private keys and ciphertexts. We also propose a new IPE scheme with the shortest private key to build DBE, namely, the need for a short private key. Finally, we study the encryption efficiency of DBE by splitting our IPE encryption algorithm into offline and online algorithms.},
  Doi                      = {10.1109/TIFS.2015.2489179},
  File                     = {Guo-2016-p247-257.pdf:References\\Guo-2016-p247-257.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {biometrics (access control);fuzzy set theory;private key cryptography;public key cryptography;DBE;Euclidean distance;Hamming distance;IPE encryption algorithm;Mahalanobis distance;binary representations;biometric identity;biometric-based encryption;ciphertext;decryption;distance measurement;distance-based encryption;fuzziness;fuzzy identity-based encryption;image processing community;inner product encryption;pattern recognition;public identity;shortest private key;threshold value;vector;Encryption;Euclidean distance;Feature extraction;Pattern recognition;Biometrics;Euclidean Distance;Euclidean distance;Identity-based Encryption;Identity-based encryption;Mahalanobis Distance;Mahalanobis distance;Pattern Recognition;biometrics;pattern recognition},
  Reviewtime               = {379},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7295612}
}

@Article{Hamadene-2016-p1226-1238,
  Title                    = {One-Class Writer-Independent Offline Signature Verification Using Feature Dissimilarity Thresholding},
  Author                   = {A. Hamadene and Y. Chibani},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1226-1238},
  Volume                   = {11},

  Abstract                 = {Usual handwritten signature verification systems address the writer-independent (WI) approach using only bi-class robust classifiers to deal with the most challenging tasks. Indeed, WI concept, the reduced size of references and one-class signature verification are still open issues in practical cases. In this paper, we propose a one-class WI system using feature dissimilarity measures thresholding for classification and a reduced number of references. The proposed system involves the use of contourlet transform-based directional code co-occurrence matrix feature generation method. The verification is achieved through a WI threshold which is automatically selected using a new signature stability criterion. The proposed WI concept is besides addressed through the mixture of different writers' data sets in both the design and verification stages. Experimental results show the effectiveness of the proposed system in spite of the strict verification protocol using the one-class concept, a unique threshold for accepting or rejecting a questioned signature, the reduced number of writers, and the limited number of reference signatures.},
  Doi                      = {10.1109/TIFS.2016.2521611},
  File                     = {Hamadene-2016-p1226-1238.pdf:References\\Hamadene-2016-p1226-1238.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {feature extraction;handwriting recognition;image classification;matrix algebra;transforms;bi-class robust classifier;classification;contourlet transform-based directional code co-occurrence matrix feature generation method;design stage;feature dissimilarity measures thresholding;handwritten signature verification systems;one-class WI system;one-class writer-independent offline signature verification;reference signatures;signature stability criterion;verification protocol;verification stage;writer data sets;Computed tomography;Forgery;Robustness;Stability criteria;Transforms;Contourlet transform;directional code co-occurrence matrix;directional code cooccurrence matrix;feature dissimilarity measure;offline handwritten signature;one-class verification;signature stability;writer-independent},
  Reviewtime               = {208},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7390262}
}

@Article{Han-2016-p258-272,
  Title                    = {Regional Patterns and Vulnerability Analysis of Chinese Web Passwords},
  Author                   = {W. Han and Z. Li and L. Yuan and W. Xu},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {258-272},
  Volume                   = {11},

  Abstract                 = {Current research on password security pays much attention on users who speak Indo-European languages (English, Spanish, and so on), and thus the countermeasures are heavily influenced by Indo-European speakers' choices as well. However, languages have a strong impact on passwords. Analysis without considering other languages (e.g., Chinese) might lead to some biased results, such as Chinese passwords are one of the most difficult ones to guess. We believe that such a conclusion could be biased because, to the best of our knowledge, little empirical study has examined the regional differences of passwords at a large scale, especially on Chinese passwords. In this paper, we comprehensively study the differences between passwords from Chinese and English-dominant users, leveraging over 100 million leaked and publicly available passwords from Chinese and international websites in recent years. We find that Chinese prefer digits when composing their passwords, while English-dominant users prefer letters, especially lowercase letters. However, their strength against password guessing is similar. Second, we observe that both groups of users prefer to use the patterns that they are familiar with, e.g., Chinese Pinyins for Chinese and English words for English-dominant users. In particular, since multiple input methods require various sequences of letters to enter the same Chinese characters, we evaluate the impacts of various Chinese input methods, in addition to Pinyin. Third, we observe that both Chinese and English-dominant users prefer their conventional format when they use dates to construct passwords. Based on these observations, we improve two password guessing methods: 1) probabilistic context-free grammar (PCFG)-based password guessing method and 2) Markov model-based password guessing method. For the PCFG-based method, the guessing efficiency increases by up to 48% after inserting Pinyins (about 2.3% more entries) into the attack dictionary and inserting the ob- erved composition rules into the guessing rule set. For the Markov-model-based method, the guessing efficiency increases by up to 4.7% after we increase the percentage of Pinyins in the training set. Our research sheds light on understanding the impact of regional patterns on passwords.},
  Doi                      = {10.1109/TIFS.2015.2490620},
  File                     = {Han-2016-p258-272.pdf:References\\Han-2016-p258-272.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {Internet;Markov processes;context-free grammars;probability;security of data;Chinese Pinyins;Chinese Web passwords;Chinese input methods;Indo-European languages;Markov model-based password guessing method;PCFG-based password guessing method;password regional differences;probabilistic context-free grammar;regional patterns;vulnerability analysis;Internet;Keyboards;Markov processes;Probabilistic logic;Security;Training;Authentication;Markov Model;Markov model;PCFG;Password;Password PatternsChinese Users;chinese users;password;password patterns},
  Reviewtime               = {210},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7298428}
}

@Article{Han-2016-p556-570,
  Title                    = {A Game Theoretical Approach to Defend Against Co-Resident Attacks in Cloud Computing: Preventing Co-Residence Using Semi-Supervised Learning},
  Author                   = {Y. Han and T. Alpcan and J. Chan and C. Leckie and B. I. P. Rubinstein},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {556-570},
  Volume                   = {11},

  Abstract                 = {While cloud computing has facilitated easy and affordable access to IT resources, it has also introduced a wide range of security risks from almost every layer and component of cloud systems. In this paper, we focus on one risk at the virtual machine level and the co-resident attack, where by constructing various types of side channels, malicious users can obtain sensitive information from other virtual machines that co-locate on the same physical server. Most previous work has focused on the elimination of side channels, or more generally speaking, the possible countermeasures after attackers co-locate with their targets. In contrast, we provide a different perspective, and propose a defence mechanism that makes it difficult and expensive for attackers to achieve co-residence in the first place. Specifically, we first identify the potential differences between the behaviors of attackers and legal users. Second, we apply clustering analysis and semi-supervised learning techniques to classify users. Third, we model the problem as a two-player security game, and give a detailed analysis of the optimum strategies for both players. Finally, we demonstrate that the attacker's overall cost is increased dramatically by one-to-two orders of magnitude as a result of our defence mechanism.},
  Doi                      = {10.1109/TIFS.2015.2505680},
  File                     = {Han-2016-p556-570.pdf:References\\Han-2016-p556-570.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cloud computing;cryptography;game theory;learning (artificial intelligence);virtual machines;cloud computing;clustering analysis;co-resident attack;game theoretical approach;security game;semisupervised learning technique;virtual machine level;Australia;Cloud computing;Electronic mail;Games;Resource management;Security;Servers;Cloud computing security;cloud computing security;co-resident attack;security game;semi-supervised learning},
  Reviewtime               = {255},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7347426}
}

@Article{Hu-2016-p388-399,
  Title                    = {Optimal Coding and Allocation for Perfect Secrecy in Multiple Clouds},
  Author                   = {P. Hu and C. W. Sung and S. W. Ho and T. H. Chan},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {388-399},
  Volume                   = {11},

  Abstract                 = {For a user to store data in the cloud, using services provided by multiple cloud storage providers (CSPs) is a promising approach to increase the level of data availability and confidentiality, as it is unlikely that different CSPs are out of service at the same time or collude with each other to extract information of a user. This paper investigates the problem of storing data reliably and securely in multiple CSPs constrained by given budgets with minimum cost. Previous works, with variations in problem formulations, typically tackle the problem by decoupling it into sub-problems and solve them separately. While such a decoupling approach is simple, the resultant solution is suboptimal. This paper is the first one which considers the problem as a whole and derives a jointly optimal coding and storage allocation scheme, which achieves perfect secrecy with minimum cost. The analytical result reveals that the optimal coding scheme is the nested maximum-distance-separable code and the optimal amount of data to be stored in the CSPs exhibits a certain structure. The exact parameters of the code and the exact storage amount to each CSP can be determined numerically by simple 2-D search.},
  Doi                      = {10.1109/TIFS.2015.2500193},
  File                     = {Hu-2016-p388-399.pdf:References\\Hu-2016-p388-399.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cloud computing;data privacy;security of data;storage management;2D search;confidentiality;decoupling approach;multiple CSP;multiple cloud storage providers;multiple clouds;nested maximum-distance-separable code;optimal coding;perfect secrecy;storage allocation scheme;Algorithm design and analysis;Cloud computing;Encoding;Peer-to-peer computing;Resource management;Security;Servers;Cloud storage;information-theoretic security;perfect secrecy;storage allocation},
  Reviewtime               = {214},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7328302}
}

@Article{Hu-2016-p1549-1564,
  Title                    = {Signal-Level Information Fusion for Less Constrained Iris Recognition Using Sparse-Error Low Rank Matrix Factorization},
  Author                   = {Y. Hu and K. Sirlantzis and G. Howells},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1549-1564},
  Volume                   = {11},

  Abstract                 = {Iris recognition systems working in less constrained environments with the subject at-a-distance and on-the-move suffer from the noise and degradations in the iris captures. These noise and degradations significantly deteriorate iris recognition performance. In this paper, we propose a novel signal-level information fusion method to mitigate the influence of noise and degradations for less constrained iris recognition systems. The proposed method is based on low rank approximation (LRA). Given multiple noisy captures of the same eye, we assume that: 1) the potential noiseless images lie in a low rank subspace and 2) the noise is spatially sparse. Based on these assumptions, we seek an LRA of noisy captures to separate the noiseless images and noise for information fusion. Specifically, we propose a sparse-error low rank matrix factorization model to perform LRA, decomposing the noisy captures into a low rank component and a sparse error component. The low rank component estimates the potential noiseless images, while the error component models the noise. Then, the low rank and error components are utilized to perform signal-level fusion separately, producing two individually fused images. Finally, we combine the two fused images at the code level to produce one iris code as the final fusion result. Experiments on benchmark data sets demonstrate that the proposed signal-level fusion method is able to achieve a generally improved iris recognition performance in less constrained environment, in comparison with the existing iris recognition algorithms, especially for the iris captures with heavy noise and low quality.},
  Doi                      = {10.1109/TIFS.2016.2541612},
  File                     = {Hu-2016-p1549-1564.pdf:References\\Hu-2016-p1549-1564.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {approximation theory;image fusion;iris recognition;matrix decomposition;sparse matrices;LRA;error component models;iris captures;iris code;iris recognition systems;low rank approximation;noiseless images;signal-level information fusion method;sparse-error low rank matrix factorization model;Approximation algorithms;Degradation;Image resolution;Iris recognition;Noise measurement;Signal resolution;Sparse matrices;Iris recognition;information fusion;iris recognition;less constrained environment},
  Reviewtime               = {232},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7433420}
}

@Article{Hua-2016-p1003-1016,
  Title                    = {Audio Authentication by Exploring the Absolute-Error-Map of ENF Signals},
  Author                   = {G. Hua and Y. Zhang and J. Goh and V. L. L. Thing},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {1003-1016},
  Volume                   = {11},

  Abstract                 = {Recently, the electric network frequency (ENF), a natural signature embedded in many audio recordings, has been utilized as a criterion to examine the authenticity of audio recordings. ENF-based audio authentication system involves extraction of the ENF signal from a questioned audio recording, and matching it with the reference signal stored in an ENF database. This establishes a popular application of audio timestamp verification. In this paper, we explore another important application, i.e., ENF-based audio tampering detection, which has received less research attention. Specifically, we introduce the absolute-error-map (AEM) between the ENF signals obtained from the testing audio recording and the database. The AEM serves as an ensemble of the raw data associated with the ENF matching process. Through intensive analysis of the AEM, we propose two algorithms to jointly deal with timestamp verification and tampering detection, including insertion, deletion, and splicing attacks, respectively. The first algorithm is based on exhaustive point search and measurement, while the second algorithm leverages the image erosion technique to achieve fast detection of tampering type and tampered region, thus the second algorithm sacrifices some accuracy for speed. The authentication mechanism is that the system first determines if the testing data have been tampered with, and then outputs the timestamp information if no tampering is detected. Otherwise, it outputs the tampering type and tampered region. We demonstrate the effectiveness of the proposed solution via both synthetic and practical examples from our practically deployed audio authentication system.},
  Doi                      = {10.1109/TIFS.2016.2516824},
  File                     = {Hua-2016-p1003-1016.pdf:References\\Hua-2016-p1003-1016.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {audio recording;search problems;signal detection;AEM;ENF matching process;ENF signals;ENF-based audio tampering detection;absolute-error-map;audio authentication;audio recordings;audio timestamp verification;electric network frequency;exhaustive point search;image erosion technique;Audio recording;Authentication;Data mining;Databases;Splicing;Testing;Time-frequency analysis;Electric network frequency (ENF);audio authentication;audio forensics;audio tampering detection},
  Reviewtime               = {117},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7378470}
}

@Article{Huo-2016-p1528-1541,
  Title                    = {Analysis and Validation of Active Eavesdropping Attacks in Passive FHSS RFID Systems},
  Author                   = {F. Huo and P. Mitran and G. Gong},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1528-1541},
  Volume                   = {11},

  Abstract                 = {In this paper, we present a generalized framework for active eavesdropping in a frequency hopping spread spectrum passive radio frequency identification system. In our model, there exists an adversarial reader who is able to transmit its own continuous wave signal outside the frequency band of the legitimate reader. Due to the fact that under backscatter modulation, the tag cannot distinguish different frequencies and simply sets the impedance in its circuitry to either low or high to reflect a bit of 1 or 0, and the adversarial reader's received signal is a weighted sum of the response to both its own signal and the legitimate reader's signal. Using this model, we provide a theoretical analysis of the capability of the adversarial reader in terms of the decoding error probability for slow frequency and fast frequency hopping systems. We derive analytic formulas and conduct experiments using software defined radios that act as the legitimate reader, the adversarial reader, and Intel Wireless Identification Sensing Platform tags with parameters as specified in EPC Gen2. Simulations are also used to validate our findings. We find from the theoretical analysis as well the experimental results that the active eavesdropper can achieve a better decoding error rate than a conventional passive eavesdropper, even in the case that the eavesdropper's signal is a low power signal.},
  Doi                      = {10.1109/TIFS.2016.2541309},
  File                     = {Huo-2016-p1528-1541.pdf:References\\Huo-2016-p1528-1541.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {decoding;error statistics;radiofrequency identification;active eavesdropping attacks;analytic formulas;backscatter modulation;continuous wave signal;decoding error probability;frequency hopping spread spectrum passive radio frequency identification system;generalized framework;intel wireless identification sensing platform tags;legitimate reader;passive FHSS RFID systems;Backscatter;Decoding;Error probability;Frequency modulation;Passive RFID tags;RFID;SFH and FFH spread spectrum;active eavesdropping attack;error probability;security analysis},
  Reviewtime               = {98},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7432027}
}

@Article{Ji-2016-p1398-1411,
  Title                    = {Seed-Based De-Anonymizability Quantification of Social Networks},
  Author                   = {S. Ji and W. Li and N. Z. Gong and P. Mittal and R. Beyah},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1398-1411},
  Volume                   = {11},

  Abstract                 = {In this paper, we implement the first comprehensive quantification of the perfect de-anonymizability and partial de-anonymizability of real-world social networks with seed information under general scenarios, which provides the theoretical foundation for the existing structure-based de-anonymization attacks and closes the gap between de-anonymization practice and theory. Based on our quantification, we conduct a large-scale evaluation of the de-anonymizability of 24 real-world social networks by quantitatively showing the conditions for perfectly and partially de-anonymizing a social network, how de-anonymizable a social network is, and how many users of a social network can be successfully de-anonymized. Furthermore, we show that both theoretically and experimentally, the overall structural information-based de-anonymization attack can be more powerful than the seed-based de-anonymization attack, and even without any seed information, a social network can be perfectly or partially de-anonymized. Finally, we discuss the implications of this paper. Our findings are expected to shed on research questions in the areas of structural data anonymization and de-anonymization and to help data owners evaluate their structural data vulnerability before data sharing and publishing.},
  Doi                      = {10.1109/TIFS.2016.2529591},
  File                     = {Ji-2016-p1398-1411.pdf:References\\Ji-2016-p1398-1411.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {security of data;social networking (online);seed information;seed-based de-anonymizability quantification;seed-based de-anonymization attack;social networks;structural data anonymization;structural data vulnerability;structural information-based de-anonymization attack;Advertising;Data models;Data privacy;Erbium;Forensics;Security;Social network services;De-anonymization;evaluation;quantification;social networks},
  Reviewtime               = {251},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7405301}
}

@Article{Jin-2016-p1306-1320,
  Title                    = {MagPairing: Pairing Smartphones in Close Proximity Using Magnetometers},
  Author                   = {R. Jin and L. Shi and K. Zeng and A. Pande and P. Mohapatra},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1306-1320},
  Volume                   = {11},

  Abstract                 = {With the prevalence of mobile computing, lots of wireless devices need to establish secure communication on the fly without pre-shared secrets. Device pairing is critical for bootstrapping secure communication between two previously unassociated devices over the wireless channel. Using auxiliary out-of-band channels involving visual, acoustic, tactile, or vibrational sensors has been proposed as a feasible option to facilitate device pairing. However, these methods usually require users to perform additional tasks, such as copying, comparing, and shaking. It is preferable to have a natural and intuitive pairing method with minimal user tasks. In this paper, we introduce a new method, called MagPairing, for pairing smartphones in close proximity by exploiting correlated magnetometer readings. In MagPairing, users only need to naturally tap the smartphones together for a few seconds without performing any additional operations in authentication and key establishment. Our method exploits the fact that smartphones are equipped with tiny magnets. Highly correlated magnetic field patterns are produced when two smartphones are close to each other. We design MagPairing protocol and implement it on Android smartphones. We conduct extensive simulations and real-world experiments to evaluate MagPairing. Experiments verify that the captured sensor data on which MagPairing is based has high entropy and sufficient length, and is nondisclosure to attackers more than few centimeters away. Usability tests on various kinds of smartphones by totally untrained users show that the whole pairing process needs only 4.5 s on average with more than 90% success rate.},
  Doi                      = {10.1109/TIFS.2015.2505626},
  File                     = {Jin-2016-p1306-1320.pdf:References\\Jin-2016-p1306-1320.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {Android (operating system);smart phones;Android smart phones;MagPairing protocol;auxiliary out-of-band channels;bootstrapping secure communication;captured sensor data;close proximity;device pairing;intuitive pairing method;magnetic field patterns;magnetometer readings;minimal user tasks;mobile computing;usability tests;vibrational sensors;wireless channel;wireless devices;Communication system security;Magnetic sensors;Magnetometers;Smart phones;Wireless communication;Wireless sensor networks;Device pairing;authentication;magnetometer;smartphone;usability},
  Reviewtime               = {201},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7347440}
}

@Article{Jolfaei-2016-p235-246,
  Title                    = {On the Security of Permutation-Only Image Encryption Schemes},
  Author                   = {A. Jolfaei and X. W. Wu and V. Muthukkumarasamy},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {235-246},
  Volume                   = {11},

  Abstract                 = {Permutation is a commonly used primitive in multimedia (image/video) encryption schemes, and many permutation-only algorithms have been proposed in recent years for the protection of multimedia data. In permutation-only image ciphers, the entries of the image matrix are scrambled using a permutation mapping matrix which is built by a pseudo-random number generator. The literature on the cryptanalysis of image ciphers indicates that the permutation-only image ciphers are insecure against ciphertext-only attacks and/or known/chosenplaintext attacks. However, the previous studies have not been able to ensure the correct retrieval of the complete plaintext elements. In this paper, we revisited the previous works on cryptanalysis of permutation-only image encryption schemes and made the cryptanalysis work on chosen-plaintext attacks complete and more efficient. We proved that in all permutationonly image ciphers, regardless of the cipher structure, the correct permutation mapping is recovered completely by a chosenplaintext attack. To the best of our knowledge, for the first time, this paper gives a chosen-plaintext attack that completely determines the correct plaintext elements using a deterministic method. When the plain-images are of size M Ã— N and with L different color intensities, the number n of required chosen plain-images to break the permutation-only image encryption algorithm is n = Î“logL(MN)1. The complexity of the proposed attack is O (n Â· M N) which indicates its feasibility in a polynomial amount of computation time. To validate the performance of the proposed chosen-plaintext attack, numerous experiments were performed on two recently proposed permutation-only image/video ciphers. Both theoretical and experimental results showed that the proposed attack outperforms the state-of-theart cryptanalytic methods.},
  Doi                      = {10.1109/TIFS.2015.2489178},
  File                     = {Jolfaei-2016-p235-246.pdf:References\\Jolfaei-2016-p235-246.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptography;matrix algebra;multimedia systems;video coding;ciphertext-only attack;cryptanalysis;cryptanalytic method;deterministic method;image matrix;multimedia encryption;permutation mapping matrix;permutation-only image cipher;permutation-only image encryption;pseudorandom number generator;video cipher;Ciphers;Computational complexity;Encryption;Image color analysis;Multimedia communication;Chosen-plaintext attack;cryptanalysis;image encryption;permutation},
  Reviewtime               = {172},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7295616}
}

@Article{Jung-2016-p868-868,
  Title                    = {Rebuttal to Comments on Control Cloud Data Access Privilege and Anonymity With Fully Anonymous Attribute-Based Encryption},
  Author                   = {T. Jung and X. Y. Li and Z. Wan and M. Wan},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {868-868},
  Volume                   = {11},

  Abstract                 = {Ma et al. recently submitted a comment correspondence which points out a flaw in our paper (a sequel of our earlier paper published in the Proceedings of IEEE INFOCOM). The flaw led to the leakage of the system-wide master key; therefore, we improved our own scheme by addressing it.},
  Doi                      = {10.1109/TIFS.2015.2509946},
  File                     = {Jung-2016-p868-868.pdf:References\\Jung-2016-p868-868.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cloud computing;cryptography;control cloud data access privilege;fully anonymous attribute-based encryption;system-wide master key;Cloud computing;Cryptography;Encryption;Information security},
  Reviewtime               = {57},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7398216}
}

@Article{Kolokotronis-2016-p1500-1514,
  Title                    = {Secretly Pruned Convolutional Codes: Security Analysis and Performance Results},
  Author                   = {N. Kolokotronis and A. Katsiotis and N. Kalouptsidis},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1500-1514},
  Volume                   = {11},

  Abstract                 = {Constructions of secure channel encoders, based on secret pruning, are considered in this paper. The key defines how pruning is applied on a mother convolutional code. This results in a secret subspace that legitimate users are using to perform decoding, in contrast to an eavesdropper that employs the mother code. Both reliability and security aspects of the joint scheme are treated. We derive the expected weight enumerating function of the secret subcode and show that the legitimate users achieve a better performance (that depends on the pruning rate) in terms of word and bit error rate compared with the eavesdroppers. The security relies on the notion of indistinguishability against chosen plaintext attacks. The security proofs are given in the random oracle model, and it is shown that a randomized version of the proposed joint scheme is semantically secure by relying on the hardness of the learning parities with noise problem. The above-mentioned results are achieved by introducing a new model for physical encryption to consider the contribution of the channel noise to the system's security.},
  Doi                      = {10.1109/TIFS.2016.2537262},
  File                     = {Kolokotronis-2016-p1500-1514.pdf:References\\Kolokotronis-2016-p1500-1514.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {channel coding;convolutional codes;cryptography;error statistics;telecommunication network reliability;telecommunication security;bit error rate;channel noise contribution;chosen plaintext attacks;expected weight enumerating function;learning parities;physical encryption;random oracle model;secret pruning;secret subcode;secretly pruned convolutional codes;secure channel encoders;security analysis;security proofs;word rate;Convolutional codes;Decoding;Encoding;Encryption;Reliability;Semantic security;channel coding;indistinguishability;learning parity with noise;physical layer},
  Reviewtime               = {256},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7423757}
}

@Article{Komogortsev-2016-p621-632,
  Title                    = {Oculomotor Plant Characteristics: The Effects of Environment and Stimulus},
  Author                   = {O. Komogortsev and A. Karpov and C. Holland},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {621-632},
  Volume                   = {11},

  Abstract                 = {This paper presents an objective evaluation of the effects of environmental factors, such as stimulus presentation and eye tracking specifications, on the biometric accuracy of oculomotor plant characteristic biometrics. This paper examines the largest known data set for eye movement biometrics, with eye movements recorded from 323 subjects over multiple sessions. Six spatial precision tiers (0.01Â°, 0.11Â°, 0.21Â°, 0.31Â°, 0.41Â°, and 0.51Â°), six temporal resolution tiers (1000, 500, 250, 120, 75, and 30 Hz), and three stimulus types (horizontal, random, and textual) are evaluated to identify acceptable conditions under which to collect eye movement data. The results suggest the use of eye tracking equipment providing at least 0.1Â° spatial precision and 30-Hz sampling rate for biometric purposes, and the use of a horizontal pattern stimulus when using the 2-D oculomotor plant model developed by Komogortsev et al.},
  Doi                      = {10.1109/TIFS.2015.2503263},
  File                     = {Komogortsev-2016-p621-632.pdf:References\\Komogortsev-2016-p621-632.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {biometrics (access control);environmental factors;eye;object tracking;2D oculomotor plant model;biometric accuracy;environmental factors;eye movement biometrics;eye movement data collection;eye tracking specifications;horizontal stimulus type;oculomotor plant characteristics;random stimulus type;spatial precision tiers;stimulus presentation;temporal resolution tiers;textual stimulus type;Accuracy;Biological system modeling;Computational modeling;Fingerprint recognition;Iris recognition;Mathematical model;Visual systems;Biometrics;eye movements;mathematical modeling;pattern analysis;security and protection},
  Reviewtime               = {265},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7335612}
}

@Article{Kong-2016-p594-608,
  Title                    = {Iterative Distributed Minimum Total MSE Approach for Secure Communications in MIMO Interference Channels},
  Author                   = {Z. Kong and S. Yang and F. Wu and S. Peng and L. Zhong and L. Hanzo},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {594-608},
  Volume                   = {11},

  Abstract                 = {In this paper, we consider the problem of jointly designing transmit precoding (TPC) matrix and receive filter matrix subject to both secrecy and per-transmitter power constraints in the multiple-input multiple-output (MIMO) interference channel, where K legitimate transmitter-receiver pairs communicate in the presence of an external eavesdropper. Explicitly, we jointly design the TPC and receive filter matrices based on the minimum total mean-squared error (MSE) criterion under a given and feasible information-theoretic degrees of freedom. More specifically, we formulate this problem by minimizing the total MSEs of the signals communicated between the legitimate transmitter-receiver pairs, while ensuring that the MSE of the signals decoded by the eavesdropper remains higher than a certain threshold. We demonstrate that the joint design of the TPC and receive filter matrices subject to both secrecy and transmit power constraints can be accomplished by an efficient iterative distributed algorithm. The convergence of the proposed iterative algorithm is characterized as well. Furthermore, the performance of the proposed algorithm, including both its secrecy rate and MSE, is characterized with the aid of numerical results. We demonstrate that the proposed algorithm outperforms the traditional interference alignment algorithm in terms of both the achievable secrecy rate and the MSE. As a benefit, secure communications can be guaranteed by the proposed algorithm for the MIMO interference channel even in the presence of a sophisticated/strong eavesdropper, whose number of antennas is much higher than that of each legitimate transmitter and receiver.},
  Doi                      = {10.1109/TIFS.2015.2493888},
  File                     = {Kong-2016-p594-608.pdf:References\\Kong-2016-p594-608.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {MIMO communication;antenna arrays;convergence of numerical methods;iterative methods;matrix algebra;mean square error methods;precoding;radio receivers;radio transmitters;radiofrequency filters;radiofrequency interference;telecommunication security;wireless channels;MIMO interference channels;external eavesdropper;information-theoretic degrees-of-freedom;iterative distributed minimum total MSE approach;minimum total mean-squared error criterion;multiple-input multiple-output interference channels;per-transmitter power constraints;receive filter matrix;secrecy constraints;secure communications;transmit precoding matrix;transmitter-receiver pairs;Interference channels;MIMO;Receiving antennas;Transmitting antennas;MIMO interference channel;interference alignment;physical layer security;secure communications;total MSE},
  Reviewtime               = {61},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7308038}
}

@Article{Kumar-2016-p1027-1038,
  Title                    = {PHY-Layer Authentication Using Duobinary Signaling for Spectrum Enforcement},
  Author                   = {V. Kumar and J. M. J. Park and K. Bian},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {1027-1038},
  Volume                   = {11},

  Abstract                 = {Spectrum security and enforcement is one of the major challenges that need to be addressed before spectrum sharing technologies can be adopted widely. The problem of rogue transmitters is a major threat to the viability of spectrum sharing. One approach for deterring rogue transmissions is to enable receivers to authenticate or uniquely identify transmitters. Although cryptographic mechanisms at the higher layers have been widely used to authenticate transmitters, the ability to authenticate transmitters at the physical (PHY) layer has a number of key advantages over higher layer approaches. In existing schemes, the authentication signal is added to the message signal in such a way that the authentication signal appears as noise to the message signal and vice versa. Hence, existing schemes are constrained by a fundamental tradeoff between the message signal's signal-to-noise ratio (SNR) and the authentication signal's SNR. In this paper, we extend the precoded duobinary signaling (P-DS) technique to devise a new PHY-layer authentication scheme called P-DS for authentication (P-DSA). P-DSA exploits the redundancy introduced by P-DS to embed the authentication signal into the message signal. P-DSA is not constrained by the aforementioned tradeoff between the message and authentication signals. Our results show that P-DSA improves the detection performance compared with the prior art without sacrificing message throughput or increasing transmission power.},
  Doi                      = {10.1109/TIFS.2016.2516904},
  File                     = {Kumar-2016-p1027-1038.pdf:References\\Kumar-2016-p1027-1038.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptographic protocols;radio spectrum management;telecommunication signalling;authentication signal embedding;detection performance improvement;message signal signal-to-noise ratio;physical layer authentication;precoded duobinary signaling technique;rogue transmitter problem;spectrum security and enforcement;transmitter authentication;Authentication;Interference;Privacy;Radio transmitters;Receivers;Signal to noise ratio;PHY-layer authentication;duobinary signaling;spectrum enforcement;spectrum sharing},
  Reviewtime               = {307},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7378490}
}

@Article{Lazzeretti-2016-p642-657,
  Title                    = {Piecewise Function Approximation With Private Data},
  Author                   = {R. Lazzeretti and T. Pignata and M. Barni},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {642-657},
  Volume                   = {11},

  Abstract                 = {We present two secure two party computation (STPC) protocols for piecewise function approximation on private data. The protocols rely on a piecewise approximation of the to-be-computed function easing the implementation in an STPC setting. The first protocol relies entirely on garbled circuits (GCs), while the second one exploits a hybrid construction where GC and homomorphic encryption are used together. In addition to piecewise constant and linear approximation, polynomial interpolation is also considered. From a communication complexity perspective, the full-GC implementation is preferable when the input and output variables can be represented with a small number of bits, while the hybrid solution is preferable otherwise. With regard to computational complexity, the full-GC solution is generally more convenient.},
  Doi                      = {10.1109/TIFS.2015.2503268},
  File                     = {Lazzeretti-2016-p642-657.pdf:References\\Lazzeretti-2016-p642-657.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {communication complexity;cryptographic protocols;data privacy;function approximation;interpolation;GCs;STPC protocols;communication complexity;data privacy;garbled circuits;homomorphic encryption;linear approximation;piecewise constant approximation;piecewise function approximation;polynomial interpolation;secure two party computation protocols;Encryption;Function approximation;Piecewise linear approximation;Polynomials;Protocols;Computing with private data;Garbled Circuits;Homomorphic Encryption;Secure Two Party Computation;Secure two party computation;Signal Processing in the Encrypted Domain;computing with private data;garbled circuits;homomorphic encryption;signal processing in the encrypted domain},
  Reviewtime               = {178},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7336550}
}

@Article{Li-2016-p543-555,
  Title                    = {A Security-Enhanced Alignment-Free Fuzzy Vault-Based Fingerprint Cryptosystem Using Pair-Polar Minutiae Structures},
  Author                   = {C. Li and J. Hu},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {543-555},
  Volume                   = {11},

  Abstract                 = {Alignment-free fingerprint cryptosystems perform matching using relative information between minutiae, e.g., local minutiae structures, is promising, because it can avoid the recognition errors and information leakage caused by template alignment/registration. However, as most local minutiae structures only contain relative information of a few minutiae in a local region, they are less discriminative than the global minutiae pattern. Besides, the similarity measures for trivially/coarsely quantized features in the existing work cannot provide a robust way to deal with nonlinear distortions, a common form of intra-class variation. As a result, the recognition accuracy of current alignment-free fingerprint cryptosystems is unsatisfying. In this paper, we propose an alignment-free fuzzy vault-based fingerprint cryptosystem using highly discriminative pair-polar (P-P) minutiae structures. The fine quantization used in our system can largely retain information about a fingerprint template and enables the direct use of a traditional, well-established minutiae matcher. In terms of template/key protection, the proposed system fuses cancelable biometrics and biocryptography. Transforming the P-P minutiae structures before encoding destroys the correlations between them, and can provide privacy-enhancing features, such as revocability and protection against cross-matching by setting distinct transformation seeds for different applications. The comparison with other minutiae-based fingerprint cryptosystems shows that the proposed system performs favorably on selected publicly available databases and has strong security.},
  Doi                      = {10.1109/TIFS.2015.2505630},
  File                     = {Li-2016-p543-555.pdf:References\\Li-2016-p543-555.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptography;data privacy;fingerprint identification;fuzzy set theory;image matching;image registration;P-P minutiae structures;biocryptography;cancelable biometrics;fingerprint template;information leakage;intraclass variation;local minutiae structures;minutiae matcher;nonlinear distortions;pair-polar minutiae structures;privacy-enhancing features;quantization;recognition errors;security-enhanced alignment-free fuzzy vault-based fingerprint cryptosystem;template alignment;template registration;template-key protection;Cryptography;Fingerprint recognition;Nonlinear distortion;Quantization (signal);Robustness;Fingerprint;alignment-free;biocryptosystem;cancelable;fuzzy vault;local minutiae structures;pair-polar minutiae structures;quantization},
  Reviewtime               = {236},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7347427}
}

@Article{Li-2016-p344-357,
  Title                    = {Steganalysis Over Large-Scale Social Networks With High-Order Joint Features and Clustering Ensembles},
  Author                   = {F. Li and K. Wu and J. Lei and M. Wen and Z. Bi and C. Gu},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {344-357},
  Volume                   = {11},

  Abstract                 = {This paper tackles a recent challenge in identifying culprit actors, who try to hide confidential payload with steganography, among many innocent actors in social media networks. The problem is called steganographer detection problem and is significantly different from the traditional stego detection problem that classifies an individual object as a cover or a stego. To solve the steganographer detection problem over large-scale social media networks, this paper proposes a method that uses high-order joint features and clustering ensembles. It employs 250-D features calculated from the high-order joint matrices of Discrete Cosine Transform (DCT) coefficients of JPEG images, which indicate the dependencies of image content. Furthermore, a number of hierarchical sub-clusterings trained by the features are integrated as a clustering ensemble based on the majority voting strategy, which is used to make optimal decisions on suspicious steganographers. Experimental results show that the proposed scheme is effective and efficient in identifying potential steganographers in large-scale social media networks, and has better performance when tested against the state-of-the-art steganographic methods.},
  Doi                      = {10.1109/TIFS.2015.2496910},
  File                     = {Li-2016-p344-357.pdf:References\\Li-2016-p344-357.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {discrete cosine transforms;image coding;matrix algebra;social networking (online);steganography;250-D features;DCT;JPEG image;clustering ensembles;discrete cosine transform;hierarchical subclustering;high-order joint features;high-order joint matrices;large-scale social network;majority voting strategy;steganalysis;steganographer detection problem;steganography;stego detection problem;suspicious steganographer;Discrete cosine transforms;Feature extraction;Media;Social network services;Transform coding;Clustering ensembles;High-order joint features;JPEG image;Social networks;Steganalysis;clustering ensembles;high-order joint features;social networks},
  Reviewtime               = {303},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7314932}
}

@Article{Li-2016-p86-99,
  Title                    = {Cooperative Change Detection for Voltage Quality Monitoring in Smart Grids},
  Author                   = {S. Li and X. Wang},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {86-99},
  Volume                   = {11},

  Abstract                 = {This paper considers the real-time voltage quality monitoring in smart grid systems. The goal is to detect the occurrence of disturbances in the nominal sinusoidal voltage signal as quickly as possible such that protection measures can be taken in time. Based on an autoregressive model for the disturbance, we propose a generalized local likelihood ratio detector, which processes meter readings sequentially and alarms as soon as the test statistic exceeds a prescribed threshold. The proposed detector not only reacts to a wide range of disturbances, but also achieves lower detection delay compared with the conventional block processing method. Then, we further propose to deploy multiple meters to monitor the voltage signal cooperatively. The distributed meters communicate wirelessly to a central meter, where the data fusion and detection are performed. In light of the limited bandwidth of wireless channels, we develop a level-triggered sampling scheme, where each meter transmits only one-bit each time asynchronously. The proposed multi-meter scheme features substantially low communication overhead, while its performance is close to that of the ideal case where distributed meter readings are perfectly available at the central meter.},
  Doi                      = {10.1109/TIFS.2015.2477796},
  File                     = {Li-2016-p86-99.pdf:References\\Li-2016-p86-99.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {autoregressive processes;power system faults;power system measurement;power system reliability;smart meters;smart power grids;autoregressive model;central meter;cooperative change detection;data detection;data fusion;distributed meter readings;disturbance detection;generalized local likelihood ratio detector;level-triggered sampling scheme;lower detection delay;multimeter scheme;nominal sinusoidal voltage signal;real-time voltage quality monitoring;smart grid system;test statistic;wireless channel;Delays;Detectors;Meter reading;Monitoring;Noise;Noise measurement;Voltage signal disturbance;autoregressive model;change detection;level-triggered sampling},
  Reviewtime               = {302},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7254183}
}

@Article{Lin-2016-p46-58,
  Title                    = {On the Fast Fading Gaussian Wiretap Channel With Statistical Channel State Information at the Transmitter},
  Author                   = {P. H. Lin and E. Jorswieck},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {46-58},
  Volume                   = {11},

  Abstract                 = {In this paper, we investigate the ergodic secrecy capacity of the fast fading Gaussian wiretap channel when only the statistics of the channel state information are known at the transmitter. We derive conditions for the existence of degradedness and a positive ergodic secrecy capacity under the usual stochastic order, the convex order, and the increasing convex order between the legitimate and eavesdropper channels. For more general orders, we prove the secrecy capacity of layered erasure wiretap channels and propose a layered signaling for the achievable scheme, and we derive an upper bound on the capacity for fast fading Gaussian wiretap channels. Finally, the numerical results show that under Nakagami-m fast fading channels, the proposed layered signaling outperforms the Gaussian codebook in several cases. In particular, in certain cases, the Gaussian codebook can achieve only a zero secrecy rate, whereas the proposed scheme achieves positive secrecy rates. Therefore, the connectivity of wireless networks can be significantly improved by the proposed scheme.},
  Doi                      = {10.1109/TIFS.2015.2476464},
  File                     = {Lin-2016-p46-58.pdf:References\\Lin-2016-p46-58.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {Gaussian channels;Nakagami channels;channel capacity;channel coding;statistical analysis;telecommunication security;telecommunication signalling;Gaussian codebook;Nakagami-m fast fading Gaussian wiretap channel capacity;layered signaling;positive ergodic secrecy capacity;statistical channel state information;transmitter;wireless network;Antennas;Channel capacity;Fading;Joints;Random variables;Receivers;Upper bound;Information theoretic security;degraded wiretap channel;ergodic secrecy capacity;fast-fading;stochastic order},
  Reviewtime               = {355},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7239601}
}

@Article{Lin-2016-p126-140,
  Title                    = {Preprocessing Reference Sensor Pattern Noise via Spectrum Equalization},
  Author                   = {X. Lin and C. T. Li},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {126-140},
  Volume                   = {11},

  Abstract                 = {Although sensor pattern noise (SPN) has been proved to be an effective means to uniquely identify digital cameras, some non-unique artifacts, shared among cameras undergo the same or similar in-camera processing procedures, often give rise to false identifications. Therefore, it is desirable and necessary to suppress these unwanted artifacts so as to improve the accuracy and reliability. In this paper, we propose a novel preprocessing approach for attenuating the influence of the non-unique artifacts on the reference SPN to reduce the false identification rate. Specifically, we equalize the magnitude spectrum of the reference SPN through detecting and suppressing the peaks according to the local characteristics, aiming at removing the interfering periodic artifacts. Combined with six SPN extractions or enhancement methods, our proposed spectrum equalization algorithm is evaluated on the Dresden image database as well as our own database, and compared with the state-of-the-art preprocessing schemes. The experimental results indicate that the proposed procedure outperforms, or at least performs comparable with, the existing methods in terms of the overall receiver operating characteristic curves and kappa statistic computed from a confusion matrix, and tends to be more resistant to JPEG compression for medium and small image blocks.},
  Doi                      = {10.1109/TIFS.2015.2478748},
  File                     = {Lin-2016-p126-140.pdf:References\\Lin-2016-p126-140.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cameras;image coding;image enhancement;statistical analysis;Dresden image database;JPEG compression;SPN extractions;confusion matrix;digital cameras;enhancement methods;false identification rate;image blocks;in-camera processing procedures;kappa statistics;periodic artifacts;receiver operating characteristic curves;reference sensor pattern noise preprocessing;spectrum equalization algorithm;Cameras;Digital images;Discrete Fourier transforms;Forensics;Image coding;Noise;Transform coding;Multimedia forensics;PRNU;sensor pattern noise;source camera identification (SCI);spectrum equalization},
  Reviewtime               = {180},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7265059}
}

@Article{Liu-2016-p484-497,
  Title                    = {Fine-Grained Two-Factor Access Control for Web-Based Cloud Computing Services},
  Author                   = {J. K. Liu and M. H. Au and X. Huang and R. Lu and J. Li},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {484-497},
  Volume                   = {11},

  Abstract                 = {In this paper, we introduce a new fine-grained two-factor authentication (2FA) access control system for web-based cloud computing services. Specifically, in our proposed 2FA access control system, an attribute-based access control mechanism is implemented with the necessity of both a user secret key and a lightweight security device. As a user cannot access the system if they do not hold both, the mechanism can enhance the security of the system, especially in those scenarios where many users share the same computer for web-based cloud services. In addition, attribute-based control in the system also enables the cloud server to restrict the access to those users with the same set of attributes while preserving user privacy, i.e., the cloud server only knows that the user fulfills the required predicate, but has no idea on the exact identity of the user. Finally, we also carry out a simulation to demonstrate the practicability of our proposed 2FA system.},
  Doi                      = {10.1109/TIFS.2015.2493983},
  File                     = {Liu-2016-p484-497.pdf:References\\Liu-2016-p484-497.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {Web services;authorisation;cloud computing;data privacy;private key cryptography;Web-based cloud computing services;attribute-based access control mechanism;cloud server;fine-grained 2FA access control system;fine-grained two-factor access control;fine-grained two-factor authentication access control system;lightweight security device;user privacy preservation;user secret key;Access control;Authentication;Cloud computing;Computers;Public key;Access Control;Fine-grained;Two-Factor;Web Services;Web services;access control;two-factor},
  Reviewtime               = {203},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7305762}
}

@Article{Liu-2016-p677-690,
  Title                    = {Designing Analog Fountain Timing Channels: Undetectability, Robustness, and Model-Adaptation},
  Author                   = {W. Liu and G. Liu and J. Zhai and Y. Dai and D. Ghosal},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {677-690},
  Volume                   = {11},

  Abstract                 = {In existing model-based timing channels, the requirement for the target model to be shared between the sender and the receiver limits the sender's ability to adapt to changes in the inter-packet delay (IPD) distribution of the application traffic. In this paper, using analog fountain codes (AFCs) with a general model-fitting coding framework, we design timing channel schemes that allow the sender to change the target model without synchronizing with the receiver. We first propose analog fountain timing channels based on symbol transition when the application packet streams have IPD distribution that is shape similar to the distribution of AFC code symbol values. For more general packet streams, we then propose analog fountain timing channels based on symbol split in which the linearly mapped symbols are split using a symbol probability split matrix to mimic the IPD distribution of the application traffic. We use real VoIP and SSH traffic to compare the proposed schemes with model-based timing channels using LT codes and AFC. Experimental results show that both the proposed schemes are model-secure. The robustness of the two schemes is higher than the model-based timing channels using LT codes whereas not as good as those using AFC when the sender and receiver sides are synchronized with respect to the target model. Moreover, when the sender and the receiver are not synchronized with respect to the model, the robustness of the proposed schemes is significantly higher than model-based timing channels.},
  Doi                      = {10.1109/TIFS.2015.2505688},
  File                     = {Liu-2016-p677-690.pdf:References\\Liu-2016-p677-690.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {channel coding;matrix algebra;probability;telecommunication traffic;AFC;IPD distribution;LT code;SSH traffic;VoIP;analog fountain code;analog fountain timing channel;general model-fitting coding framework;interpacket delay distribution;packet streaming;symbol probability split matrix;Adaptation models;Encoding;Frequency control;Jitter;Receivers;Robustness;Timing;Network timing channel;analog fountain codes;model-adaptation;symbol probability split matrix;symbol probability transition matrix},
  Reviewtime               = {250},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7347395}
}

@Article{Liu-2016-p514-527,
  Title                    = {Detection of Superpoints Using a Vector Bloom Filter},
  Author                   = {W. Liu and W. Qu and J. Gong and K. Li},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {514-527},
  Volume                   = {11},

  Abstract                 = {Internet attacks, such as distributed denial-of-service attacks and worm attacks, are increasing in severity and frequency. Identifying and mitigating realtime attacks are an important and challenging task for network administrators. An infected host can make a large number of connections to distinct destinations during a short time. Such a host is called a superpoint. Detecting superpoints can be utilized for traffic engineering and anomaly detection. This paper proposes a novel data streaming method for detecting superpoints and proves guarantees on its accuracy with low memory requirements. The superior performance of this method comes from a new data structure, called vector bloom filter (VBF), which is a variant of standard BF. The VBF consists of six hash functions, four of which take some consecutive bits from the input string as the corresponding value, respectively. The information of superpoints is obtained by using the overlapping of hash bit strings of the VBF. Theoretical analysis and experimental results show that the proposed method can detect superpoints precisely and efficiently through comparison with other existing approaches.},
  Doi                      = {10.1109/TIFS.2015.2503269},
  File                     = {Liu-2016-p514-527.pdf:References\\Liu-2016-p514-527.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {Internet;computer network security;cryptography;data structures;Internet attacks;VBF;anomaly detection;data streaming method;data structure;distributed denial-of-service attacks;hash bit strings;hash functions;memory requirements;network administrators;superpoint detection;traffic engineering;vector bloom filter;worm attacks;Accuracy;Arrays;Grippers;IP networks;Information filters;Bloom filter;Cardinality estimation;IP flow;Random variable;Superpoint;bloom filter;cardinality estimation;random variable;superpoint},
  Reviewtime               = {153},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7336545}
}

@Article{Liu-2016-p1592-1602,
  Title                    = {Masking Transmission Line Outages via False Data Injection Attacks},
  Author                   = {X. Liu and Z. Li and X. Liu and Z. Li},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1592-1602},
  Volume                   = {11},

  Abstract                 = {Today's power systems become more prone to cyber-attacks due to the high integration of information technologies. In this paper, we demonstrate that the outages of some lines can be masked by injecting false data into a set of measurements. The success of the topology attack can be guaranteed by making that: 1) the injected false data obeys Kirchhoff current law and Kirchhoff voltage law to avoid being detected by the bad data detection program in the state estimation and 2) the residual in the line outage detection is increased such that the line outage cannot be detected by phasor measurement unit data. A bilevel optimization problem is set up to determine the optimal attack vector that can maximize the residual of the outaged line. The IEEE 39-bus and 118-bus systems are used to demonstrate the masking scheme.},
  Doi                      = {10.1109/TIFS.2016.2542061},
  File                     = {Liu-2016-p1592-1602.pdf:References\\Liu-2016-p1592-1602.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {IEEE standards;optimisation;phasor measurement;power transmission lines;IEEE 118-bus systems;IEEE 39-bus systems;Kirchhoff current law;Kirchhoff voltage law;bilevel optimization problem;cyber-attacks;false data injection attacks;information technologies;optimal attack vector;phasor measurement unit data;power systems;state estimation;topology attack;transmission line outages;Jacobian matrices;Phasor measurement units;Power grids;Power measurement;Topology;Transmission line measurements;False data injection attacks;false data injection attacks;line outage detection;phasor measurement unit;power systems},
  Reviewtime               = {134},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7433442}
}

@Article{Liu-2016-p1385-1397,
  Title                    = {Efficient Implementation of NIST-Compliant Elliptic Curve Cryptography for 8-bit AVR-Based Sensor Nodes},
  Author                   = {Z. Liu and H. Seo and J. GroÃŸschÃ¤dl and H. Kim},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1385-1397},
  Volume                   = {11},

  Abstract                 = {In this paper, we introduce a highly optimized software implementation of standards-compliant elliptic curve cryptography (ECC) for wireless sensor nodes equipped with an 8-bit AVR microcontroller. We exploit the state-of-the-art optimizations and propose novel techniques to further push the performance envelope of a scalar multiplication on the NIST P-192 curve. To illustrate the performance of our ECC software, we develope the prototype implementations of different cryptographic schemes for securing communication in a wireless sensor network, including elliptic curve Diffie-Hellman (ECDH) key exchange, the elliptic curve digital signature algorithm (ECDSA), and the elliptic curve Menezes-Qu-Vanstone (ECMQV) protocol. We obtain record-setting execution times for fixed-base, point variable-base, and double-base scalar multiplication. Compared with the related work, our ECDH key exchange achieves a performance gain of roughly 27% over the best previously published result using the NIST P-192 curve on the same platform, while our ECDSA performs twice as fast as the ECDSA implementation of the well-known TinyECC library. We also evaluate the impact of Karatsuba's multiplication technique on the overall execution time of a scalar multiplication. In addition to offering high performance, our implementation of scalar multiplication has a highly regular execution profile, which helps to protect against certain side-channel attacks. Our results show that NIST-compliant ECC can be implemented efficiently enough to be suitable for resource-constrained sensor nodes.},
  Doi                      = {10.1109/TIFS.2015.2491261},
  File                     = {Liu-2016-p1385-1397.pdf:References\\Liu-2016-p1385-1397.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {digital signatures;public key cryptography;wireless sensor networks;AVR-based sensor nodes;ECC software;ECDH key exchange;ECDSA;ECMQV protocol;NIST-compliant elliptic curve cryptography;TinyECC library;elliptic curve Diffie-Hellman key exchange;elliptic curve Menezes-Qu-Vanstone protocol;elliptic curve digital signature algorithm;wireless sensor nodes;Clocks;Elliptic curve cryptography;Microcontrollers;NIST;Registers;Software;Wireless sensor networks;Elliptic curve cryptography;Multi- Precision Arithmetic;NIST Curve P-192;NIST curve P-192;Scalar Multiplication;Wireless Sensor Networks;multi-precision arithmetic;scalar multiplication;wireless sensor networks},
  Reviewtime               = {255},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7299326}
}

@Article{Ma-2016-p866-867,
  Title                    = {Comments on Control Cloud Data Access Privilege and Anonymity With Fully Anonymous Attribute-Based Encryption},
  Author                   = {H. Ma and R. Zhang and W. Yuan},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {866-867},
  Volume                   = {11},

  Abstract                 = {Most of the known attribute-based encryption (ABE) schemes focused on the data contents privacy and the access control, but less attention was paid to the privilege control and the identity privacy problem. Recently in IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY (TIFS) (DOI:10.1109/TIFS.2014.2368352), Jung et al. proposed an anonymous attribute-based encryption scheme for access privilege and anonymity, which exhibited a lot of interesting ideas and gave the proof in the standard model. However, after carefully revisiting the scheme, we found that any valid user can compute the system-wide master key and their proof has some mistakes, hence, it fails to meet their security definitions.},
  Doi                      = {10.1109/TIFS.2015.2509865},
  File                     = {Ma-2016-p866-867.pdf:References\\Ma-2016-p866-867.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cloud computing;data privacy;information retrieval;public key cryptography;ABE scheme;IEEE;TIFS;Transaction On Information Forensics And Security;anonymous attribute-based encryption;control cloud data access anonymity;control cloud data access privilege;data content privacy;identity privacy problem;system-wide master key;Cloud computing;Cryptography;Data privacy;Information retrieval;Cryptanalysis;access privilege;anonymity;attribute-based encryption},
  Reviewtime               = {141},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7360926}
}

@Article{Maiorana-2016-p163-175,
  Title                    = {On the Permanence of EEG Signals for Biometric Recognition},
  Author                   = {E. Maiorana and D. La Rocca and P. Campisi},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {163-175},
  Volume                   = {11},

  Abstract                 = {Brain signals have been investigated for more than a century in the medical field. However, despite the broad interest in clinical applications, their use as a biometric identifier has been only recently considered by the scientific community. In this paper, we focus on the permanence across time of brain signals, specifically of electroencephalographic (EEG) signals, issue of paramount importance for the deployment of brain-based biometric recognition systems in real life, not yet fully addressed. In particular, we speculate about the stability of EEG features by analyzing the recognition performance that can be achieved when comparing EEG signals acquired during different sessions. We carry out an extensive set of experimental tests, performed on several EEG-based biometric systems over a large database, comprising three recordings taken from 50 healthy subjects in resting state conditions, acquired in a time span of approximately one month and a half. The results confirm that a significant level of permanence can be guaranteed.},
  Doi                      = {10.1109/TIFS.2015.2481870},
  File                     = {Maiorana-2016-p163-175.pdf:References\\Maiorana-2016-p163-175.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {biometrics (access control);electroencephalography;medical signal processing;EEG signal;biometric identifier;biometric recognition;brain signal;clinical application;electroencephalographic signal;permanence;Biometrics (access control);Brain modeling;Databases;Electrodes;Electroencephalography;Feature extraction;Scalp;Biometrics;Database;Electroencephalography;Permanence;database;electroencephalography;permanence},
  Reviewtime               = {294},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7275167}
}

@Article{Naini-2016-p358-372,
  Title                    = {Where You Are Is Who You Are: User Identification by Matching Statistics},
  Author                   = {F. M. Naini and J. Unnikrishnan and P. Thiran and M. Vetterli},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {358-372},
  Volume                   = {11},

  Abstract                 = {Most users of online services have unique behavioral or usage patterns. These behavioral patterns can be exploited to identify and track users by using only the observed patterns in the behavior. We study the task of identifying users from statistics of their behavioral patterns. In particular, we focus on the setting in which we are given histograms of users' data collected during two different experiments. We assume that, in the first data set, the users' identities are anonymized or hidden and that, in the second data set, their identities are known. We study the task of identifying the users by matching the histograms of their data in the first data set with the histograms from the second data set. In recent works, the optimal algorithm for this user identification task is introduced. In this paper, we evaluate the effectiveness of this method on three different types of data sets with up to 50 000 users, and in multiple scenarios. Using data sets such as call data records, web browsing histories, and GPS trajectories, we demonstrate that a large fraction of users can be easily identified given only histograms of their data; hence, these histograms can act as users' fingerprints. We also verify that simultaneous identification of users achieves better performance compared with one-by-one user identification. Furthermore, we show that using the optimal method for identification indeed gives higher identification accuracy than the heuristics-based approaches in the practical scenarios. The accuracy obtained under this optimal method can thus be used to quantify the maximum level of user identification that is possible in such settings. We show that the key factors affecting the accuracy of the optimal identification algorithm are the duration of the data collection, the number of users in the anonymized data set, and the resolution of the data set. We also analyze the effectiveness of k-anonymization in resisting user identification attacks on these data sets.},
  Doi                      = {10.1109/TIFS.2015.2498131},
  File                     = {Naini-2016-p358-372.pdf:References\\Naini-2016-p358-372.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {authorisation;data acquisition;optimisation;statistical analysis;GPS trajectories;Web browsing histories;anonymized data set;anonymized user identities;behavioral pattern;call data records;data collection;data histograms;hidden user identities;k-anonymization;online services;optimal algorithm;optimal identification algorithm;optimal method;statistics;usage patterns;user identification attacks;users fingerprints;Accuracy;Bipartite graph;Data privacy;Databases;Histograms;Testing;Training;Data privacy;de-anonymization;identification of persons},
  Reviewtime               = {72},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7321027}
}

@Article{Nogueira-2016-p1206-1213,
  Title                    = {Fingerprint Liveness Detection Using Convolutional Neural Networks},
  Author                   = {R. F. Nogueira and R. de Alencar Lotufo and R. Campos Machado},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1206-1213},
  Volume                   = {11},

  Abstract                 = {With the growing use of biometric authentication systems in the recent years, spoof fingerprint detection has become increasingly important. In this paper, we use convolutional neural networks (CNNs) for fingerprint liveness detection. Our system is evaluated on the data sets used in the liveness detection competition of the years 2009, 2011, and 2013, which comprises almost 50 000 real and fake fingerprints images. We compare four different models: two CNNs pretrained on natural images and fine-tuned with the fingerprint images, CNN with random weights, and a classical local binary pattern approach. We show that pretrained CNNs can yield the state-of-the-art results with no need for architecture or hyperparameter selection. Data set augmentation is used to increase the classifiers performance, not only for deep architectures but also for shallow ones. We also report good accuracy on very small training sets (400 samples) using these large pretrained networks. Our best model achieves an overall rate of 97.1% of correctly classified samples-a relative improvement of 16% in test error when compared with the best previously published results. This model won the first prize in the fingerprint liveness detection competition 2015 with an overall accuracy of 95.5%.},
  Doi                      = {10.1109/TIFS.2016.2520880},
  File                     = {Nogueira-2016-p1206-1213.pdf:References\\Nogueira-2016-p1206-1213.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {feedforward neural nets;fingerprint identification;learning (artificial intelligence);object detection;biometric authentication systems;convolutional neural networks;data sets;fingerprint liveness detection;fingerprint recognition;local binary pattern approach;machine learning;random weights;supervised learning;Convolution;Feature extraction;Fingerprint recognition;Kernel;Neural networks;Principal component analysis;Support vector machines;Fingerprint recognition;machine learning;neural networks;supervised learning},
  Reviewtime               = {186},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7390065}
}

@Article{Oikawa-2016-p5-18,
  Title                    = {Manifold Learning and Spectral Clustering for Image Phylogeny Forests},
  Author                   = {M. A. Oikawa and Z. Dias and A. de Rezende Rocha and S. Goldenstein},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {5-18},
  Volume                   = {11},

  Abstract                 = {The ever-increasing number of gadgets being used to create digital content, as well as the easiness in sharing, editing, and republishing this content, brings the problem of dealing with a large amount of digital objects (e.g., images or videos) whose content is very similar. Some issues faced by investigators of digital crimes when analyzing this type of data include finding the original source of a suspect image, and the responsible for first publishing it. It is also challenging to determine how these objects are related to each other. Recent efforts in developing algorithms to find automatically the underlying relationship among groups of digital media objects with similar content have been explored in the multimedia phylogeny field. A tree structure is used to represent the relationship among these objects, inspired by the phylogenetic trees in biology. Discovering whether these objects came from the same source or from different sources is fundamentally a clustering problem: 1) related objects belong to the same cluster (tree) and 2) unrelated objects should fit in different clusters. In this paper, we address the problem of finding these clusters in sets of semantically similar images, prior to tree reconstruction. We propose the combination of manifold learning and spectral clustering approaches, which have been successfully used in different applications embedding the original data into a lower, but meaningful, dimensional space. Experiments with more than 40 000 test cases show that the proposed approach improves the accuracy in finding the correct number of trees in the set, as well as the reconstruction of the phylogeny trees.},
  Doi                      = {10.1109/TIFS.2015.2442527},
  File                     = {Oikawa-2016-p5-18.pdf:References\\Oikawa-2016-p5-18.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {electronic publishing;genetics;image forensics;learning (artificial intelligence);pattern clustering;biology;content republishing;digital crime;image phylogeny forest;manifold learning approach;multimedia phylogeny;multimedia phylogeny field;phylogenetic tree;spectral clustering approach;tree structure;Clustering algorithms;Eigenvalues and eigenfunctions;Image reconstruction;Manifolds;Phylogeny;Symmetric matrices;Vegetation;Image forensics;manifold learning;multimedia phylogeny;phylogeny forests;spectral clustering},
  Reviewtime               = {160},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7118711}
}

@Article{Oliveira-2016-p328-343,
  Title                    = {Multiple Parenting Phylogeny Relationships in Digital Images},
  Author                   = {A. A. de Oliveira and P. Ferrara and A. De Rosa and A. Piva and M. Barni and S. Goldenstein and Z. Dias and A. Rocha},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {328-343},
  Volume                   = {11},

  Abstract                 = {Recently, several studies have been concerned with modeling the parenthood relationships between near duplicates in a set of images. Two images share a parenthood relationship if one is obtained by applying transformations to the other. However, this is not the only form of parenting that can exist among images. An image might be a composition created through the combination of the semantic information existent in two or more source images, establishing a relationship between the sources and the composite. The problem of identifying these relations in a set containing near-duplicate subsets of source and composition images is referred to as multiple parenting phylogeny. Thus far, researchers tackled this problem with a three-step solution: 1) separation of near-duplicate groups; 2) classification of the relations between the groups; and 3) identification of the images used to create the original composition. In this work, we extend upon this framework by introducing key improvements, such as better identification of when two images share content, and improved ways to compare this content. In addition, we also introduce a new realistic professionally created data set of compositions involving multiple parenting relationships. The method we present in this paper is properly evaluated through quantitative metrics, established for assessing the accuracy in finding multiple parenting relationships. Finally, we discuss some particularities of the framework, such as the importance of an accurate reconstruction of phylogenies and the method's behavior when dealing with more complex compositions.},
  Doi                      = {10.1109/TIFS.2015.2493989},
  File                     = {Oliveira-2016-p328-343.pdf:References\\Oliveira-2016-p328-343.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {image classification;image forensics;composition images;digital images;image classification;image forensics;image identification;multiple parenting phylogeny relationships;near-duplicate groups;near-duplicate subsets;parenthood relationships modeling;quantitative metrics;semantic information;source images;Accuracy;Estimation;Image coding;Image color analysis;Image reconstruction;Phylogeny;Vegetation;Compositions;Image Forensics;Image forensics;Multimedia Phylogeny;Multiple Parenting Phylogeny;compositions;multimedia phylogeny;multiple parenting phylogeny},
  Reviewtime               = {181},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7305756}
}

@Article{Pasquini-2016-p1425-1437,
  Title                    = {A Deterministic Approach to Detect Median Filtering in 1D Data},
  Author                   = {C. Pasquini and G. Boato and N. Alajlan and F. G. B. De Natale},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1425-1437},
  Volume                   = {11},

  Abstract                 = {In this paper, we propose a forensic technique that is able to detect the application of a median filter to 1D data. The method relies on deterministic mathematical properties of the median filter, which lead to the identification of specific relationships among the sample values that cannot be found in the filtered sequences. Hence, their presence in the analyzed 1D sequence allows excluding the application of the median filter. Owing to its deterministic nature, the method ensures 0% false negatives, and although false positives (sequences not filtered classified as filtered) are theoretically possible, experimental results show that the false alarm rate is null for sufficiently long sequences. Furthermore, the proposed technique has the capability to locate with good precision a median filtered part of 1-D data and provides a good estimate of the window size used.},
  Doi                      = {10.1109/TIFS.2016.2530636},
  File                     = {Pasquini-2016-p1425-1437.pdf:References\\Pasquini-2016-p1425-1437.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {image filtering;image forensics;median filters;1D data;deterministic approach;false alarm rate;forensic technique;median filter application detection;median filter deterministic mathematical property;Biomedical imaging;Correlation;Detectors;Feature extraction;Filtering;Forensics;Indexes;Detection;Forensics;Median filter;detection;median filter},
  Reviewtime               = {186},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7407398}
}

@Article{Phuong-2016-p35-45,
  Title                    = {Hidden Ciphertext Policy Attribute-Based Encryption Under Standard Assumptions},
  Author                   = {T. V. X. Phuong and G. Yang and W. Susilo},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {35-45},
  Volume                   = {11},

  Abstract                 = {We propose two new ciphertext policy attribute-based encryption (CP-ABE) schemes where the access policy is defined by AND-gate with wildcard. In the first scheme, we present a new technique that uses only one group element to represent an attribute, while the existing ABE schemes of the same type need to use three different group elements to represent an attribute for the three possible values (namely, positive, negative, and wildcard). Our new technique leads to a new CP-ABE scheme with constant ciphertext size, which, however, cannot hide the access policy used for encryption. The main contribution of this paper is to propose a new CP-ABE scheme with the property of hidden access policy by extending the technique we used in the construction of our first scheme. In particular, we show a way to bridge ABE based on AND-gate with wildcard with inner product encryption and then use the latter to achieve the goal of hidden access policy. We prove that our second scheme is secure under the standard decisional linear and decisional bilinear Diffie-Hellman assumptions.},
  Doi                      = {10.1109/TIFS.2015.2475723},
  File                     = {Phuong-2016-p35-45.pdf:References\\Phuong-2016-p35-45.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptography;logic gates;ABE;AND-gate;access policy;constant ciphertext size;decisional bilinear Diffie-Hellman assumptions;encryption;group element;hidden access policy;hidden ciphertext policy attribute;product encryption;Access control;Encryption;IEEE transactions;Logic gates;Standards;Attribute Based Encryption;Attribute based encryption;Hidden Policy;Inner Product Encryption;Vi??te???s formula;Vieteâ€™s Formula;hidden policy;inner product encryption},
  Reviewtime               = {122},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7236899}
}

@Article{Pinto-2016-p1542-1548,
  Title                    = {Two-Dimensional Wavelet Analysis of Supraorbital Margins of the Human Skull for Characterizing Sexual Dimorphism},
  Author                   = {S. C. D. Pinto and P. UrbanovÃ¡ and R. M. Cesar-Jr},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1542-1548},
  Volume                   = {11},

  Abstract                 = {The accurate determination of the sex of human skeletal remains is a critical challenge in forensic pathology and skeletal anthropology. The pelvis and the skull are the most commonly used skeletal sites for determining the sex of skeletons. In the skull, the supraorbital region, which includes the supraorbital margin, is considered a specific sexually dimorphic trait. In the traditional approach, sex is determined through visual and tactile assessment. This paper introduces a methodology for the objective quantification of sexually dimorphic features using wavelet transform, which is a multiscale mathematical tool that allows for the measurement of shape variations that are hidden at different scales of resolution. The method was successfully applied for the sex determination of a pilot sample of 3D meshes-digital records of supraorbital morphology. This information can be used by experts to improve the accuracy of biologic profile assessment of a human skeleton, and to describe the geographic and temporal variations within and among populations.},
  Doi                      = {10.1109/TIFS.2016.2541611},
  File                     = {Pinto-2016-p1542-1548.pdf:References\\Pinto-2016-p1542-1548.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {anthropology;bone;image forensics;medical image processing;shape recognition;wavelet transforms;3D meshes;biologic profile assessment;forensic pathology;geographic variations;human skeletal sex determination;human skull;multiscale mathematical tool;sexual dimorphism characterization;sexually dimorphic features;sexually dimorphic trait;skeletal anthropology;supraorbital margins;supraorbital morphology;supraorbital region;tactile assessment;temporal variations;two-dimensional wavelet analysis;visual assessment;wavelet transform;Shape;Skull;Surface morphology;Surface treatment;Surface waves;Three-dimensional displays;Wavelet transforms;Sex determination;sexual dimorphism;shape analysis;skeletal remains;supraorbital margin;wavelet transform},
  Reviewtime               = {415},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7433449}
}

@Article{Raghavendra-2016-p922-936,
  Title                    = {Exploring the Usefulness of Light Field Cameras for Biometrics: An Empirical Study on Face and Iris Recognition},
  Author                   = {R. Raghavendra and K. B. Raja and C. Busch},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {922-936},
  Volume                   = {11},

  Abstract                 = {A light field sensor can provide useful information in terms of multiple depth (or focus) images, holding additional information that is quite useful for biometric applications. In this paper, we examine the applicability of a light field camera for biometric applications by considering two prominently used biometric characteristics: 1) face and 2) iris. To this extent, we employed a Lytro light field camera to construct two new and relatively large scale databases, for both face and iris biometrics. We then explore the additional information available from different depth images, which are rendered by light field camera, in two different manners: 1) by selecting the best focus image from the set of depth images and 2) combining all the depth images using super-resolution schemes to exploit the supplementary information available within the set elements. Extensive evaluations are carried out on our newly constructed database, demonstrating the significance of using additional information rendered by a light field camera to improve the overall performance of the biometric system.},
  Doi                      = {10.1109/TIFS.2015.2512559},
  File                     = {Raghavendra-2016-p922-936.pdf:References\\Raghavendra-2016-p922-936.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {face recognition;image resolution;iris recognition;Lytro light field camera;best focus image;biometric system;depth images;face recognition;iris recognition;light field sensor;super-resolution schemes;Cameras;Databases;Face;Iris recognition;Lenses;Biometrics;Face recognition;Iris recognition;Light field camera;Visible Iris;biometrics;face recognition;iris recognition;visible iris},
  Reviewtime               = {249},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7366563}
}

@Article{Roy-2016-p1412-1424,
  Title                    = {Local-Gravity-Face (LG-face) for Illumination-Invariant and Heterogeneous Face Recognition},
  Author                   = {H. Roy and D. Bhattacharjee},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1412-1424},
  Volume                   = {11},

  Abstract                 = {This paper proposes a novel method called local-gravity-face (LG-face) for illumination-invariant and heterogeneous face recognition (HFR). LG-face employs a concept called the local gravitational force angle (LGFA). The LGFA is the direction of the gravitational force that the center pixel exerts on the other pixels within a local neighborhood. A theoretical analysis shows that the LGFA is an illumination-invariant feature, considering only the reflectance part of the local texture effect of the neighboring pixels. It also preserves edge information. Rank 1 recognition rates of 97.78% on the CMU-PIE database and 97.31% on the Extended Yale B database are achieved under varying illumination, demonstrating that LG-face is an effective method of illumination-invariant face recognition. For HFR, when faces appear in different modalities, LG-face produces a common feature representation. Rank 1 recognition rates of 99.96% on the CUFS database, 98.67% on the CUFSF database, and 99.78% on the CASIA-HFB database show that the LG-face is also an effective method for HFR. The proposed method also performs consistently in the presence of complicated variations and noise.},
  Doi                      = {10.1109/TIFS.2016.2530043},
  File                     = {Roy-2016-p1412-1424.pdf:References\\Roy-2016-p1412-1424.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {edge detection;face recognition;feature extraction;image representation;image texture;reflectivity;visual databases;CASIA-HFB database;CMU-PIE database;CUFS database;CUFSF database;Extended-Yale B database;LG-face method;LGFA;center pixel;edge information preservation;feature representation;illumination;illumination-invariant feature;illumination-invariant heterogeneous face recognition;local gravitational force angle;local neighborhood;local texture;local-gravity-face method;rank-1 recognition rates;reflectance part;Databases;Face;Face recognition;Gravity;Lighting;Solid modeling;Visualization;Gravitational force;HFR;Local-Gravity-Face;illumination-invariant face recognition;local gravitational force angle;local-gravity-face},
  Reviewtime               = {142},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7406737}
}

@Article{Ruan-2016-p176-187,
  Title                    = {Profiling Online Social Behaviors for Compromised Account Detection},
  Author                   = {X. Ruan and Z. Wu and H. Wang and S. Jajodia},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {176-187},
  Volume                   = {11},

  Abstract                 = {Account compromization is a serious threat to users of online social networks (OSNs). While relentless spammers exploit the established trust relationships between account owners and their friends to efficiently spread malicious spam, timely detection of compromised accounts is quite challenging due to the well established trust relationship between the service providers, account owners, and their friends. In this paper, we study the social behaviors of OSN users, i.e., their usage of OSN services, and the application of which in detecting the compromised accounts. In particular, we propose a set of social behavioral features that can effectively characterize the user social activities on OSNs. We validate the efficacy of these behavioral features by collecting and analyzing real user clickstreams to an OSN website. Based on our measurement study, we devise individual user's social behavioral profile by combining its respective behavioral feature metrics. A social behavioral profile accurately reflects a user's OSN activity patterns. While an authentic owner conforms to its account's social behavioral profile involuntarily, it is hard and costly for impostors to feign. We evaluate the capability of the social behavioral profiles in distinguishing different OSN users, and our experimental results show the social behavioral profiles can accurately differentiate individual OSN users and detect compromised accounts.},
  Doi                      = {10.1109/TIFS.2015.2482465},
  File                     = {Ruan-2016-p176-187.pdf:References\\Ruan-2016-p176-187.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {behavioural sciences computing;data privacy;social networking (online);trusted computing;OSN user social behavioral profiles;compromised account detection;malicious spam;online social behaviors;online social networks;trust relationships;Accuracy;Browsers;Facebook;Feature extraction;Measurement;Navigation;Uniform resource locators;Online social behavior;compromised accounts detection;data analysis;privacy},
  Reviewtime               = {153},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7277068}
}

@Article{Ruiz-Blondet-2016-p1618-1629,
  Title                    = {CEREBRE: A Novel Method for Very High Accuracy Event-Related Potential Biometric Identification},
  Author                   = {M. V. Ruiz-Blondet and Z. Jin and S. Laszlo},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1618-1629},
  Volume                   = {11},

  Abstract                 = {The vast majority of existing work on brain biometrics has been conducted on the ongoing electroencephalogram. Here, we argue that the averaged event-related potential (ERP) may provide the potential for more accurate biometric identification, as its elicitation allows for some control over the cognitive state of the user to be obtained through the design of the challenge protocol. We describe the Cognitive Event-RElated Biometric REcognition (CEREBRE) protocol, an ERP biometric protocol designed to elicit individually unique responses from multiple functional brain systems (e.g., the primary visual, facial recognition, and gustatory/appetitive systems). Results indicate that there are multiple configurations of data collected with the CEREBRE protocol that all allow 100% identification accuracy in a pool of 50 users. We take this result as the evidence that ERP biometrics are a feasible method of user identification and worthy of further research.},
  Doi                      = {10.1109/TIFS.2016.2543524},
  File                     = {Ruiz-Blondet-2016-p1618-1629.pdf:References\\Ruiz-Blondet-2016-p1618-1629.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {biometrics (access control);brain;cognition;electroencephalography;medical signal processing;CEREBRE protocol;ERP biometric protocol;averaged event-related potential;biometric identification;brain biometrics;cognitive event-related biometric recognition protocol;event-related potential biometric identification;multiple functional brain systems;ongoing electroencephalogram;Brain;Electroencephalography;Face;Gratings;Image color analysis;Protocols;Visualization;Biometric Protocols;Biometrics;Emergent Biometrics;Performance and Evaluation;biometric protocols;emergent biometrics;performance and evaluation},
  Reviewtime               = {191},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7435286}
}

@Article{Safaka-2016-p1177-1191,
  Title                    = {Creating Secrets Out of Packet Erasures},
  Author                   = {I. Safaka and L. Czap and K. Argyraki and C. Fragouli},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1177-1191},
  Volume                   = {11},

  Abstract                 = {We present protocols for creating pairwise secrets between nodes in a wireless network, so that these secrets are secure from an eavesdropper, Eve, with unbounded computational and memory capabilities, but with limited network presence. We first present a basic secret-agreement protocol for single-hop networks, where secrets are constructed using traffic exchanged between the nodes, and we show that under standard theoretical assumptions, our protocol is information-theoretically secure. Second, we propose a secret-agreement protocol for arbitrary, multi-hop networks that build on the basic protocol but also comprises design features for leveraging additional sources, that multi-hop offers, for secrecy. Finally, we evaluate our protocols, and we provide experimental evidence that it is feasible to create thousands of secret bits per second, in realistic wireless setups, the security of which is independent of Eve's computational capabilities.},
  Doi                      = {10.1109/TIFS.2016.2520887},
  File                     = {Safaka-2016-p1177-1191.pdf:References\\Safaka-2016-p1177-1191.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptographic protocols;radio networks;telecommunication security;multihop networks;packet erasures;pairwise secrets creation;secret-agreement protocol;single-hop networks;unbounded computational capabilities;unbounded memory capabilities;wireless network;Ad hoc networks;Communication system security;Protocols;Security;Spread spectrum communication;Wireless networks;Secret key generation;multi-hop key agreement;multihop key agreement;packet erasures;wireless networks},
  Reviewtime               = {129},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7389379}
}

@Article{Saini-2016-p1055-1070,
  Title                    = {Jammer-Assisted Resource Allocation in Secure OFDMA With Untrusted Users},
  Author                   = {R. Saini and A. Jindal and S. De},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {1055-1070},
  Volume                   = {11},

  Abstract                 = {In this paper, we consider the problem of resource allocation in the orthogonal frequency division multiple access system with single source and M untrusted users in presence of a friendly jammer. The jammer is used to improve either the weighted sum secure rate or the overall system fairness. The formulated optimization problem in both the cases is a mixed integer non-linear programming problem, belonging to the class of NP-hard. In the sum secure rate maximization scenario, we decouple the problem and first obtain the subcarrier allocation at source and the decision for jammer power utilization on a per-subcarrier basis. Then, we do joint source and jammer power allocation using primal decomposition and alternating optimization framework. Next, we consider fair resource allocation by introducing a novel concept of subcarrier snatching with the help of jammer. We propose two schemes for jammer power utilization, called proactively fair allocation (PFA) and on-demand allocation (ODA). PFA considers equitable distribution of jammer power among the subcarriers, while ODA distributes jammer power based on the user demand. In both cases of jammer usage, we also present suboptimal solutions that solve the power allocation at a highly reduced complexity. Asymptotically optimal solutions are derived to benchmark optimality of the proposed schemes. We compare the performance of our proposed schemes with equal power allocation at source and jammer. Our simulation results demonstrate that the jammer can indeed help in improving either the sum secure rate or the overall system fairness.},
  Doi                      = {10.1109/TIFS.2016.2516912},
  File                     = {Saini-2016-p1055-1070.pdf:References\\Saini-2016-p1055-1070.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {OFDM modulation;frequency division multiple access;integer programming;nonlinear programming;resource allocation;subcarrier multiplexing;telecommunication security;fair resource allocation;jammer-assisted resource allocation;mixed integer nonlinear programming problem;on-demand allocation;orthogonal frequency division multiple access system;proactively fair allocation;secure OFDMA;subcarrier allocation;subcarrier snatching;untrusted users;weighted sum secure rate;Antennas;Complexity theory;Jamming;Optimization;Physical layer;Resource management;Security;Secure OFDMA;friendly jammer;max-min fairness;rate maximization;subcarrier snatching},
  Reviewtime               = {221},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7378480}
}

@Article{Salmani-2016-p1214-1225,
  Title                    = {Vulnerability Analysis of a Circuit Layout to Hardware Trojan Insertion},
  Author                   = {H. Salmani and M. M. Tehranipoor},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1214-1225},
  Volume                   = {11},

  Abstract                 = {While the horizontal integrated circuit design process is extensively practiced, untrusted foundries can impose significant threats on the security of final products. A carefully inserted extra circuitry as a hardware trojan in a circuit layout can interfere with circuit functionality under very rare circumstances with inconsiderable footprints. In this paper, we introduce a novel layout-level vulnerability analysis flow to evaluate the susceptibility of a circuit layout's regions to hardware Trojan insertion. We also present several metrics based on a circuit layout to quantify the possibility of hardware Trojan insertion in a specific region of layout. Results of applying our flow to several benchmarks have revealed considerably high vulnerability of circuit layouts to hardware Trojan insertion. Furthermore, several Trojans are implemented and inserted in layout regions with different vulnerabilities to evaluate the effectiveness of our new metrics. Our novel layout-level vulnerability analysis flow makes it possible to quantitatively determine the vulnerability of different implementations of a circuit and analyze the susceptibility of each corner of circuit layout to different types of functional Trojans.},
  Doi                      = {10.1109/TIFS.2016.2520910},
  File                     = {Salmani-2016-p1214-1225.pdf:References\\Salmani-2016-p1214-1225.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {circuit layout;copy protection;hardware-software codesign;integrated circuit design;system-on-chip;circuit layout;hardware trojan insertion;horizontal integrated circuit design;layout-level vulnerability analysis flow;Benchmark testing;Delays;Hardware;Layout;Metals;Routing;Trojan horses;Circuit Layout;Hardware Security;Hardware Trojans;Hardware security;Trojan Trigger and Trojan Payload;Trojan payload;Trojan trigger;Vulnerability Analysis;circuit layout;hardware Trojans;vulnerability analysis},
  Reviewtime               = {144},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7390050}
}

@Article{Saxena-2016-p1438-1452,
  Title                    = {Authentication Scheme for Flexible Charging and Discharging of Mobile Vehicles in the V2G Networks},
  Author                   = {N. Saxena and B. J. Choi},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1438-1452},
  Volume                   = {11},

  Abstract                 = {Navigating security and privacy challenges is one of the crucial requirements in the vehicle-to-grid (V2G) network. Since electric vehicles (EVs) need to provide their private information to aggregators/servers when charging/discharging at different charging stations, privacy of the vehicle owners can be compromised if the information is misused, traced, or revealed. In a wide V2G network, where vehicles can move outside of their home network to visiting networks, security and privacy become even more challenging due to untrusted entities in the visiting networks. Although some privacy-preserving solutions were proposed in the literature to tackle this problem, they do not protect against well-known security attacks and generate a huge overhead. Therefore, we propose a mutual authentication scheme to preserve privacy of the EV's information from aggregators/servers in the home as well as distributed visiting V2G networks. Our scheme, based on a bilinear pairing technique with an accumulator performing batch verification, yields higher system efficiency, defeats various security attacks, and maintains untraceability, forward privacy, and identity anonymity. A performance analysis shows that our scheme, in comparison with the existing solutions, significantly generates lower communication and computation overheads in the home and centralized V2G networks, and comparable overheads in the distributed visiting V2G networks.},
  Doi                      = {10.1109/TIFS.2016.2532840},
  File                     = {Saxena-2016-p1438-1452.pdf:References\\Saxena-2016-p1438-1452.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {electric vehicles;accumulator;authentication scheme;batch verification;bilinear pairing technique;centralized V2G networks;electric vehicles;flexible charging;flexible discharging;home V2G networks;mobile vehicles;privacy-preserving solutions;security;untraceability;vehicle-to-grid network;Authentication;Batteries;Charging stations;Electric vehicles;Privacy;Authentication;V2G;authentication;bilinear pairing;privacy-preserving;security attacks},
  Reviewtime               = {182},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7414504}
}

@Article{Saxena-2016-p907-921,
  Title                    = {Authentication and Authorization Scheme for Various User Roles and Devices in Smart Grid},
  Author                   = {N. Saxena and B. J. Choi and R. Lu},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {907-921},
  Volume                   = {11},

  Abstract                 = {The smart grid, as the next generation of the power grid, is characterized by employing many different types of intelligent devices, such as intelligent electronic devices located at substations, smart meters positioned in the home area network, and outdoor field equipment deployed in the fields. In addition, there are various users in the smart grid network, including customers, operators, maintenance personnel, and so on, who use these devices for various purposes. Therefore, a secure and efficient mutual authentication and authorization scheme is needed in the smart grid to prevent various insider and outsider attacks on many different devices. In this paper, we propose an authentication and authorization scheme for mitigating outsider and insider threats in the smart grid by verifying the user authorization and performing the user authentication together whenever a user accesses the devices. The proposed scheme computes each user role dynamically using an attribute-based access control and verifies the identity of the user together with the device. Security and performance analysis show that the proposed scheme resists various insider as well as outsider attacks, and is more efficient in terms of communication and computation costs in comparison with the existing schemes. The correctness of the proposed scheme is also proved using BAN-Logic and Proverif.},
  Doi                      = {10.1109/TIFS.2015.2512525},
  File                     = {Saxena-2016-p907-921.pdf:References\\Saxena-2016-p907-921.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {authorisation;power system security;smart power grids;attribute-based access control;authentication scheme;authorization scheme;home area network;intelligent electronic devices;outdoor field equipment;power grid;smart grid;smart meters;substations;Authentication;Authorization;Maintenance engineering;Protocols;Smart grids;Substations;Authentication;Authorization;Insider Threat;Security;Smart Grid;Smart grid;authentication;authorization;insider threat;security},
  Reviewtime               = {171},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7366583}
}

@Article{Schoettle-2016-p760-773,
  Title                    = {Game Theory and Adaptive Steganography},
  Author                   = {P. SchÃ¶ttle and R. BÃ¶hme},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {760-773},
  Volume                   = {11},

  Abstract                 = {According to conventional wisdom, content-adaptive embedding offers more steganographic security than random uniform embedding. We scrutinize this view and note that it is barely substantiated in the literature as only recently adaptive steganographic systems are tested against an attacker who anticipates the adaptivity and incorporates this knowledge into the detection strategy. For a better theoretical understanding of strategical embedding and detection, we propose a game-theoretic framework to study adaptive steganography while taking the knowledge of the steganalyst into account. We instantiate the framework with a stylized cover model and study both parties' optimal strategies. The model has a unique equilibrium in mixed strategies, which depends on the heterogeneity of the cover source. We add realism by introducing imperfect recoverability of the adaptivity criterion and prove that naiÌˆve adaptive embedding-the strategy implemented in many practical schemes-is only optimal if perfect steganography is possible or if the adaptivity criterion is not recoverable at all. In practice, where steganography is imperfect and adaptivity criteria are partially recoverable, the optimal embedding strategy is between naiÌˆve adaptive and random uniform embedding.},
  Doi                      = {10.1109/TIFS.2015.2509941},
  File                     = {Schoettle-2016-p760-773.pdf:References\\Schoettle-2016-p760-773.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {game theory;steganography;adaptive steganography security;adaptivity criterion;cover source heterogeneity;detection strategy;game theory;naiÌˆve adaptive embedding strategy;random uniform embedding;stylized cover model;Adaptation models;Approximation methods;Encoding;Game theory;Games;Probability distribution;Security;Adaptive Steganography;Adaptive steganography;Game Theory;Security;game theory;security},
  Reviewtime               = {83},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7360156}
}

@Article{Sedighi-2016-p221-234,
  Title                    = {Content-Adaptive Steganography by Minimizing Statistical Detectability},
  Author                   = {V. Sedighi and R. Cogranne and J. Fridrich},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {221-234},
  Volume                   = {11},

  Abstract                 = {Most current steganographic schemes embed the secret payload by minimizing a heuristically defined distortion. Similarly, their security is evaluated empirically using classifiers equipped with rich image models. In this paper, we pursue an alternative approach based on a locally estimated multivariate Gaussian cover image model that is sufficiently simple to derive a closed-form expression for the power of the most powerful detector of content-adaptive least significant bit matching but, at the same time, complex enough to capture the non-stationary character of natural images. We show that when the cover model estimator is properly chosen, the state-of-the-art performance can be obtained. The closed-form expression for detectability within the chosen model is used to obtain new fundamental insight regarding the performance limits of empirical steganalysis detectors built as classifiers. In particular, we consider a novel detectability limited sender and estimate the secure payload of individual images.},
  Doi                      = {10.1109/TIFS.2015.2486744},
  File                     = {Sedighi-2016-p221-234.pdf:References\\Sedighi-2016-p221-234.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {Gaussian processes;image classification;image matching;statistical analysis;steganography;classifiers;content-adaptive least significant bit matching;content-adaptive steganography;cover model estimator;locally estimated multivariate Gaussian cover image model;natural images;secret payload;security;statistical detectability;steganalysis detectors;steganographic schemes;Detectors;Distortion;Encoding;Noise;Payloads;Random variables;Security;Adaptive steganography and steganalysis;hypothesis testing theory;information hiding;multivariate Gaussian;optimal detection},
  Reviewtime               = {124},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7289422}
}

@Article{Senftleben-2016-p1578-1591,
  Title                    = {On the Privacy and Performance of Mobile Anonymous Microblogging},
  Author                   = {M. Senftleben and A. Barroso and M. Bucicoiu and M. Hollick and S. Katzenbeisser and E. Tews},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1578-1591},
  Volume                   = {11},

  Abstract                 = {Microblogging is a popular form of online social networking activity. It allows users to send messages in a one-to-many publish-subscribe manner. Most current service providers are centralized and deploy a client-server model with unencrypted message content. As a consequence, all user behavior can, by default, be monitored, and censoring based on message content can easily be enforced on the server side. A distributed, peer-to-peer microblogging system consisting of mobile smartphone-equipped users that exchange group encrypted messages in an anonymous and censorship-resistant manner can alleviate privacy and censorship issues. We experimentally evaluate message spread of such systems with simulations that run on a range of synthetic and real-world mobility inputs, thus extending the previous work. We show that such systems are feasible for a range of mobility and network settings, both under normal and under adversarial conditions, e.g., under the presence of nodes which jam the network or send spam.},
  Doi                      = {10.1109/TIFS.2016.2541633},
  File                     = {Senftleben-2016-p1578-1591.pdf:References\\Senftleben-2016-p1578-1591.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {client-server systems;cryptography;data privacy;mobile computing;peer-to-peer computing;smart phones;social networking (online);censorship-resistance;client-server model;distributed microblogging system;mobile anonymous microblogging performance;mobile anonymous microblogging privacy;online social networking activity;peer-to-peer microblogging system;smartphone;unencrypted message content;Buffer storage;Cryptography;Global Positioning System;Mobile communication;Peer-to-peer computing;Synchronization;Microblogging;anonymity;censorship-resistance;mobile networking;peer-2-peer;simulation},
  Reviewtime               = {146},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7433417}
}

@Article{Shen-2016-p498-513,
  Title                    = {Performance Analysis of Touch-Interaction Behavior for Active Smartphone Authentication},
  Author                   = {C. Shen and Y. Zhang and X. Guan and R. A. Maxion},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {498-513},
  Volume                   = {11},

  Abstract                 = {The increasing use of touchscreen smartphones to access sensitive and privacy data has given rise to the need of secure and usable authentication technique. Smartphone users have their own unique behavioral characteristics when performing touch operations. These personal characteristics are reflected on different rhythm, strength, and angle preferences of touch-interaction behavior. This paper investigates the reliability and applicability on the usage of users' touch-interaction behavior for active authentication on smartphones. For each common type of touch operations, both static and dynamic features are extracted and analyzed for fine-grained characterization of users' touch behavior. Classification techniques (nearest neighbor, neural network, support vector machine, and random forest) are applied to the feature space for performing the task of active authentication. Analyses are conducted using data from around 134 900 touch operations of 71 participants in real-world scenarios, and the authentication performance is evaluated across various types of touch operations, varying operation lengths, different application tasks, and different application scenarios. The extensive experimental results are included to show that touch-interaction behavior exhibits sufficient discriminability and stability among smartphone users for active authentication, and achieves equal-error rates between 1.72% and 9.01% for different types of touch operations with the operation length of 11; the authentication accuracies improve when having long observation or small timespan between the training and testing phases, and express more reliably and stably in a specific task than in the free task. We also discuss a number of avenues for additional research that we believe are necessary to advance the state-of-the-art in this area.},
  Doi                      = {10.1109/TIFS.2015.2503258},
  File                     = {Shen-2016-p498-513.pdf:References\\Shen-2016-p498-513.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {biometrics (access control);data privacy;feature extraction;neural nets;reliability;smart phones;support vector machines;touch sensitive screens;active smartphone authentication;data privacy;dynamic feature extraction;neural network;performance analysis;reliability;static feature extraction;support vector machine;touch-interaction behavior;touchscreen smartphone;Accuracy;Authentication;Feature extraction;Sensors;Stability analysis;Support vector machines;Biometrics;active authentication;performance evaluation;touch interaction},
  Reviewtime               = {315},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7335628}
}

@Article{Shu-2016-p528-542,
  Title                    = {Fast Detection of Transformed Data Leaks},
  Author                   = {X. Shu and J. Zhang and D. D. Yao and W. C. Feng},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {528-542},
  Volume                   = {11},

  Abstract                 = {The leak of sensitive data on computer systems poses a serious threat to organizational security. Statistics show that the lack of proper encryption on files and communications due to human errors is one of the leading causes of data loss. Organizations need tools to identify the exposure of sensitive data by screening the content in storage and transmission, i.e., to detect sensitive information being stored or transmitted in the clear. However, detecting the exposure of sensitive information is challenging due to data transformation in the content. Transformations (such as insertion and deletion) result in highly unpredictable leak patterns. In this paper, we utilize sequence alignment techniques for detecting complex data-leak patterns. Our algorithm is designed for detecting long and inexact sensitive data patterns. This detection is paired with a comparable sampling algorithm, which allows one to compare the similarity of two separately sampled sequences. Our system achieves good detection accuracy in recognizing transformed leaks. We implement a parallelized version of our algorithms in graphics processing unit that achieves high analysis throughput. We demonstrate the high multithreading scalability of our data leak detection method required by a sizable organization.},
  Doi                      = {10.1109/TIFS.2015.2503271},
  File                     = {Shu-2016-p528-542.pdf:References\\Shu-2016-p528-542.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptography;multi-threading;sampling methods;complex data-leak pattern detection;computer systems;content screening;data leak detection method;data loss;data storage;data transformation;data transmission;encryption;multithreading scalability;organizational security;sampled sequences;sampling algorithm;sensitive data leakage;sensitive information detection;sequence alignment techniques;sizable organization;unpredictable leak patterns;Algorithm design and analysis;Automata;Heuristic algorithms;Leak detection;Organizations;Pattern matching;Security;Data leak detection;alignment;content inspection;dynamic programming;parallelism;sampling},
  Reviewtime               = {129},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7336560}
}

@Article{Sitova-2016-p877-892,
  Title                    = {HMOG: New Behavioral Biometric Features for Continuous Authentication of Smartphone Users},
  Author                   = {Z. SitovÃ¡ and J. Å edÄ›nka and Q. Yang and G. Peng and G. Zhou and P. Gasti and K. S. Balagani},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {877-892},
  Volume                   = {11},

  Abstract                 = {We introduce hand movement, orientation, and grasp (HMOG), a set of behavioral features to continuously authenticate smartphone users. HMOG features unobtrusively capture subtle micro-movement and orientation dynamics resulting from how a user grasps, holds, and taps on the smartphone. We evaluated authentication and biometric key generation (BKG) performance of HMOG features on data collected from 100 subjects typing on a virtual keyboard. Data were collected under two conditions: 1) sitting and 2) walking. We achieved authentication equal error rates (EERs) as low as 7.16% (walking) and 10.05% (sitting) when we combined HMOG, tap, and keystroke features. We performed experiments to investigate why HMOG features perform well during walking. Our results suggest that this is due to the ability of HMOG features to capture distinctive body movements caused by walking, in addition to the hand-movement dynamics from taps. With BKG, we achieved the EERs of 15.1% using HMOG combined with taps. In comparison, BKG using tap, key hold, and swipe features had EERs between 25.7% and 34.2%. We also analyzed the energy consumption of HMOG feature extraction and computation. Our analysis shows that HMOG features extracted at a 16-Hz sensor sampling rate incurred a minor overhead of 7.9% without sacrificing authentication accuracy. Two points distinguish our work from current literature: 1) we present the results of a comprehensive evaluation of three types of features (HMOG, keystroke, and tap) and their combinations under the same experimental conditions and 2) we analyze the features from three perspectives (authentication, BKG, and energy consumption on smartphones).},
  Doi                      = {10.1109/TIFS.2015.2506542},
  File                     = {Sitova-2016-p877-892.pdf:References\\Sitova-2016-p877-892.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {message authentication;smart phones;HMOG feature extraction;authentication equal error rates;behavioral biometric features;biometric key generation performance;continuous authentication;hand movement orientation and grasp;smartphone users;Accelerometers;Authentication;Feature extraction;Gyroscopes;Legged locomotion;Magnetometers;Resistance;Behavioral biometrics;HMOG;biometric key generation;continuous authentication;energy evaluation},
  Reviewtime               = {318},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7349202}
}

@Article{Sun-2016-p937-950,
  Title                    = {Complementary Cohort Strategy for Multimodal Face Pair Matching},
  Author                   = {Y. Sun and K. Nasrollahi and Z. Sun and T. Tan},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {937-950},
  Volume                   = {11},

  Abstract                 = {Face pair matching is the task of determining whether two face images represent the same person. Due to the limited expressive information embedded in the two face images as well as various sources of facial variations, it becomes a quite difficult problem. Toward the issue of few available images provided to represent each face, we propose to exploit an extra cohort set (identities in the cohort set are different from those being compared) by a series of cohort list comparisons. Useful cohort coefficients are then extracted from both sorted cohort identities and sorted cohort images for complementary information. To augment its robustness to complicated facial variations, we further employ multiple face modalities owing to their complementary value to each other for the face pair matching task. The final decision is made by fusing the extracted cohort coefficients with the direct matching score for all the available face modalities. To investigate the capacity of each individual modality on matching faces, the cohort behavior, and the performance achieved using our complementary cohort strategy, we conduct a set of experiments on two recently collected multimodal face databases. It is shown that using different modalities leads to different face pair matching performance. For each modality, employing our cohort scheme significantly reduces the equal error rate. By applying the proposed multimodal complementary cohort strategy, we achieve the best performance on our face pair matching task.},
  Doi                      = {10.1109/TIFS.2015.2512561},
  File                     = {Sun-2016-p937-950.pdf:References\\Sun-2016-p937-950.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {face recognition;image matching;image representation;cohort coefficients;complementary cohort strategy;complementary information;face image representation;face modalities;facial variation;matching score;multimodal face database;multimodal face pair matching;Computer vision;Databases;Face;Face recognition;Sun;Three-dimensional displays;Training;Face recognition;RGB-D;cohort information;multimodal fusion},
  Reviewtime               = {165},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7366578}
}

@Article{Taha-2016-p811-822,
  Title                    = {SIIMCO: A Forensic Investigation Tool for Identifying the Influential Members of a Criminal Organization},
  Author                   = {K. Taha and P. D. Yoo},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {811-822},
  Volume                   = {11},

  Abstract                 = {Members of a criminal organization, who hold central positions in the organization, are usually targeted by criminal investigators for removal or surveillance. This is because they play key and influential roles by acting as commanders, who issue instructions or serve as gatekeepers. Removing these central members (i.e., influential members) is most likely to disrupt the organization and put it out of business. Most often, criminal investigators are even more interested in knowing the portion of these influential members, who are the immediate leaders of lower level criminals. These lower level criminals are the ones who usually carry out the criminal works; therefore, they are easier to identify. The ultimate goal of investigators is to identify the immediate leaders of these lower level criminals in order to disrupt future crimes. We propose, in this paper, a forensic analysis system called SIIMCO that can identify the influential members of a criminal organization. Given a list of lower level criminals in a criminal organization, SIIMCO can also identify the immediate leaders of these criminals. SIIMCO first constructs a network representing a criminal organization from either mobile communication data that belongs to the organization or crime incident reports. It adopts the concept space approach to automatically construct a network from crime incident reports. In such a network, a vertex represents an individual criminal, and a link represents the relationship between two criminals. SIIMCO employs formulas that quantify the degree of influence/importance of each vertex in the network relative to all other vertices. We present these formulas through a series of refinements. All the formulas incorporate novel-weighting schemes for the edges of networks. We evaluated the quality of SIIMCO by comparing it experimentally with two other systems. Results showed marked improvement.},
  Doi                      = {10.1109/TIFS.2015.2510826},
  File                     = {Taha-2016-p811-822.pdf:References\\Taha-2016-p811-822.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {criminal law;mobile computing;social networking (online);surveillance;SIIMCO;crime incident reports;criminal organization;forensic investigation tool;mobile communication data;surveillance;Digital forensics;Mobile communication;Mobile handsets;Organizations;Social network services;Standards organizations;Forensic investigation;central nodes;criminal network;digital forensic;forensic analysis;mobile communication data;relative importance;social network},
  Reviewtime               = {61},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7361998}
}

@Article{Tang-2016-p734-745,
  Title                    = {Adaptive Steganalysis Based on Embedding Probabilities of Pixels},
  Author                   = {W. Tang and H. Li and W. Luo and J. Huang},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {734-745},
  Volume                   = {11},

  Abstract                 = {In modern steganography, embedding modifications are highly concentrated on the textural regions within an image, as such regions are difficult to model for steganalysis. Previous studies have shown that compared with non-adaptive strategies, this content adaptive strategy achieves stronger security against existing steganalysis. Based on the experiments and analyses, however, we found that this embedding property would inevitably lead to a large limitation in existing adaptive steganography. That is, it is possible for steganalyzers to estimate the regions that have probably been modified after data hiding. In this paper, we propose an adaptive steganalytic scheme based on embedding probabilities of pixels. The main idea of our scheme is that we assign different weights to different pixels in feature extraction. For those pixels with high embedding probabilities, their corresponding weights are larger, since they should contribute more to steganalysis and vice versa. By doing so, we can concentrate our attention on the regions that have probably been modified and significantly reduce the impact of other unchanged smooth regions. It is expected that our proposed method is an improvement on the existing steganalytic methods, which usually assume every pixel has the same contribution to steganalysis. The extensive experiments evaluated on four typical adaptive steganographic methods have shown the effectiveness of the proposed scheme, especially for low embedding rates, for example, lower than 0.20 bpp.},
  Doi                      = {10.1109/TIFS.2015.2507159},
  File                     = {Tang-2016-p734-745.pdf:References\\Tang-2016-p734-745.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {image coding;probability;steganography;adaptive steganalytic scheme;image steganography;pixel embedding probability;Adaptation models;Correlation;Distortion;Feature extraction;Image edge detection;Security;Sun;Adaptive Steganalysis;Adaptive Steganography;Adaptive steganography;Embedding Probabilities;Re-Embedding;adaptive steganalysis;embedding probabilities;re-embedding},
  Reviewtime               = {195},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7350138}
}

@Article{Tang-2016-p200-214,
  Title                    = {Robust Image Hashing With Ring Partition and Invariant Vector Distance},
  Author                   = {Z. Tang and X. Zhang and X. Li and S. Zhang},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {200-214},
  Volume                   = {11},

  Abstract                 = {Robustness and discrimination are two of the most important objectives in image hashing. We incorporate ring partition and invariant vector distance to image hashing algorithm for enhancing rotation robustness and discriminative capability. As ring partition is unrelated to image rotation, the statistical features that are extracted from image rings in perceptually uniform color space, i.e., CIE L*a*b* color space, are rotation invariant and stable. In particular, the Euclidean distance between vectors of these perceptual features is invariant to commonly used digital operations to images (e.g., JPEG compression, gamma correction, and brightness/contrast adjustment), which helps in making image hash compact and discriminative. We conduct experiments to evaluate the efficiency with 250 color images, and demonstrate that the proposed hashing algorithm is robust at commonly used digital operations to images. In addition, with the receiver operating characteristics curve, we illustrate that our hashing is much better than the existing popular hashing algorithms at robustness and discrimination.},
  Doi                      = {10.1109/TIFS.2015.2485163},
  File                     = {Tang-2016-p200-214.pdf:References\\Tang-2016-p200-214.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptography;feature extraction;image coding;image colour analysis;vectors;Euclidean distance;color images;digital operations;image hashing;invariant vector distance;receiver operating characteristics curve;ring partition;statistical feature extraction;Discrete cosine transforms;Feature extraction;Image coding;Image color analysis;Robustness;Standards;Transform coding;CIE L*a*b* color space;Image hashing;image rotation;invariant distance;ring partition},
  Reviewtime               = {325},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7286814}
}

@Article{Tkachenko-2016-p571-583,
  Title                    = {Two-Level QR Code for Private Message Sharing and Document Authentication},
  Author                   = {I. Tkachenko and W. Puech and C. Destruel and O. Strauss and J. M. Gaudin and C. Guichard},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {571-583},
  Volume                   = {11},

  Abstract                 = {The quick response (QR) code was designed for storage information and high-speed reading applications. In this paper, we present a new rich QR code that has two storage levels and can be used for document authentication. This new rich QR code, named two-level QR code, has public and private storage levels. The public level is the same as the standard QR code storage level; therefore, it is readable by any classical QR code application. The private level is constructed by replacing the black modules by specific textured patterns. It consists of information encoded using q-ary code with an error correction capacity. This allows us not only to increase the storage capacity of the QR code, but also to distinguish the original document from a copy. This authentication is due to the sensitivity of the used patterns to the print-and-scan (P&S) process. The pattern recognition method that we use to read the second-level information can be used both in a private message sharing and in an authentication scenario. It is based on maximizing the correlation values between P&S degraded patterns and reference patterns. The storage capacity can be significantly improved by increasing the code alphabet q or by increasing the textured pattern size. The experimental results show a perfect restoration of private information. It also highlights the possibility of using this new rich QR code for document authentication.},
  Doi                      = {10.1109/TIFS.2015.2506546},
  File                     = {Tkachenko-2016-p571-583.pdf:References\\Tkachenko-2016-p571-583.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {QR codes;data privacy;document handling;feature extraction;message authentication;P&S degraded patterns;P&S process;code alphabet;correlation value maximization;document authentication;error correction capacity;information encoding;pattern recognition method;print-and-scan process;private information restoration;private message sharing;private storage level;public storage level;q-ary code;quick response code;reference patterns;second-level information read;standard QR code storage level;textured pattern size;textured patterns;two-level QR code;Authentication;Distortion;Error correction codes;Image color analysis;Industries;Printers;Standards;QR code;document authentication;pattern recognition;print-and-scan process;private message;two storage levels},
  Reviewtime               = {165},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7349185}
}

@Article{Tongaonkar-2016-p468-483,
  Title                    = {Condition Factorization: A Technique for Building Fast and Compact Packet Matching Automata},
  Author                   = {A. Tongaonkar and R. Sekar},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {468-483},
  Volume                   = {11},

  Abstract                 = {Rule-based matching on network packet headers is a central problem in firewalls, and network intrusion, monitoring, and access-control systems. To enhance performance, rules are typically compiled into a matching automaton that can quickly identify the subset of rules that are applicable to a given network packet. While deterministic automata provide the best performance, previous research has shown that such automata can be exponential in the size and/or number of rules. Nondeterministic automata can avoid size explosion, but their matching time can increase quickly with the number of rules. In contrast, we present a new technique that constructs polynomial size automata. Moreover, we show that the matching time of our automata is insensitive to the number of rules. The key idea in our approach is that of decomposing and reordering the tests on packet header fields so that the result of performing a test can be utilized on behalf of many rules. Our experiments demonstrate major reductions in space requirements over previous techniques, as well as significant improvements in matching speed. Our technique can uniformly handle prioritized and unprioritized rules, and support applications that require single-match as well as multi-match.},
  Doi                      = {10.1109/TIFS.2015.2489182},
  File                     = {Tongaonkar-2016-p468-483.pdf:References\\Tongaonkar-2016-p468-483.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {authorisation;deterministic automata;firewalls;access-control system;compact packet matching automata;condition factorization;deterministic automata;intrusion detection;nondeterministic automata;rule-based matching;Automata;Firewalls (computing);Intrusion detection;Monitoring;Payloads;Polynomials;Runtime;Firewalls;Intrusion Detection Systems;Network Monitoring;Packet Classification;Packet classification;firewalls;intrusion detection systems;network monitoring},
  Reviewtime               = {359},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7295611}
}

@Article{Wang-2016-p704-719,
  Title                    = {Geometric Range Search on Encrypted Spatial Data},
  Author                   = {B. Wang and M. Li and H. Wang},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {704-719},
  Volume                   = {11},

  Abstract                 = {Geometric range search is a fundamental primitive for spatial data analysis in SQL and NoSQL databases. It has extensive applications in location-based services, computer-aided design, and computational geometry. Due to the dramatic increase in data size, it is necessary for companies and organizations to outsource their spatial data sets to third-party cloud services (e.g., Amazon) in order to reduce storage and query processing costs, but, meanwhile, with the promise of no privacy leakage to the third party. Searchable encryption is a technique to perform meaningful queries on encrypted data without revealing privacy. However, geometric range search on spatial data has not been fully investigated nor supported by existing searchable encryption schemes. In this paper, we design a symmetric-key searchable encryption scheme that can support geometric range queries on encrypted spatial data. One of our major contributions is that our design is a general approach, which can support different types of geometric range queries. In other words, our design on encrypted data is independent from the shapes of geometric range queries. Moreover, we further extend our scheme with the additional use of tree structures to achieve search complexity that is faster than linear. We formally define and prove the security of our scheme with indistinguishability under selective chosen-plaintext attacks, and demonstrate the performance of our scheme with experiments in a real cloud platform (Amazon EC2).},
  Doi                      = {10.1109/TIFS.2015.2506145},
  File                     = {Wang-2016-p704-719.pdf:References\\Wang-2016-p704-719.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {SQL;computational geometry;cryptography;query processing;NoSQL database;SQL database;computational geometry;computer-aided design;encrypted spatial data;geometric range search;location-based services;query processing costs;searchable encryption;selective chosen-plaintext attacks;storage reduction;symmetric-key searchable encryption scheme;Cloud computing;Companies;Data privacy;Encryption;Shape;Spatial databases;Geometric range search;encrypted data;spatial data},
  Reviewtime               = {185},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7347418}
}

@Article{Wang-2016-p1151-1164,
  Title                    = {Against Double Fault Attacks: Injection Effort Model, Space and Time Randomization Based Countermeasures for Reconfigurable Array Architecture},
  Author                   = {B. Wang and L. Liu and C. Deng and M. Zhu and S. Yin and S. Wei},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1151-1164},
  Volume                   = {11},

  Abstract                 = {With the increasing accuracy of fault injections, it has become possible to inject two faults into specific circuit regions precisely at a certain time. Unfortunately, most existing fault attack countermeasures are based on the single fault assumption, and it is, therefore, very difficult to resist double fault attacks. Reconfigurable array architecture (RAA) has the ability to introduce spatial and time randomness by dynamic reconfiguration, which can alleviate the threat of double fault attacks. This paper, for the first time, analyzes the double fault attack issues in the fault injection phase systematically. An evaluation model, named injection effort model (IEM), is proposed to quantify the efforts of a successful fault injection. In IEM, the real injection process is described mathematically using the probability method, so that a theoretical basis can be provided for the corresponding countermeasure design. Based on the concept of spatial and time randomization, three countermeasures are implemented on RAA for the purpose of decreasing the implementation overhead under the premise of ensuring the security. When these countermeasures are adopted, tradeoffs can be made between the double fault resistance and the extra overhead through changing the degree of randomness. Experiments are carried out to analyze the relationship between the resistance and the overhead using Advanced Encryption Standard (AES), Data Encryption Standard (DES), and Camellia. When the overhead constraints in terms of throughput, hardware resources, and energy are 5%, 35%, and 10% respectively, the double fault resistance can increase by two to four orders of magnitude (ranging from 824 to 10 149 for different algorithms).},
  Doi                      = {10.1109/TIFS.2016.2518130},
  File                     = {Wang-2016-p1151-1164.pdf:References\\Wang-2016-p1151-1164.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptography;probability;reconfigurable architectures;AES;Camellia;DES;IEM;RAA;advanced encryption standard;data encryption standard;double fault attacks;double fault resistance;dynamic reconfiguration;fault injections;injection effort model;probability method;reconfigurable array architecture;space randomization based countermeasures;time randomization based countermeasures;Arrays;Circuit faults;Hardware;Redundancy;Resistance;Resists;Security;Double fault attacks;countermeasures against double fault attacks;injection effort model (IEM);reconfigurable array architecture (RAA)},
  Reviewtime               = {172},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7383285}
}

@Article{Wang-2016-p1165-1176,
  Title                    = {Identity-Based Proxy-Oriented Data Uploading and Remote Data Integrity Checking in Public Cloud},
  Author                   = {H. Wang and D. He and S. Tang},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1165-1176},
  Volume                   = {11},

  Abstract                 = {More and more clients would like to store their data to public cloud servers (PCSs) along with the rapid development of cloud computing. New security problems have to be solved in order to help more clients process their data in public cloud. When the client is restricted to access PCS, he will delegate its proxy to process his data and upload them. On the other hand, remote data integrity checking is also an important security problem in public cloud storage. It makes the clients check whether their outsourced data are kept intact without downloading the whole data. From the security problems, we propose a novel proxy-oriented data uploading and remote data integrity checking model in identity-based public key cryptography: identity-based proxy-oriented data uploading and remote data integrity checking in public cloud (ID-PUIC). We give the formal definition, system model, and security model. Then, a concrete ID-PUIC protocol is designed using the bilinear pairings. The proposed ID-PUIC protocol is provably secure based on the hardness of computational Diffie-Hellman problem. Our ID-PUIC protocol is also efficient and flexible. Based on the original client's authorization, the proposed ID-PUIC protocol can realize private remote data integrity checking, delegated remote data integrity checking, and public remote data integrity checking.},
  Doi                      = {10.1109/TIFS.2016.2520886},
  File                     = {Wang-2016-p1165-1176.pdf:References\\Wang-2016-p1165-1176.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cloud computing;cryptographic protocols;data integrity;data privacy;file servers;outsourcing;public key cryptography;PCS;cloud computing;computational Diffie-Hellman problem;concrete ID-PUIC protocol;delegated remote data integrity checking;identity-based proxy-oriented data uploading;identity-based public key cryptography;outsourced data;private remote data integrity checking;proxy-oriented data uploading;public cloud servers;public cloud storage;public remote data integrity checking;security problems;Cloud computing;Data models;Protocols;Public key cryptography;Servers;Cloud computing;Identity-based cryptography;Proxy public key cryptography;Remote data integrity checking;identity-based cryptography;proxy public key cryptography;remote data integrity checking},
  Reviewtime               = {101},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7389383}
}

@Article{Wang-2016-p1265-1277,
  Title                    = {An Efficient File Hierarchy Attribute-Based Encryption Scheme in Cloud Computing},
  Author                   = {S. Wang and J. Zhou and J. K. Liu and J. Yu and J. Chen and W. Xie},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1265-1277},
  Volume                   = {11},

  Abstract                 = {Ciphertext-policy attribute-based encryption (CP-ABE) has been a preferred encryption technology to solve the challenging problem of secure data sharing in cloud computing. The shared data files generally have the characteristic of multilevel hierarchy, particularly in the area of healthcare and the military. However, the hierarchy structure of shared files has not been explored in CP-ABE. In this paper, an efficient file hierarchy attribute-based encryption scheme is proposed in cloud computing. The layered access structures are integrated into a single access structure, and then, the hierarchical files are encrypted with the integrated access structure. The ciphertext components related to attributes could be shared by the files. Therefore, both ciphertext storage and time cost of encryption are saved. Moreover, the proposed scheme is proved to be secure under the standard assumption. Experimental simulation shows that the proposed scheme is highly efficient in terms of encryption and decryption. With the number of the files increasing, the advantages of our scheme become more and more conspicuous.},
  Doi                      = {10.1109/TIFS.2016.2523941},
  File                     = {Wang-2016-p1265-1277.pdf:References\\Wang-2016-p1265-1277.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cloud computing;cryptography;storage management;CP-ABE;ciphertext storage saving;ciphertext-policy attribute-based encryption;cloud computing;file hierarchy attribute-based encryption scheme;integrated access structure;layered access structures;secure data sharing;time cost saving;Authorization;Cardiology;Cloud computing;Encryption;Periodic structures;Attribute-based encryption;Ciphertext-policy;Cloud computing;Data sharing;File Hierarchy;attribute-based encryption;ciphertext-policy;data sharing;file hierarchy},
  Reviewtime               = {165},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7401059}
}

@Article{Werghi-2016-p964-979,
  Title                    = {Boosting 3D LBP-Based Face Recognition by Fusing Shape and Texture Descriptors on the Mesh},
  Author                   = {N. Werghi and C. Tortorici and S. Berretti and A. Del Bimbo},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {964-979},
  Volume                   = {11},

  Abstract                 = {In this paper, we present a novel approach for fusing shape and texture local binary patterns (LBPs) on a mesh for 3D face recognition. Using a recently proposed framework, we compute LBP directly on the face mesh surface, then we construct a grid of the regions on the facial surface that can accommodate global and partial descriptions. Compared with its depth-image counterpart, our approach is distinguished by the following features: 1) inherits the intrinsic advantages of mesh surface (e.g., preservation of the full geometry); 2) does not require normalization; and 3) can accommodate partial matching. In addition, it allows early level fusion of texture and shape modalities. Through experiments conducted on the BU-3DFE and Bosphorus databases, we assess different variants of our approach with regard to facial expressions and missing data, also in comparison to the state-of-the-art solutions.},
  Doi                      = {10.1109/TIFS.2016.2515505},
  File                     = {Werghi-2016-p964-979.pdf:References\\Werghi-2016-p964-979.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {face recognition;feature extraction;image fusion;image matching;image texture;3D LBP-based face recognition;BU-3DFE database;Bosphorus database;LBP fusion;depth-image counterpart;face mesh surface;facial surface;local binary patterns;partial matching;shape descriptor;texture descriptor;Boosting;Face;Face recognition;Geometry;Histograms;Shape;Three-dimensional displays;3D face recognition;Mesh-LBP;feature and score fusion;mesh-LBP},
  Reviewtime               = {123},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7373633}
}

@Article{Wu-2016-p1278-1290,
  Title                    = {Artificial-Noise-Aided Message Authentication Codes With Information-Theoretic Security},
  Author                   = {X. Wu and Z. Yang and C. Ling and X. G. Xia},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1278-1290},
  Volume                   = {11},

  Abstract                 = {In the past, two main approaches for the purpose of authentication, including information-theoretic authentication codes and complexity-theoretic message authentication codes (MACs), were almost independently developed. In this paper, we consider to construct new MACs, which are both computationally secure and information-theoretically secure. Essentially, we propose a new cryptographic primitive, namely, artificial-noise-aided MACs (ANA-MACs), where artificial noise is used to interfere with the complexity-theoretic MACs and quantization is further employed to facilitate packet-based transmission. With a channel coding formulation of key recovery in the MACs, the generation of standard authentication tags can be seen as an encoding process for the ensemble of codes, where the shared key between Alice and Bob is considered as the input and the message is used to specify a code from the ensemble of codes. Then, we show that artificial noise in ANA-MACs can be well employed to resist the key recovery attack even if the opponent has an unlimited computing power. Finally, a pragmatic approach for the analysis of ANA-MACs is provided, and we show how to balance the three performance metrics, including the completeness error, the false acceptance probability, and the conditional equivocation about the key. The analysis can be well applied to a class of ANA-MACs, where MACs with Rijndael cipher are employed.},
  Doi                      = {10.1109/TIFS.2016.2524514},
  File                     = {Wu-2016-p1278-1290.pdf:References\\Wu-2016-p1278-1290.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {channel coding;cryptography;information theory;message authentication;ANA-MAC;Rijndael cipher;artificial-noise-aided message authentication codes;channel coding;completeness error;complexity-theoretic MAC;conditional equivocation;cryptographic primitive;false acceptance probability;information-theoretic security;key recovery attack;packet-based transmission;performance metrics;quantization;standard authentication tags generation;Authentication;Cryptography;Message authentication;Physical layer;Receivers;Transmitters;Information-theoretic authentication codes;channel coding and decoding;information-theoretic security;message authentication codes},
  Reviewtime               = {107},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7398045}
}

@Article{Xiang-2016-p951-963,
  Title                    = {Perceptual Visual Security Index Based on Edge and Texture Similarities},
  Author                   = {T. Xiang and S. Guo and X. Li},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {951-963},
  Volume                   = {11},

  Abstract                 = {With the development in recent decades of various efficient image encryption algorithms, such as selective encryption, a great demand has arisen for methods of evaluating the visual security of encrypted images. Existing solutions usually adopt well-known metrics of visual quality assessment to measure the quality of encrypted images, but they often exhibit undesired behavior on perceptually encrypted images of low quality. In this paper, we propose a novel visual security index (VSI) based on the human visual system. The proposed VSI evaluates two aspects of the content similarity between plain and encrypted images: the edge similarity extracted via multi-threshold edge detection and the texture similarity measured by means of the co-occurrence matrix. These two components are further integrated to obtain the proposed VSI through adaptive similarity weighting. Extensive experiments were performed on two publicly available image databases. Our experimental results demonstrate that compared with many existing state-of-the-art visual security metrics, the proposed VSI exhibits a better performance and stability on low-quality images.},
  Doi                      = {10.1109/TIFS.2016.2515503},
  File                     = {Xiang-2016-p951-963.pdf:References\\Xiang-2016-p951-963.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptography;edge detection;image texture;matrix algebra;VSI;co-occurrence matrix;content similarity;edge similarity;encrypted image quality;human visual system;image databases;image encryption algorithms;low-quality image;multithreshold edge detection;perceptual visual security index;selective encryption;similarity weighting;texture similarity;visual quality assessment;visual security evaluation;Encryption;Image edge detection;Indexes;Measurement;Visualization;Edge Detection;Texture Features;Visual Security Index;Visual security index;edge detection;human visual system;selective encryption;texture features},
  Reviewtime               = {239},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7373638}
}

@Article{Xiong-2016-p1017-1026,
  Title                    = {Secure Transmission Against Pilot Spoofing Attack: A Two-Way Training-Based Scheme},
  Author                   = {Q. Xiong and Y. C. Liang and K. H. Li and Y. Gong and S. Han},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {1017-1026},
  Volume                   = {11},

  Abstract                 = {The pilot spoofing attack is one kind of active eavesdropping activities conducted by a malicious user during the channel training phase. By transmitting the identical pilot (training) signals as those of the legal users, such an attack is able to manipulate the channel estimation outcome, which may result in a larger channel rate for the adversary but a smaller channel rate for the legitimate receiver. With the intention of detecting the pilot spoofing attack and minimizing its damages, we design a two-way training-based scheme. The effective detector exploits the intrusive component created by the adversary, followed by a secure beamforming-assisted data transmission. In addition to the solid detection performance, this scheme is also capable of obtaining the estimations of both legitimate and illegitimate channels, which allows the users to achieve secure communication in the presence of pilot spoofing attack. The detection probability is evaluated based on the derived test threshold at a given requirement on the probability of false alarming. The achievable secrecy rate is utilized to measure the security level of the data transmission. Our analysis shows that even without any pre-assumed knowledge of eavesdropper, the proposed scheme is still able to achieve the maximal secrecy rate in certain cases. Numerical results are provided to show that our scheme could achieve a high detection probability as well as secure transmission.},
  Doi                      = {10.1109/TIFS.2016.2516825},
  File                     = {Xiong-2016-p1017-1026.pdf:References\\Xiong-2016-p1017-1026.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {probability;security of data;active eavesdropping activities;channel estimation;channel training phase;detection probability;intrusive component;pilot spoofing attack;secure beamforming-assisted data transmission;secure transmission;two-way training-based scheme;Channel estimation;Data communication;Downlink;Receivers;Training;Transmitters;Uplink;Physical layer security;active eavesdropping;pilot spoofing attack;two-way training method},
  Reviewtime               = {199},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7378474}
}

@Article{Xu-2016-p1252-1264,
  Title                    = {ICCDetector: ICC-Based Malware Detection on Android},
  Author                   = {K. Xu and Y. Li and R. H. Deng},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1252-1264},
  Volume                   = {11},

  Abstract                 = {Most existing mobile malware detection methods (e.g., Kirin and DroidMat) are designed based on the resources required by malwares (e.g., permissions, application programming interface (API) calls, and system calls). These methods capture the interactions between mobile apps and Android system, but ignore the communications among components within or cross application boundaries. As a consequence, the majority of the existing methods are less effective in identifying many typical malwares, which require a few or no suspicious resources, but leverage on inter-component communication (ICC) mechanism when launching stealthy attacks. To address this challenge, we propose a new malware detection method, named ICCDetector. ICCDetector outputs a detection model after training with a set of benign apps and a set of malwares, and employs the trained model for malware detection. The performance of ICCDetector is evaluated with 5264 malwares, and 12026 benign apps. Compared with our benchmark, which is a permission-based method proposed by Peng et al. in 2012 with an accuracy up to 88.2%, ICCDetector achieves an accuracy of 97.4%, roughly 10% higher than the benchmark, with a lower false positive rate of 0.67%, which is only about a half of the benchmark. After manually analyzing false positives, we discover 43 new malwares from the benign data set, and reduce the number of false positives to seven. More importantly, ICCDetector discovers 1708 more advanced malwares than the benchmark, while it misses 220 obvious malwares, which can be easily detected by the benchmark. For the detected malwares, ICCDetector further classifies them into five newly defined malware categories, which help understand the relationship between malicious behaviors and ICC characteristics. We also provide a systemic analysis of ICC patterns of benign apps and malwares.},
  Doi                      = {10.1109/TIFS.2016.2523912},
  File                     = {Xu-2016-p1252-1264.pdf:References\\Xu-2016-p1252-1264.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {invasive software;mobile computing;smart phones;API calls;Android system;DroidMat;ICC characteristics;ICC-based malware detection;ICCDetector;Kirin;application programming interface calls;benign data set;intercomponent communication mechanism;malicious behaviors;mobile apps;mobile malware detection methods;permission-based method;stealthy attacks;system calls;Androids;Benchmark testing;Humanoid robots;Malware;Matched filters;Receivers;Registers;Android;ICC;Malware Detection;malware detection},
  Reviewtime               = {196},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7398053}
}

@Article{Xu-2016-p1139-1150,
  Title                    = {Simultaneously Generating Secret and Private Keys in a Cooperative Pairwise-Independent Network},
  Author                   = {P. Xu and Z. Ding and X. Dai and G. K. Karagiannidis},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1139-1150},
  Volume                   = {11},

  Abstract                 = {This paper studies the problem of simultaneously generating a secret key (SK) and a private key (PK) between Alice and Bob, in a cooperative pairwise-independent network (PIN) with two relays. In the PIN, the pairwise source observed by every pair of terminals is independent of those sources observed by any other pairs. The SK needs to be protected from Eve, while the PK needs to be protected not only from Eve but also from the two relays. Two cooperative SK-PK generation algorithms are proposed: both of them first generate common randomness, based on the well-established pairwise key generation technique and the application of the one-time pad; but then, the two algorithms utilize the XOR operation and a specific random-binning-based SK-PK codebook to generate the expected keys, respectively. The achievable SK-PK rate regions of both the two proposed algorithms are analyzed. Of particular interest is the second algorithm with random-bing based codebook, whose achievable key rate region is demonstrated to be exactly the same as the derived outer bound, a crucial step for establishing the key capacity of this PIN model. Finally, the two proposed SK-PK generation algorithms are extended to a cooperative wireless network, where the correlated source observations are obtained from estimating wireless channels during a training phase.},
  Doi                      = {10.1109/TIFS.2016.2516970},
  File                     = {Xu-2016-p1139-1150.pdf:References\\Xu-2016-p1139-1150.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cooperative communication;private key cryptography;random processes;wireless channels;XOR operation;common randomness generation;cooperative PIN;cooperative SK-PK generation algorithm;cooperative pairwise-independent network;cooperative wireless network;random-binning-based SK-PK codebook;simultaneous secret and private key generation;wireless channels;Algorithm design and analysis;Communication system security;Random variables;Relays;Security;Wireless networks;Information-theoretic security;cooperative PIN model;key capacity region;private key;secret key},
  Reviewtime               = {264},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7378467}
}

@Article{Xu-2016-p373-387,
  Title                    = {Secure Transmission Design for Cognitive Radio Networks With Poisson Distributed Eavesdroppers},
  Author                   = {X. Xu and B. He and W. Yang and X. Zhou and Y. Cai},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {373-387},
  Volume                   = {11},

  Abstract                 = {In this paper, we study physical layer security in an underlay cognitive radio (CR) network. We consider the problem of secure communication between a secondary transmitter-receiver pair in the presence of randomly distributed eavesdroppers under an interference constraint set by the primary user. For different channel knowledge assumptions at the transmitter, we design four transmission protocols to achieve the secure transmission in the CR network. We give a comprehensive performance analysis for each protocol in terms of transmission delay, security, reliability, and the overall secrecy throughput. Furthermore, we determine the optimal design parameter for each transmission protocol by solving the optimization problem of maximizing the secrecy throughput subject to both security and reliability constraints. Numerical results illustrate the performance comparison between different transmission protocols.},
  Doi                      = {10.1109/TIFS.2015.2500178},
  File                     = {Xu-2016-p373-387.pdf:References\\Xu-2016-p373-387.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {Poisson distribution;cognitive radio;protocols;radio receivers;radio transmitters;telecommunication security;Poisson distributed eavesdroppers;cognitive radio networks;physical layer security;secondary transmitter-receiver;secure transmission design;transmission protocols;Fading;Measurement;Protocols;Reliability;Security;Throughput;Wireless networks;Physical layer security;cognitive radio networks;on-off transmission;secrecy guard zone},
  Reviewtime               = {201},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7328286}
}

@Article{Yan-2016-p1486-1499,
  Title                    = {Jamming Resilient Communication Using MIMO Interference Cancellation},
  Author                   = {Q. Yan and H. Zeng and T. Jiang and M. Li and W. Lou and Y. T. Hou},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1486-1499},
  Volume                   = {11},

  Abstract                 = {Jamming attack is a serious threat to the wireless communications. Reactive jamming maximizes the attack efficiency by jamming only when the targets are communicating, which can be readily implemented using software-defined radios. In this paper, we explore the use of the multi-input multi-output (MIMO) technology to achieve jamming resilient orthogonal frequency-division multiplexing (OFDM) communication. In particular, MIMO interference cancellation treats jamming signals as noise and strategically cancels them out, while transmit precoding adjusts the signal directions to optimize the decoding performance. We first investigate the reactive jamming strategies and their impacts on the MIMO-OFDM receivers. We then present a MIMO-based anti-jamming scheme that exploits MIMO interference cancellation and transmit precoding technologies to turn a jammed non-connectivity scenario into an operational network. We implement our jamming resilient communication scheme using software-defined radios. Our testbed evaluation shows the destructive power of reactive jamming attack, and also validates the efficacy and efficiency of our defense mechanisms in the presence of numerous types of reactive jammers with different jamming signal powers.},
  Doi                      = {10.1109/TIFS.2016.2535906},
  File                     = {Yan-2016-p1486-1499.pdf:References\\Yan-2016-p1486-1499.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {MIMO communication;OFDM modulation;decoding;jamming;radiofrequency interference;MIMO interference cancellation;MIMO technology;MIMO-OFDM receivers;OFDM communication;decoding performance;destructive power;jamming attack;jamming resilient communication;jamming signal powers;multi-input multi-output technology;operational network;orthogonal frequency-division multiplexing;reactive jamming;reactive jamming attack;signal directions;software defined radios;transmit precoding technologies;wireless communications;Channel estimation;Integrated circuits;Interference cancellation;Jamming;MIMO;OFDM;Wireless communication;MIMO;Reactive jamming;interference cancellation;software defined radio;transmit precoding},
  Reviewtime               = {250},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7422100}
}

@Article{Yang-2016-p746-759,
  Title                    = {Conjunctive Keyword Search With Designated Tester and Timing Enabled Proxy Re-Encryption Function for E-Health Clouds},
  Author                   = {Y. Yang and M. Ma},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {746-759},
  Volume                   = {11},

  Abstract                 = {An electronic health (e-health) record system is a novel application that will bring great convenience in healthcare. The privacy and security of the sensitive personal information are the major concerns of the users, which could hinder further development and widely adoption of the systems. The searchable encryption (SE) scheme is a technology to incorporate security protection and favorable operability functions together, which can play an important role in the e-health record system. In this paper, we introduce a novel cryptographic primitive named as conjunctive keyword search with designated tester and timing enabled proxy reencryption function (Re-dtPECK), which is a kind of a time-dependent SE scheme. It could enable patients to delegate partial access rights to others to operate search functions over their records in a limited time period. The length of the time period for the delegatee to search and decrypt the delegator's encrypted documents can be controlled. Moreover, the delegatee could be automatically deprived of the access and search authority after a specified period of effective time. It can also support the conjunctive keywords search and resist the keyword guessing attacks. By the solution, only the designated tester is able to test the existence of certain keywords. We formulate a system model and a security model for the proposed Re-dtPECK scheme to show that it is an efficient scheme proved secure in the standard model. The comparison and extensive simulations demonstrate that it has a low computation and storage overhead.},
  Doi                      = {10.1109/TIFS.2015.2509912},
  File                     = {Yang-2016-p746-759.pdf:References\\Yang-2016-p746-759.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cloud computing;cryptography;data privacy;electronic health records;health care;Re-dtPECK;conjunctive keyword search;delegator encrypted document;designated tester;e-health cloud;e-health record system;electronic health record system;healthcare;keyword guessing attack;proxy reencryption function;search function;searchable encryption scheme;sensitive personal information privacy;sensitive personal information security;time-dependent SE scheme;Encryption;Keyword search;Seals;Servers;Timing;Searchable encryption;conjunctive keywords;designated tester;e-health;resist offline keyword guessing attack;searchable encryption;time control},
  Reviewtime               = {186},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7360175}
}

@Article{Ye-2016-p426-428,
  Title                    = {Comments on Joint Global and Local Structure Discriminant Analysis},
  Author                   = {Q. Ye and J. Jing},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {426-428},
  Volume                   = {11},

  Abstract                 = {Joint global and local structure discriminant analysis (JGLDA) is a recently-developed linear discriminant analysis approach, which considers the diversity of data across classes. In this communication, however, we will show that the discussion on the characterization of within-class locality and within-class diversity of previous efforts on local linear discriminant analysis (LLDA) and JGLDA is flawed.},
  Doi                      = {10.1109/TIFS.2015.2490624},
  File                     = {Ye-2016-p426-428.pdf:References\\Ye-2016-p426-428.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {matrix algebra;security of data;statistical analysis;JGLDA;LLDA;joint global-and-local structure discriminant analysis;linear discriminant analysis;within-class diversity;within-class locality;Diversity reception;Forestry;Kernel;Laplace equations;Linear discriminant analysis;Linear programming;Robustness;Joint global and local structure discriminant analysis;kernel weighting;within-class diversity;within-class locality},
  Reviewtime               = {428},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7297822}
}

@Article{Yi-2016-p1346-1361,
  Title                    = {Private Cell Retrieval From Data Warehouses},
  Author                   = {X. Yi and R. Paulet and E. Bertino and G. Xu},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1346-1361},
  Volume                   = {11},

  Abstract                 = {Publicly accessible data warehouses are an indispensable resource for data analysis. However, they also pose a significant risk to the privacy of the clients, since a data warehouse operator may follow the client's queries and infer what the client is interested in. Private information retrieval (PIR) techniques allow the client to retrieve a cell from a data warehouse without revealing to the operator which cell is retrieved and, therefore, protects the privacy of the client's queries. However, PIR cannot be used to hide online analytical processing (OLAP) operations performed by the client, which may disclose the client's interest. This paper presents a solution for private cell retrieval from a data warehouse on the basis of the Paillier cryptosystem. By our solution, the client can privately perform OLAP operations on the data warehouse and retrieve one (or more) cell without revealing any information about which cell is selected. In addition, we propose a solution for private block download on the basis of the Paillier cryptosystem. Our private block download allows the client to download an encrypted block from a data warehouse without revealing which block in a cloaking region is downloaded and improves the feasibility of our private cell retrieval. Our solutions ensure both the server's privacy and the client's privacy. Our experiments have shown that our solutions are practical.},
  Doi                      = {10.1109/TIFS.2016.2527620},
  File                     = {Yi-2016-p1346-1361.pdf:References\\Yi-2016-p1346-1361.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptography;data analysis;data mining;data privacy;data warehouses;query processing;OLAP operations;PIR techniques;Paillier cryptosystem;client privacy;data analysis;data warehouse operator;online analytical processing operations;private block download;private cell retrieval;private information retrieval techniques;server privacy;Data privacy;Data warehouses;Encryption;Privacy;Protocols;Servers;Data privacy;data warehouse;homomorphic encryption;private information retrieval},
  Reviewtime               = {213},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7404016}
}

@Article{Yu-2016-p1362-1375,
  Title                    = {Enabling Cloud Storage Auditing With Verifiable Outsourcing of Key Updates},
  Author                   = {J. Yu and K. Ren and C. Wang},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1362-1375},
  Volume                   = {11},

  Abstract                 = {Key-exposure resistance has always been an important issue for in-depth cyber defence in many security applications. Recently, how to deal with the key exposure problem in the settings of cloud storage auditing has been proposed and studied. To address the challenge, existing solutions all require the client to update his secret keys in every time period, which may inevitably bring in new local burdens to the client, especially those with limited computation resources, such as mobile phones. In this paper, we focus on how to make the key updates as transparent as possible for the client and propose a new paradigm called cloud storage auditing with verifiable outsourcing of key updates. In this paradigm, key updates can be safely outsourced to some authorized party, and thus the key-update burden on the client will be kept minimal. In particular, we leverage the third party auditor (TPA) in many existing public auditing designs, let it play the role of authorized party in our case, and make it in charge of both the storage auditing and the secure key updates for key-exposure resistance. In our design, TPA only needs to hold an encrypted version of the client's secret key while doing all these burdensome tasks on behalf of the client. The client only needs to download the encrypted secret key from the TPA when uploading new files to cloud. Besides, our design also equips the client with capability to further verify the validity of the encrypted secret keys provided by the TPA. All these salient features are carefully designed to make the whole auditing procedure with key exposure resistance as transparent as possible for the client. We formalize the definition and the security model of this paradigm. The security proof and the performance simulation show that our detailed design instantiations are secure and efficient.},
  Doi                      = {10.1109/TIFS.2016.2528500},
  File                     = {Yu-2016-p1362-1375.pdf:References\\Yu-2016-p1362-1375.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cloud computing;cryptography;outsourcing;TPA;cloud storage auditing;encrypted secret key;key updates verifiable outsourcing;key-exposure resistance;public auditing designs;secure key updates;third party auditor;Cloud computing;Computational modeling;Computer science;Cryptography;Outsourcing;Protocols;Cloud storage;cloud storage auditing;key update;outsourcing computing;verifiability},
  Reviewtime               = {83},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7404239}
}

@Article{Yu-2016-p658-659,
  Title                    = {Comments on Public Integrity Auditing for Dynamic Data Sharing With MultiuserModification},
  Author                   = {Y. Yu and Y. Li and J. Ni and G. Yang and Y. Mu and W. Susilo},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {658-659},
  Volume                   = {11},

  Abstract                 = {Recently, a practical public integrity auditing scheme supporting multiuser data modification (IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, DOI 10.1109/TIFS.2015.2423264) was proposed. Although the protocol was claimed secure, in this paper, we show that the proposal fails to achieve soundness, the most essential property that an auditing scheme should provide. Specifically, we show that a cloud server can collude with a revoked user to deceive a third-party auditor (TPA) that a stored file keeps virgin even when the entire file has been deleted.},
  Doi                      = {10.1109/TIFS.2015.2501728},
  File                     = {Yu-2016-p658-659.pdf:References\\Yu-2016-p658-659.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptographic protocols;TPA;cloud server;dynamic data sharing;multiuser data modification;public integrity auditing scheme;security protocol;third-party auditor;Authentication;Cloud computing;Cryptographic protocols;Protocols;Servers;Cloud storage;data integrity;soundness},
  Reviewtime               = {68},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7331310}
}

@Article{Zeitouni-2016-p1106-1116,
  Title                    = {Remanence Decay Side-Channel: The PUF Case},
  Author                   = {S. Zeitouni and Y. Oren and C. Wachsmann and P. Koeberl and A. R. Sadeghi},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1106-1116},
  Volume                   = {11},

  Abstract                 = {We present a side-channel attack based on remanence decay in volatile memory and show how it can be exploited effectively to launch a noninvasive cloning attack against SRAM physically unclonable functions (PUFs) - an important class of PUFs typically proposed as lightweight security primitives, which use existing memory on the underlying device. We validate our approach using SRAM PUFs instantiated on two 65-nm CMOS devices. We discuss countermeasures against our attack and propose the constructive use of remanence decay to improve the cloning resistance of SRAM PUFs. Moreover, as a further contribution of independent interest, we show how to use our evaluation results to significantly improve the performance of the recently proposed TARDIS scheme, which is based on remanence decay in SRAM memory and used as a time-keeping mechanism for low-power clockless devices.},
  Doi                      = {10.1109/TIFS.2015.2512534},
  File                     = {Zeitouni-2016-p1106-1116.pdf:References\\Zeitouni-2016-p1106-1116.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {CMOS integrated circuits;SRAM chips;public key cryptography;CMOS devices;SRAM PUF;SRAM memory;SRAM physically unclonable functions;TARDIS scheme;cloning resistance improvement;low-power clockless devices;noninvasive cloning attack;performance improvement;remanence decay side-channel;side-channel attack;time-keeping mechanism;volatile memory;Cloning;Cryptography;Performance evaluation;Random access memory;Remanence;SRAM PUF;data remanence decay;fault injection attack;side-channel analysis},
  Reviewtime               = {255},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7366576}
}

@Article{Zhang-2016-p1039-1054,
  Title                    = {A Framework for the Analysis and Evaluation of Algebraic Fault Attacks on Lightweight Block Ciphers},
  Author                   = {F. Zhang and S. Guo and X. Zhao and T. Wang and J. Yang and F. X. Standaert and D. Gu},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {1039-1054},
  Volume                   = {11},

  Abstract                 = {Algebraic fault analysis (AFA), which combines algebraic cryptanalysis with fault attacks, has represented serious threats to the security of lightweight block ciphers. Inspired by an earlier framework for the analysis of side-channel attacks presented at EUROCRYPT 2009, a new generic framework is proposed to analyze and evaluate algebraic fault attacks on lightweight block ciphers. We interpret AFA at three levels: 1) the target; 2) the adversary; and 3) the evaluator. We describe the capability of an adversary in four parts: 1) the fault injector; 2) the fault model describer; 3) the cipher describer; and 4) the machine solver. A formal fault model is provided to cover most of current fault attacks. Different strategies of building optimal equation set are also provided to accelerate the solving process. At the evaluator level, we consider the approximate information metric and the actual security metric. These metrics can be used to guide adversaries, cipher designers, and industrial engineers. To verify the feasibility of the proposed framework, we make a comprehensive study of AFA on an ultra-lightweight block cipher called LBlock. Three scenarios are exploited, which include injecting a fault to encryption, to key scheduling, or modifying the round number or counter. Our best results show that a single fault injection is enough to recover the master key of LBlock within the affordable complexity in each scenario. To verify the generic feature of the proposed framework, we apply AFA to three other block ciphers, i.e., Data Encryption Standard, PRESENT, and Twofish. The results demonstrate that our framework can be used for different ciphers with different structures.},
  Doi                      = {10.1109/TIFS.2016.2516905},
  File                     = {Zhang-2016-p1039-1054.pdf:References\\Zhang-2016-p1039-1054.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptography;AFA;LBlock;adversary;algebraic cryptanalysis;algebraic fault attacks;cipher describer;data encryption standard;evaluator;fault injector;fault model describer;lightweight block ciphers;machine solver;side-channel attacks;target;Ciphers;Circuit faults;Encryption;Mathematical model;Measurement;Algebraic fault analysis (AFA);CryptoMiniSAT;LBlock;Lightweight block cipher;Security evaluation;lightweight block cipher;security evaluation},
  Reviewtime               = {192},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7378309}
}

@Article{Zhang-2016-p609-620,
  Title                    = {Interference Improves PHY Security for Cognitive Radio Networks},
  Author                   = {H. Zhang and T. Wang and L. Song and Z. Han},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {609-620},
  Volume                   = {11},

  Abstract                 = {In a cognitive radio (CR) network, the transmitting signal of a secondary user (SU) is traditionally considered to be harmful for the primary user (PU), since it decreases the capacity of the PU's channel. However, for PU's secrecy capacity, the SUs' interference can be beneficial if it decreases the capacity of the source-eavesdropper channel more than that of the source-destination channel. In this paper, we consider using the SUs' interference to improve the PU's secrecy capacity and providing the SUs the opportunity to access the spectrum as a reward. But, there exists a tradeoff between the SUs' channel capacity and the PU's secrecy capacity. To decide which SUs can share the spectrum with the PU, we present a coalition formation game model with nontransferable utility, and propose a merge and split algorithm. The simulation results verify the efficiency of the proposed algorithm in terms of both the SUs' channel capacity and the PU's secrecy capacity in various scenarios.},
  Doi                      = {10.1109/TIFS.2015.2500184},
  File                     = {Zhang-2016-p609-620.pdf:References\\Zhang-2016-p609-620.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {channel capacity;cognitive radio;game theory;radiofrequency interference;telecommunication security;CR network;PHY security;PU secrecy capacity;SU interference;coalition formation game model;cognitive radio networks;merge and split algorithm;primary user;secondary user;source-destination channel;source-eavesdropper channel capacity;Channel capacity;Games;Interference;Physical layer;Receivers;Security;Transmitters;Cognitive radio network;Termsâ€”Cognitive radio network;coalitional formation game;nontransferable utility;physical layer security},
  Reviewtime               = {407},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7328294}
}

@Article{Zhang-2016-p1192-1205,
  Title                    = {Specific Emitter Identification via Hilbert - Huang Transform in Single-Hop and Relaying Scenarios},
  Author                   = {J. Zhang and F. Wang and O. A. Dobre and Z. Zhong},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1192-1205},
  Volume                   = {11},

  Abstract                 = {In this paper, we investigate the specific emitter identification (SEI) problem, which distinguishes different emitters using features generated by the nonlinearity of the power amplifiers of emitters. SEI is performed by measuring the features representing the individual specifications of emitters and making a decision based on their differences. In this paper, the SEI problem is considered in both single-hop and relaying scenarios, and three algorithms based on the Hilbert spectrum are proposed. The first employs the entropy and the first- and second-order moments as identification features, which describe the uniformity of the Hilbert spectrum. The second uses the correlation coefficient as an identification feature, by evaluating the similarity between different Hilbert spectra. The third exploits Fisher's discriminant ratio to obtain the identification features by selecting the Hilbert spectrum elements with strong class separability. When compared with the existing literature, we further consider the identification problem in a relaying scenario, in which the fingerprint of different emitters is contaminated by the relay's fingerprints. Moreover, we explore the identification performance under various channel conditions, such as additive white Gaussian noise, non-Gaussian noise, and fading. Extensive simulation experiments are performed to evaluate the identification performance of the proposed algorithms, and results show their effectiveness in both single-hop and relaying scenarios, as well as under different channel conditions.},
  Doi                      = {10.1109/TIFS.2016.2520908},
  File                     = {Zhang-2016-p1192-1205.pdf:References\\Zhang-2016-p1192-1205.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {Hilbert transforms;power amplifiers;signal processing;Fisher's discriminant ratio;Hilbert spectrum;Hilbert-Huang transform;SEI problem;additive white Gaussian noise;identification feature;power amplifiers;relay fingerprints;relaying scenarios;single-hop scenarios;specific emitter identification;Feature extraction;Relays;Steady-state;Support vector machines;Time-frequency analysis;Transforms;Transient analysis;Feature extraction;Hilbert spectrum;Relay;Specific emitter identification (SEI);relay;specific emitter identification (SEI)},
  Reviewtime               = {178},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7390059}
}

@Article{Zhang-2016-p74-85,
  Title                    = {Large System Secrecy Rate Analysis for SWIPT MIMO Wiretap Channels},
  Author                   = {J. Zhang and C. Yuen and C. K. Wen and S. Jin and K. K. Wong and H. Zhu},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {74-85},
  Volume                   = {11},

  Abstract                 = {In this paper, we study the multiple-input multiple-output wiretap channel for simultaneous wireless information and power transfer, in which there is a base station (BS), an information-decoding (ID) user, and an energy-harvesting (EH) user. The messages intended to the ID user is required to be kept confidential to the EH user. Our objective is to design the optimal transmit covariance matrix at the BS for maximizing the ergodic secrecy rate subject to the harvested energy requirement for the EH user exploiting only statistical channel state information at the BS. To this end, we begin by deriving an approximation for the ergodic secrecy rate using large-dimensional random matrix theory and the method of Taylor series expansion. This approximation enables us to derive the asymptotic-optimal transmit covariance matrix that achieves the tradeoff for ergodic secrecy rate and harvested energy. The simulation results are provided to verify the accuracy of the approximation and show that a bigger rate-energy region can be achieved when the Rician factor increases or the path loss exponent decreases. We also show that when the transmit correlation increases or the distance between the eavesdropper and the BS decreases, the harvested energy will be increased, while the achieved ergodic secrecy rate decreases.},
  Doi                      = {10.1109/TIFS.2015.2477050},
  File                     = {Zhang-2016-p74-85.pdf:References\\Zhang-2016-p74-85.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {MIMO communication;Rician channels;channel coding;covariance matrices;data communication;decoding;energy harvesting;radiofrequency power transmission;statistical analysis;telecommunication power management;Rician factor;SWIPT MIMO wiretap channel;Taylor series expansion;asymptotic-optimal transmit covariance matrix;base station;energy-harvesting user;ergodic secrecy rate;information-decoding user;large system secrecy rate analysis;large-dimensional random matrix theory;multiple-input multiple-output wiretap channel;optimal transmit covariance matrix design;path loss exponent;simultaneous wireless information and power transfer;statistical channel state information;Approximation methods;Covariance matrices;MIMO;Optimization;Receivers;Wireless communication;Wireless sensor networks;MIMO wiretap channel;SWIPT;energy harvesting;large dimensional random matrix theory},
  Reviewtime               = {203},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7244226}
}

@Article{Zhang-2016-p154-162,
  Title                    = {Energy Harvesting for Physical-Layer Security in OFDMA Networks},
  Author                   = {M. Zhang and Y. Liu},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {154-162},
  Volume                   = {11},

  Abstract                 = {In this paper, we study the simultaneous wireless information and power transfer in downlink multiuser orthogonal frequency-division multiple access systems, where each user applies power splitting to coordinate the energy harvesting and secrecy information decoding processes. Assuming equal power allocation across subcarriers, we formulate an optimization problem to maximize the aggregate harvested power of all users while satisfying secrecy rate requirements of individual users by joint subcarrier allocation and power splitting ratio selection. Due to the NP-hardness of the problem, we propose two suboptimal algorithms to solve the problem. The first one is an iterative algorithm that optimizes subcarrier allocation and power splitting ratios by an alternating way in dual domain. The second algorithm is based on a two-step approach that allocates subcarriers and selects power splitting ratios sequentially. The numerical results show that the proposed methods outperform the conventional methods and provide good trade offs between performance and complexity.},
  Doi                      = {10.1109/TIFS.2015.2481797},
  File                     = {Zhang-2016-p154-162.pdf:References\\Zhang-2016-p154-162.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {OFDM modulation;computational complexity;decoding;energy harvesting;frequency division multiple access;iterative methods;optimisation;telecommunication power management;telecommunication security;NP-hardness problem;OFDMA networks;downlink multiuser orthogonal frequency-division multiple access systems;energy harvesting;iterative algorithm;joint subcarrier allocation;optimization problem;physical layer security;power allocation;power splitting ratio selection;power transfer;secrecy information decoding process;wireless information;Algorithm design and analysis;Communication system security;Complexity theory;Receivers;Resource management;Security;Wireless communication;Physical-layer security;energy harvesting;orthogonal frequency-division multiple access (OFDMA);simultaneous wireless information and power transfer (SWIPT)},
  Reviewtime               = {226},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7275174}
}

@Article{Zhang-2016-p1071-1086,
  Title                    = {Network Diversity: A Security Metric for Evaluating the Resilience of Networks Against Zero-Day Attacks},
  Author                   = {M. Zhang and L. Wang and S. Jajodia and A. Singhal and M. Albanese},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {1071-1086},
  Volume                   = {11},

  Abstract                 = {Diversity has long been regarded as a security mechanism for improving the resilience of software and networks against various attacks. More recently, diversity has found new applications in cloud computing security, moving target defense, and improving the robustness of network routing. However, most existing efforts rely on intuitive and imprecise notions of diversity, and the few existing models of diversity are mostly designed for a single system running diverse software replicas or variants. At a higher abstraction level, as a global property of the entire network, diversity and its effect on security have received limited attention. In this paper, we take the first step toward formally modeling network diversity as a security metric by designing and evaluating a series of diversity metrics. In particular, we first devise a biodiversity-inspired metric based on the effective number of distinct resources. We then propose two complementary diversity metrics, based on the least and the average attacking efforts, respectively. We provide guidelines for instantiating the proposed metrics and present a case study on estimating software diversity. Finally, we evaluate the proposed metrics through simulation.},
  Doi                      = {10.1109/TIFS.2016.2516916},
  File                     = {Zhang-2016-p1071-1086.pdf:References\\Zhang-2016-p1071-1086.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {computer network security;biodiversity-inspired metric;cloud computing security;complementary diversity metrics;moving target defense;network diversity;network routing;security mechanism;security metric;software diversity;zero-day attacks;Biological system modeling;Grippers;Measurement;Resilience;Security;Servers;Software;Biodiversity;computer security;firewalls;information security;intrusion detection},
  Reviewtime               = {181},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7378495}
}

@Article{Zhang-2016-p980-992,
  Title                    = {Privacy-Preserving Data Aggregation in Mobile Phone Sensing},
  Author                   = {Y. Zhang and Q. Chen and S. Zhong},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {980-992},
  Volume                   = {11},

  Abstract                 = {Mobile phone sensing provides a promising paradigm for collecting sensing data and has been receiving increasing attention in recent years. Different from most existing works, which protect participants' privacy by hiding the content of their data and allow the aggregator to compute some simple aggregation functions, we propose a new approach to protect participants' privacy by delinking data from its sources. This approach allows the aggregator to get the exact distribution of the data aggregation and, therefore, enables the aggregator to efficiently compute arbitrary/complicated aggregation functions. In particular, we first present an efficient protocol that allows an untrusted data aggregator to periodically collect sensed data from a group of mobile phone users without knowing which data belong to which user. Assume there are n users in the group. Our protocol achieves n-source anonymity in the sense that the aggregator only learns that the source of a piece of data is one of the n users. Then, we consider a practical scenario where users may have different source anonymity requirements and provide a solution based on dividing users into groups. This solution optimizes the efficiency of data aggregation and meets all users' requirements at the same time.},
  Doi                      = {10.1109/TIFS.2016.2515513},
  File                     = {Zhang-2016-p980-992.pdf:References\\Zhang-2016-p980-992.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {data aggregation;data privacy;mobile computing;mobile phone sensing;n-source anonymity;privacy-preserving data aggregation;untrusted data aggregator;Cryptography;Data privacy;Mobile communication;Mobile handsets;Privacy;Protocols;Sensors;Privacy;cloud computing;data aggregation;mobile sensing;security},
  Reviewtime               = {289},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7373649}
}

@Article{Zhang-2016-p100-113,
  Title                    = {Designing Secure and Dependable Mobile Sensing Mechanisms With Revenue Guarantees},
  Author                   = {Y. Zhang and H. Zhang and S. Tang and S. Zhong},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {100-113},
  Volume                   = {11},

  Abstract                 = {In many existing incentive-based mobile sensing applications, the sensing job owner runs an auction with the mobile phone users to maximize its purchased sensing resource. We notice that both the mobile phone users and the job owner could behave dishonestly to pursue their own interests. This motivates us to design secure and dependable auction mechanisms that generate the correct, promising output even when both of them could cheat. In particular, in this paper, we consider a general auction in which a buyer, who acts as the auctioneer, purchases the resource under a limited budget from a group of sellers who act as the bidders. Considering bidders' privacy and their limited computing capacity, we construct our mechanisms by integrating the innovative game theoretical techniques, logic deductions, and efficient cryptographic operations. Our mechanisms are not only proved to be strategy-proof against dishonest bidders in the sense that they are incentivized to bid their private types truthfully, but also enable all the bidders to efficiently verify the correctness of the auction's outcome, that is computed by the auctioneer, without revealing their private types to each other. Meanwhile, our mechanisms are proved to have the theoretical guarantee that the auctioneer/buyer's expected revenue (i.e. the amount of service it acquires after the auction) is no less than a certain portion of the optimal revenue that the auctioneer can acquire when it knows all the bidders' types at no cost. Our extensive evaluations show that our mechanisms achieve good performance in terms of the revenue maximization and their efficiency.},
  Doi                      = {10.1109/TIFS.2015.2478739},
  File                     = {Zhang-2016-p100-113.pdf:References\\Zhang-2016-p100-113.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptography;data protection;incentive schemes;mobile computing;purchasing;tendering;auctioneer expected revenue;bidder privacy;bidder types;buyer expected revenue;computing capacity;cryptographic operations;dishonest bidders;game theoretical techniques;general auction;incentive-based mobile sensing applications;job owner;logic deductions;mobile phone users;optimal revenue;private types;purchased sensing resource maximization;revenue guarantees;revenue maximization;secure dependable auction mechanism design;secure dependable mobile sensing mechanism design;strategy-proof mechanisms;Crowdsourcing;Cryptography;Games;Indexes;Mobile communication;Mobile handsets;Sensors;Cheating behaviors;cheating behaviors;mobile sensing;privacy-preserving verification},
  Reviewtime               = {130},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7265043}
}

@Article{Zhao-2016-p1603-1617,
  Title                    = {Anti-Forensics of Environmental-Signature-Based Audio Splicing Detection and Its Countermeasure via Rich-Features Classification},
  Author                   = {H. Zhao and Y. Chen and R. Wang and H. Malik},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1603-1617},
  Volume                   = {11},

  Abstract                 = {Numerous methods for detecting audio splicing have been proposed. Environmental-signature-based methods are considered to be the most effective forgery detection methods. The performance of existing audio forensic analysis methods is generally measured in the absence of any anti-forensic attack. Effectiveness of these methods in the presence of anti-forensic attacks is therefore unknown. In this paper, we propose an effective anti-forensic attack for environmental-signature-based splicing detection method and countermeasures to detect the presence of the anti-forensic attack. For anti-forensic attack, dereverberation-based processing is proposed. Three dereverberation methods are considered to tamper with the acoustic environment signature. Experimental results indicate that the proposed dereverberation-based anti-forensic attack significantly degrades the performance of the selected splicing detection method. The proposed countermeasures exploit artifacts introduced by the anti-forensic processing. To detect the presence of potential anti-forensic processing, a machine learning-based framework is proposed. In particular, the proposed anti-forensic detection method uses a rich-feature model consisting of Fourier coefficients, spectral properties, high-order statistics of musical noise residuals, and modulation spectral coefficients to capture traces of dereverberation attacks. The performance of the proposed framework is evaluated on both synthetic data and real-world speech recordings. The experimental results show that the proposed rich-feature model can detect the presence of anti-forensic processing with an average accuracy of 95%.},
  Doi                      = {10.1109/TIFS.2016.2543205},
  File                     = {Zhao-2016-p1603-1617.pdf:References\\Zhao-2016-p1603-1617.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {audio signal processing;environmental factors;learning (artificial intelligence);Fourier coefficients;acoustic environment signature;antiforensics;audio forensic analysis methods;audio splicing detection;dereverberation attacks;forgery detection methods;high-order statistics;machine learning-based framework;modulation spectral coefficients;musical noise residuals;real-world speech recordings;rich-features classification;spectral properties;synthetic data;Acoustics;Authentication;Feature extraction;Forensics;Robustness;Speech;Splicing;Anti-forensics;Audio Splicing Detection;Audio splicing detection;Modulation Spectrum;Musical Noise;Spectral Features;anti-forensics;modulation spectrum;musical noise;spectral features},
  Reviewtime               = {16},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7435302}
}

@Article{Zheng-2016-p633-641,
  Title                    = {Suspecting Less and Doing Better: New Insights on Palmprint Identification for Faster and More Accurate Matching},
  Author                   = {Q. Zheng and A. Kumar and G. Pan},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {633-641},
  Volume                   = {11},

  Abstract                 = {This paper introduces a generalized palmprint identification framework to unify several state-of-art 2D and 3D palmprint methods. Through this framework, we argue that the methods employing one-to-one matching strategy and binary representation for feature are more effective for palmprint identification. The analysis for the first argument is based on a statistical matching model and is supported by outperforming results on several publicly available 2D palmprpint databases. These two arguments are further evaluated for 3D palmprint matching and used to introduce a new method for encoding 3D palmprint feature. The proposed 3D feature is binary and more efficiently computed. It encodes the 3D shape of palmprint to either convex or concave. The experimental results on two publicly available, from contactless and contact-base 3D palmprint database of 177 and 200 subjects, respectively, outperform the state-of-the-art methods. This paper also provides our palmprint matching algorithm(s) in public domain, unlike the previous work in this area, which will help to further advance research efforts in this area.},
  Doi                      = {10.1109/TIFS.2015.2503265},
  File                     = {Zheng-2016-p633-641.pdf:References\\Zheng-2016-p633-641.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {concave programming;convex programming;feature extraction;image matching;image representation;palmprint recognition;statistical analysis;2D palmprint method;3D palmprint feature encoding;3D palmprint matching;3D palmprint method;binary feature representation;contact-base 3D palmprint database;generalized palmprint identification framework;one-to-one matching strategy;statistical matching model;Biometrics (access control);Databases;Encoding;Feature extraction;Image coding;Probes;Three-dimensional displays;2D Palmprint;2D palmprint;3D Palmprint;3D palmprint;Contactless Palmprint Matching;contactless palmprint matching},
  Reviewtime               = {188},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7335640}
}

@Article{Zhu-2016-p1117-1127,
  Title                    = {Generating Correlated Digital Certificates: Framework and Applications},
  Author                   = {W. T. Zhu and J. Lin},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1117-1127},
  Volume                   = {11},

  Abstract                 = {Bolstering public key authentication of networking entities, digital certificates are an entrenched part of Internet security. A digital certificate is an electronic document signed by a certificate authority (CA), vouching that the identified subject owns the declared public key (and the corresponding private key). In general, CAs are also responsible for certificate revocation as well as reissue, and certificates by nature are considered independent of each other. In this paper, we address the problem of certificate management and propose a flexible framework to create correlated certificates. We then apply it to implement the so-called multi-certificate public key infrastructure, which supports user self services, such as certificates' spontaneous substitution as well as self-reissue after self-revocation. To the best of our knowledge, this is the first scheme for certificate users to achieve self-reissue. Another application of the proposed framework is the so-called anonymous digital certificate, which still binds a user's identity to her public key, but in an anonymous yet user-controllable manner. That is, a user can reveal her identity-key binding only to her specified communication peers, while remaining anonymous to the general public, achieving privacy as these certificates are generally unlinkable.},
  Doi                      = {10.1109/TIFS.2016.2516818},
  File                     = {Zhu-2016-p1117-1127.pdf:References\\Zhu-2016-p1117-1127.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {Internet;document handling;public key cryptography;Bolstering public key authentication;Internet security;anonymous digital certificate;certificate authority;digital certificates;electronic document;generating correlated digital certificates;multicertificate public key infrastructure;networking entities;Authentication;Internet;Privacy;Public key;Standards;Internet security;authentication;digital certificate;information embedding;infrastructure;privacy;public key;self service;user identity},
  Reviewtime               = {151},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7378493}
}

@Article{Zhu-2016-p141-153,
  Title                    = {A Joint Source-Channel Adaptive Scheme for Wireless H.264/AVC Video Authentication},
  Author                   = {X. Zhu and C. W. Chen},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {141-153},
  Volume                   = {11},

  Abstract                 = {Authentication has become an emerging issue for video streaming over lossy networks. Although the advanced video coding standards, such as H.264/AVC, efficiently reduce the amount of data to be transmitted, the coding dependency brings new challenges in designing efficient stream authentication scheme. In this paper, we propose a novel joint-designed-layered source-channel adaptive scheme that integrates authentication into source and channel coding components to sufficiently use the related information to efficiently address the coding dependency and to design the optimal rate allocation scheme for the sake of end-to-end video quality. The proposed layered framework is able to minimize end-to-end quality degradation incurred by both the wireless channel noise and the authentication failure. In particular, the competing requirements of high verification probability and low authentication overhead are concurrently satisfied by the elegant design of layered hash appending with efficient adaptation to the H.264 source coding and channel conditions. A joint source-channel-authentication rate allocation scheme is then developed to achieve optimal end-to-end video quality. The experimental results on H.264 video sequences confirm the efficacy of this joint adaptive scheme and demonstrate that it indeed outperforms the state-of-the-art graph-based authentication algorithms.},
  Doi                      = {10.1109/TIFS.2015.2481366},
  File                     = {Zhu-2016-p141-153.pdf:References\\Zhu-2016-p141-153.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {combined source-channel coding;probability;telecommunication security;video coding;video streaming;wireless channels;advanced video coding standards;channel coding;high verification probability;joint source-channel adaptive scheme;layered hash appending;lossy networks;low authentication overhead;optimal rate allocation scheme;source coding;video streaming;wireless H.264/AVC video authentication;wireless channel noise;Authentication;Channel coding;Joints;Media;Streaming media;Video coding;H.264 video streaming;Multimedia authentication;digital signature;stream authentication;wireless media communication},
  Reviewtime               = {275},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7274690}
}

@Article{Zivari-Fard-2016-p1239-1251,
  Title                    = {Imperfect and Perfect Secrecy in Compound Multiple Access Channel With Confidential Message},
  Author                   = {H. Zivari-Fard and B. Akhbari and M. Ahmadian-Attari and M. R. Aref},
  Journal                  = {IEEE Transactions on Information Forensics and Security},
  Year                     = {2016},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1239-1251},
  Volume                   = {11},

  Abstract                 = {In this paper, we study the problem of secret communication over a compound Multiple Access Channel (MAC). In this channel, we assume that one of the transmitted messages is confidential, which is only decoded by its corresponding receiver and kept secret from the other receiver. We call this proposed setting the compound MAC with a confidential message. For this model, we derive general inner and outer bounds for both imperfect and perfect secrecy conditions for the second receiver. Also, as examples, we investigate less noisy and Gaussian versions of this channel, and extend the results of the discrete memoryless version to these cases. Moreover, providing numerical examples for the Gaussian case, we illustrate the comparison between achievable rate regions of compound MAC and compound MAC with a confidential message. In addition, for the Gaussian case, we show that using cooperative jamming strategy can increase the achievable secrecy rate between the legitimate transmitter and the receiver.},
  Doi                      = {10.1109/TIFS.2016.2523813},
  File                     = {Zivari-Fard-2016-p1239-1251.pdf:References\\Zivari-Fard-2016-p1239-1251.pdf:PDF},
  ISSN                     = {1556-6013},
  Keywords                 = {Gaussian channels;cooperative communication;jamming;multiuser channels;radio receivers;radio transmitters;telecommunication security;wireless channels;Gaussian case;compound MAC;compound multiple access channel;confidential message;cooperative jamming strategy;imperfect secrecy condition;legitimate receiver;legitimate transmitter;perfect secrecy condition;secrecy rate;secret communication problem;Compounds;Decoding;Jamming;Noise measurement;Receivers;Transmitters;Wireless communication;Wiretap channel;capacity-equivocation region;compound multiple access channel;imperfect secrecy;outer bound;perfect secrecy;secrecy rate region},
  Reviewtime               = {300},
  Url                      = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7395350}
}

@comment{jabref-meta: databaseType:bibtex;}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:201601\;0\;Bock-2016-p19-34\;Cheon-2016-p188-199\;Cont
i-2016-p114-125\;David-2016-p59-73\;Li-2016-p86-99\;Lin-2016-p126-140\
;Lin-2016-p46-58\;Maiorana-2016-p163-175\;Oikawa-2016-p5-18\;Phuong-20
16-p35-45\;Ruan-2016-p176-187\;Tang-2016-p200-214\;Zhang-2016-p100-113
\;Zhang-2016-p154-162\;Zhang-2016-p74-85\;Zhu-2016-p141-153\;;
1 ExplicitGroup:201602\;0\;Bajaj-2016-p303-312\;Bianchi-2016-p313-327\
;Cheng-2016-p273-288\;Dai-2016-p410-425\;Das-2016-p289-302\;Daugm-2016
-p400-409\;Gao-2016-p429-430\;Guo-2016-p247-257\;Han-2016-p258-272\;Hu
-2016-p388-399\;Jolfaei-2016-p235-246\;Li-2016-p344-357\;Naini-2016-p3
58-372\;Oliveira-2016-p328-343\;Sedighi-2016-p221-234\;Xu-2016-p373-38
7\;Ye-2016-p426-428\;;
1 ExplicitGroup:201603\;0\;Chernyshev-2016-p584-593\;Dantcheva-2016-p4
41-467\;Han-2016-p556-570\;Komogortsev-2016-p621-632\;Kong-2016-p594-6
08\;Lazzeretti-2016-p642-657\;Li-2016-p543-555\;Liu-2016-p484-497\;Liu
-2016-p514-527\;Shen-2016-p498-513\;Shu-2016-p528-542\;Tkachenko-2016-
p571-583\;Tongaonkar-2016-p468-483\;Yu-2016-p658-659\;Zhang-2016-p609-
620\;Zheng-2016-p633-641\;;
1 ExplicitGroup:201604\;0\;Carvalho-2016-p720-733\;Castiglione-2016-p8
50-865\;Caviglione-2016-p799-810\;Chen-2016-p789-798\;Chu-2016-p774-78
8\;Chu-2016-p823-836\;Duda-2016-p691-703\;Garnaev-2016-p837-849\;Giare
tta-2016-p665-676\;Jung-2016-p868-868\;Liu-2016-p677-690\;Ma-2016-p866
-867\;Schoettle-2016-p760-773\;Taha-2016-p811-822\;Tang-2016-p734-745\
;Wang-2016-p704-719\;Yang-2016-p746-759\;;
1 ExplicitGroup:201605\;0\;Almalawi-2016-p893-906\;Cheng-2016-p993-100
2\;Hua-2016-p1003-1016\;Kumar-2016-p1027-1038\;Raghavendra-2016-p922-9
36\;Saini-2016-p1055-1070\;Saxena-2016-p907-921\;Sitova-2016-p877-892\
;Sun-2016-p937-950\;Werghi-2016-p964-979\;Xiang-2016-p951-963\;Xiong-2
016-p1017-1026\;Zhang-2016-p1039-1054\;Zhang-2016-p1071-1086\;Zhang-20
16-p980-992\;;
1 ExplicitGroup:201606\;0\;Abrardo-2016-p1333-1345\;Chen-2016-p1093-11
05\;Cherkaoui-2016-p1291-1305\;Deng-2016-p1128-1138\;Diyanat-2016-p132
1-1332\;Hamadene-2016-p1226-1238\;Jin-2016-p1306-1320\;Nogueira-2016-p
1206-1213\;Safaka-2016-p1177-1191\;Salmani-2016-p1214-1225\;Wang-2016-
p1151-1164\;Wang-2016-p1165-1176\;Wang-2016-p1265-1277\;Wu-2016-p1278-
1290\;Xu-2016-p1139-1150\;Xu-2016-p1252-1264\;Yi-2016-p1346-1361\;Yu-2
016-p1362-1375\;Zeitouni-2016-p1106-1116\;Zhang-2016-p1192-1205\;Zhu-2
016-p1117-1127\;Zivari-Fard-2016-p1239-1251\;;
1 ExplicitGroup:201607\;0\;Bharadwaj-2016-p1630-1641\;Chen-2016-p1453-
1460\;Chen-2016-p1476-1485\;Compagno-2016-p1565-1577\;Dubey-2016-p1461
-1475\;Fang-2016-p1515-1527\;Hu-2016-p1549-1564\;Huo-2016-p1528-1541\;
Ji-2016-p1398-1411\;Kolokotronis-2016-p1500-1514\;Liu-2016-p1385-1397\
;Liu-2016-p1592-1602\;Pasquini-2016-p1425-1437\;Pinto-2016-p1542-1548\
;Roy-2016-p1412-1424\;Ruiz-Blondet-2016-p1618-1629\;Saxena-2016-p1438-
1452\;Senftleben-2016-p1578-1591\;Yan-2016-p1486-1499\;Zhao-2016-p1603
-1617\;;
}

